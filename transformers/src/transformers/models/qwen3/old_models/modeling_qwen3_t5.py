# # modeling_qwen3_t5

# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# #           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
# #               Do NOT edit this file manually as any edits will be overwritten by the generation of
# #             the file from the modular. If any change should be done, please apply the change to the
# #                          modular_qwen3.py file directly. One of our CI enforces this.
# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# # coding=utf-8
# # Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
# #
# # Licensed under the Apache License, Version 2.0 (the "License");
# # you may not use this file except in compliance with the License.
# # You may obtain a copy of the License at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.
# from __future__ import annotations

# import math
# from typing import Optional, List

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.amp import autocast

# from functools import partial
# from typing import Callable, Optional, Tuple, Union

# import torch
# from torch import nn

# from ...activations import ACT2FN
# from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
# from ...generation import GenerationMixin
# from ...modeling_attn_mask_utils import AttentionMaskConverter
# from ...modeling_flash_attention_utils import FlashAttentionKwargs
# from ...modeling_outputs import (
#     BaseModelOutputWithPast,
#     QuestionAnsweringModelOutput,
#     SequenceClassifierOutputWithPast,
#     TokenClassifierOutput,
# )
# from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
# from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
# from ...processing_utils import Unpack
# from ...utils import (
#     # LossKwargs,
#     add_code_sample_docstrings,
#     add_start_docstrings,
#     add_start_docstrings_to_model_forward,
#     can_return_tuple,
#     logging,
#     replace_return_docstrings,
# )
# from ...utils.deprecation import deprecate_kwarg
# from .configuration_qwen3 import Qwen3Config
# from ...utils import ModelOutput
# from dataclasses import dataclass
# from torch.distributions import Gamma
# import numpy as np
# from transformers import BertConfig, BertModel
# import torch, torch.nn.functional as F
# from typing import List


# def initialize_weights(module):
#     """
#     Recursively initializes weights for Linear, Conv, and Norm layers.
#     """
#     if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
#         # Use Kaiming normal initialization for weights
#         nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
#         # Initialize biases to zero, if they exist
#         if module.bias is not None:
#             nn.init.constant_(module.bias, 0)
#     elif isinstance(module, (nn.LayerNorm, nn.modules.normalization.RMSNorm)):
#         # Initialize normalization layers: weight to 1, bias to 0
#         if hasattr(module, 'weight') and module.weight is not None:
#             nn.init.constant_(module.weight, 1.0)
#         if hasattr(module, 'bias') and module.bias is not None:
#             nn.init.constant_(module.bias, 0)



# # Better Feature Extractors (Pro): attention + hidden states
# # =========================================================
# import math, torch
# import torch.nn as nn
# import torch.nn.functional as F
# from contextlib import contextmanager
# from typing import Optional, Tuple

# # ----------------------------
# # Utility: mixed-precision safe
# # ----------------------------
# @contextmanager
# def no_amp_fp32(enabled: bool = True):
#     if not enabled:
#         yield
#         return
#     try:
#         from torch.cuda.amp import autocast
#         with autocast(enabled=False):
#             yield
#     except Exception:
#         # CPU / no-amp fallback
#         yield

# # ----------------------------
# # Small helpers
# # ----------------------------
# def _safe_dtype_param(module: nn.Module):
#     for p in module.parameters():
#         return p.dtype
#     return torch.bfloat16

# # ============================
# # Better Feature Extractors (Pro) â€” dtype-safe, no SVD
# # ============================
# import math, torch
# import torch.nn as nn
# import torch.nn.functional as F
# from contextlib import contextmanager
# from typing import Optional

# # ----------------------------
# # Utility: mixed-precision safe
# # ----------------------------
# @contextmanager
# def no_amp_fp32(enabled: bool = True):
#     if not enabled:
#         yield
#         return
#     try:
#         from torch.cuda.amp import autocast
#         with autocast(enabled=False):
#             yield
#     except Exception:
#         yield

# # ----------------------------
# # Small helpers
# # ----------------------------
# def _num_groups(c: int, g: int = 8) -> int:
#     for k in [g, 6, 4, 3, 2, 1]:
#         if c % k == 0:
#             return k
#     return 1

# def _module_param_dtype(mod: nn.Module) -> torch.dtype:
#     for p in mod.parameters():
#         return p.dtype
#     return torch.float32

# def masked_area_downsample_1d(x: torch.Tensor, mask: torch.Tensor, k: int, eps: float = 1e-6) -> torch.Tensor:
#     """
#     x:    (B,C,N)
#     mask: (B,1,N)  with 1=valid
#     return: (B,C,k)  area interpolation weighted by valid counts
#     """
#     B, C, N = x.shape
#     assert mask.shape == (B, 1, N)
#     num = F.interpolate(x * mask, size=k, mode="area")
#     den = F.interpolate(mask,   size=k, mode="area")
#     return num / (den + eps)

# def percentile(x: torch.Tensor, q: float, dim: Optional[int] = None, keepdim: bool = False) -> torch.Tensor:
#     # q in [0,1]
#     n = x.shape[dim] if dim is not None else x.numel()
#     k = max(1, int(n * q))
#     if dim is None:
#         vals, _ = torch.kthvalue(x.view(-1), k)
#         return vals
#     vals, _ = torch.kthvalue(x, k, dim=dim, keepdim=keepdim)
#     return vals

# # ----------------------------
# # Set Transformer Primitives
# # ----------------------------
# class MultiHeadAttention(nn.Module):
#     def __init__(self, d_model: int, n_heads: int = 4, pdrop: float = 0.1):
#         super().__init__()
#         assert d_model % n_heads == 0
#         self.h = n_heads
#         self.dk = d_model // n_heads
#         self.q = nn.Linear(d_model, d_model)
#         self.k = nn.Linear(d_model, d_model)
#         self.v = nn.Linear(d_model, d_model)
#         self.o = nn.Linear(d_model, d_model)
#         self.dropout = nn.Dropout(pdrop)

#     def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:
#         B, Tq, D = Q.shape
#         Tk = K.size(1)

#         q = self.q(Q).view(B, Tq, self.h, self.dk).transpose(1, 2)  # (B,h,Tq,dk)
#         k = self.k(K).view(B, Tk, self.h, self.dk).transpose(1, 2)  # (B,h,Tk,dk)
#         v = self.v(V).view(B, Tk, self.h, self.dk).transpose(1, 2)  # (B,h,Tk,dk)

#         with no_amp_fp32(True):
#             scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.dk)  # (B,h,Tq,Tk)
#             attn = scores.softmax(dim=-1)

#         attn = self.dropout(attn)
#         out = torch.matmul(attn, v)                                    # (B,h,Tq,dk)
#         out = out.transpose(1, 2).contiguous().view(B, Tq, D)          # (B,Tq,D)
#         return self.o(out)

# class MAB(nn.Module):
#     """ Multihead Attention Block (Set Transformer) """
#     def __init__(self, d_model: int, n_heads: int = 4, pdrop: float = 0.1, ff_mult: int = 2):
#         super().__init__()
#         self.mha = MultiHeadAttention(d_model, n_heads, pdrop)
#         self.ln1 = nn.LayerNorm(d_model)
#         self.ff  = nn.Sequential(
#             nn.Linear(d_model, ff_mult * d_model),
#             nn.GELU(),
#             nn.Dropout(pdrop),
#             nn.Linear(ff_mult * d_model, d_model),
#         )
#         self.ln2 = nn.LayerNorm(d_model)

#     def forward(self, Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:
#         x = self.ln1(Q + self.mha(Q, K, K))
#         x = self.ln2(x + self.ff(x))
#         return x

# class SAB(nn.Module):
#     """ Self-Attention Block (Q=K=X) """
#     def __init__(self, d_model: int, n_heads: int = 4, pdrop: float = 0.1, num_layers: int = 1):
#         super().__init__()
#         self.layers = nn.ModuleList([MAB(d_model, n_heads, pdrop) for _ in range(num_layers)])
#     def forward(self, X: torch.Tensor) -> torch.Tensor:
#         for mab in self.layers:
#             X = mab(X, X)
#         return X

# class PMA(nn.Module):
#     """ Pooling by Multihead Attention with K learnable seeds """
#     def __init__(self, d_model: int, num_seeds: int = 4, n_heads: int = 4, pdrop: float = 0.1):
#         super().__init__()
#         self.S = nn.Parameter(torch.randn(num_seeds, d_model) / math.sqrt(d_model))
#         self.mab = MAB(d_model, n_heads, pdrop)
#     def forward(self, X: torch.Tensor) -> torch.Tensor:
#         B = X.size(0)
#         S = self.S.unsqueeze(0).expand(B, -1, -1)  # (B,K,D)
#         return self.mab(S, X)                      # (B,K,D)

# # ----------------------------
# # AttnFeatureExtractorLite  (NO SVD; 2D-FFT bands instead)
# # ----------------------------
# class AttnFeatureExtractorLite(nn.Module):
#     """
#     Input:  attn (B, L, H, k, k)
#     Output: z_att (B, D_ATT)

#     Multi-view per-(layer,head) features:
#       â€¢ Multi-scale CNN (k, k/2, k/4) with CoordConv-style positional hints
#       â€¢ Spectral (no SVD): 2D-FFT band powers + spectral entropy
#       â€¢ Graph-ish: row/col stats, diagonal band energy, Laplacian trace proxy
#     Token mixer: Set Transformer (SAB) + PMA (K seeds) â†’ flatten â†’ MLP
#     """
#     def __init__(
#         self,
#         D_ATT: int = 512,
#         d_tok: int = 160,
#         cnn_c: int = 64,
#         top_r: int = 0,          # kept for API compatibility; unused
#         K: int = 4,
#         max_layers: int = 128,
#         max_heads: int = 256,
#         sab_layers: int = 1,
#         sab_heads: int = 4,
#         pdrop: float = 0.10,
#     ):
#         super().__init__()
#         self.d_tok = d_tok

#         # CoordConv-ish stem for a single-channel map
#         def stem(in_c=3, c=cnn_c):
#             return nn.Sequential(
#                 nn.Conv2d(in_c, c, 3, padding=1), nn.GroupNorm(_num_groups(c), c), nn.GELU(),
#                 nn.Conv2d(c, c, 3, padding=1),     nn.GroupNorm(_num_groups(c), c), nn.GELU()
#             )
#         self.cnn_s0 = stem()     # original kÃ—k
#         self.cnn_s1 = stem()     # k/2
#         self.cnn_s2 = stem()     # k/4

#         # per-scale global avg/max => 2*cnn_c * 3 scales = 6*cnn_c
#         # stats (no SVD): [spec_entropy, P_low, P_mid, P_high, P_vhigh,
#         #                  rvar, cvar, rent, cent, diag_ratio, band_w1, band_w2, lap_trace] = 13
#         self.proj = nn.Linear(6 * cnn_c + 13, d_tok)

#         self.layer_emb = nn.Embedding(max_layers, d_tok)
#         self.head_emb  = nn.Embedding(max_heads, d_tok)

#         self.sab = SAB(d_model=d_tok, n_heads=sab_heads, pdrop=pdrop, num_layers=sab_layers)
#         self.pma = PMA(d_model=d_tok, num_seeds=K, n_heads=sab_heads, pdrop=pdrop)

#         self.out = nn.Sequential(
#             nn.Linear(K * d_tok, 2 * d_tok), nn.GELU(), nn.Dropout(pdrop),
#             nn.Linear(2 * d_tok, D_ATT)
#         )

#     def _coord(self, B_L_H: int, k: int, device, dtype) -> torch.Tensor:
#         """ Return (B_L_H, 2, k, k) normalized coords [-1,1] with desired dtype """
#         ys = torch.linspace(-1, 1, steps=k, device=device, dtype=dtype)
#         xs = torch.linspace(-1, 1, steps=k, device=device, dtype=dtype)
#         yy, xx = torch.meshgrid(ys, xs, indexing="ij")
#         coords = torch.stack([yy, xx], dim=0).unsqueeze(0).expand(B_L_H, -1, -1, -1)
#         return coords

#     @torch.no_grad()
#     def _spectral_graph_stats(self, A: torch.Tensor) -> torch.Tensor:
#         """
#         A: (B,1,k,k) in fp32
#         Returns: (B, 13) = [spec_entropy, P_low, P_mid, P_high, P_vhigh,
#                             rvar, cvar, rent, cent,
#                             diag_ratio, band_w1, band_w2, lap_trace]
#         """
#         B, _, k, _ = A.shape
#         A2 = A.squeeze(1)  # (B,k,k)

#         # -------- Spectral (2D FFT) --------
#         Xf = torch.fft.rfft2(A2, dim=(-2, -1))                  # (B, k, k//2+1)
#         P  = (Xf.real**2 + Xf.imag**2) + 1e-12                 # power
#         Psum = P.sum(dim=(-2, -1), keepdim=False)              # (B,)
#         Pn = P / (Psum.view(B, 1, 1) + 1e-12)                  # normalized power

#         # radial frequency grid
#         fy = torch.linspace(-0.5, 0.5, steps=k, device=A.device)
#         fx = torch.linspace(0.0,  0.5, steps=(k // 2) + 1, device=A.device)
#         yy, xx = torch.meshgrid(fy, fx, indexing="ij")
#         rad = torch.sqrt(yy**2 + xx**2)                        # (k, k//2+1)
#         max_r = rad.max().clamp_min(1e-6)
#         r1, r2, r3 = 0.15*max_r, 0.35*max_r, 0.60*max_r

#         def band(mask):
#             m = mask.to(P.dtype).unsqueeze(0)                  # (1,k,k//2+1)
#             e = (P * m).sum(dim=(-2, -1))                      # (B,)
#             return (e / (Psum + 1e-12)).unsqueeze(-1)          # (B,1)

#         Pl = band(rad <= r1)                                   # (B,1)
#         Pm = band((rad > r1) & (rad <= r2))
#         Ph = band((rad > r2) & (rad <= r3))
#         Pv = band(rad > r3)
#         sent = (-(Pn * Pn.log()).sum(dim=(-2, -1))).unsqueeze(-1)  # (B,1)

#         # -------- Graph-ish stats on rows/cols --------
#         rows = A2.clamp_min(0)
#         cols = rows.transpose(-1, -2)
#         rsum = rows.sum(dim=-1)                                # (B,k)
#         csum = cols.sum(dim=-1)                                # (B,k)
#         rvar = rsum.var(dim=-1, unbiased=False).unsqueeze(-1)  # (B,1)
#         cvar = csum.var(dim=-1, unbiased=False).unsqueeze(-1)  # (B,1)

#         def _entropy(x, dim=-1):
#             p = (x / (x.sum(dim=dim, keepdim=True) + 1e-8)).clamp_min(1e-8)
#             return (-(p * p.log()).sum(dim=dim, keepdim=True))

#         rent = _entropy(rows, dim=-1).mean(dim=-2)             # (B,1)
#         cent = _entropy(cols, dim=-1).mean(dim=-2)             # (B,1)

#         # totals as (B,1); keep everything 2-D downstream
#         total = A2.abs().sum(dim=(-1, -2), keepdim=False).unsqueeze(-1) + 1e-6  # (B,1)
#         diag  = torch.diagonal(A2, dim1=-2, dim2=-1).abs().sum(dim=-1, keepdim=True)  # (B,1)
#         diag_ratio = diag / total                                                # (B,1)

#         # diagonal band energies, returned as (B,1)
#         def band_energy(width: int):
#             mask2d = torch.zeros(k, k, device=A2.device, dtype=A2.dtype)
#             for d in range(-width, width + 1):
#                 diag_len = k - abs(d)
#                 if diag_len <= 0:
#                     continue
#                 mask2d += torch.diag(torch.ones(diag_len, device=A2.device, dtype=A2.dtype), diagonal=d)
#             m = mask2d.clamp_max(1).unsqueeze(0)                  # (1,k,k)
#             e = (A2.abs() * m).sum(dim=(-1, -2))                  # (B,)
#             return (e / total.squeeze(-1)).unsqueeze(-1)          # (B,1)

#         band_w1 = band_energy(max(1, k // 32))                    # (B,1)
#         band_w2 = band_energy(max(2, k // 16))                    # (B,1)

#         # Laplacian trace proxy
#         D_trace  = rsum.sum(dim=-1, keepdim=True)                 # (B,1)
#         A_trace  = A2.diagonal(dim1=-2, dim2=-1).sum(dim=-1, keepdim=True)  # (B,1)
#         lap_trace = D_trace - A_trace                              # (B,1)

#         return torch.cat(
#             [sent, Pl, Pm, Ph, Pv, rvar, cvar, rent, cent, diag_ratio, band_w1, band_w2, lap_trace],
#             dim=-1
#         )  # (B,13)


#     def _cnn_gpool(self, x: torch.Tensor, stem: nn.Module) -> torch.Tensor:
#         """ x: (B,3,k,k) â†’ concat(GlobalAvg, GlobalMax) via given stem; cast input to stem dtype """
#         target_dtype = _module_param_dtype(stem)
#         x = x.to(target_dtype)
#         z = stem(x)                                 # (B,C,k,k)
#         gavg = F.adaptive_avg_pool2d(z, 1).flatten(1)
#         gmax = F.adaptive_max_pool2d(z, 1).flatten(1)
#         return torch.cat([gavg, gmax], dim=-1)      # (B, 2C)

#     def forward(self, attn: torch.Tensor) -> torch.Tensor:
#         """
#         attn: (B,L,H,k,k)
#         """
#         B, L, H, k, k2 = attn.shape
#         assert k == k2
#         device = attn.device
#         T = L * H

#         maps = attn.reshape(B * T, 1, k, k)                        # (B*T,1,k,k) (bf16 likely)
#         coords = self._coord(B * T, k, device, dtype=maps.dtype)   # coords in same dtype
#         x = torch.cat([maps, coords], dim=1)                       # (B*T,3,k,k)

#         # multi-scale (dtype preserved)
#         s0 = x
#         s1 = F.avg_pool2d(s0, 2, ceil_mode=True)
#         s2 = F.avg_pool2d(s0, 4, ceil_mode=True)

#         # CNN stems (dtype alignment handled inside _cnn_gpool)
#         with no_amp_fp32(True):
#             f0 = self._cnn_gpool(s0, self.cnn_s0)
#             f1 = self._cnn_gpool(s1, self.cnn_s1)
#             f2 = self._cnn_gpool(s2, self.cnn_s2)
#             stats = self._spectral_graph_stats(maps.to(torch.float32))   # fp32 stats only

#         per_map = torch.cat([f0, f1, f2, stats.to(f0.dtype)], dim=-1)    # (B*T, 6C+13)
#         toks = self.proj(per_map).view(B, T, self.d_tok)

#         # add (layer, head) embeddings with dtype match
#         l_idx = torch.arange(L, device=device).repeat_interleave(H)
#         h_idx = torch.arange(H, device=device).repeat(L)
#         pe = (self.layer_emb(l_idx) + self.head_emb(h_idx)).to(toks.dtype)  # (T,D)
#         toks = toks + pe.unsqueeze(0)

#         # set transformer: SAB + PMA
#         toks = self.sab(toks)            # (B,T,D)
#         pooled = self.pma(toks)          # (B,K,D)
#         return self.out(pooled.flatten(1))  # (B, D_ATT)

# # ----------------------------
# # HiddenFeatureExtractorLite
# # ----------------------------
# class HiddenFeatureExtractorLite(nn.Module):
#     """
#     Input:  last_hidden (B, S, D_model), mask_tokens (B,S) optional
#     Output: z_hid (B, D_HID)

#     Streams:
#       â€¢ Gated Dilated Depthwise Conv stack over time (local patterns at multiple scales)
#       â€¢ Light SAB over time (global mixing)
#       â€¢ FFT + trend/volatility stats on channel-mean
#       â€¢ PMA pooling with K seeds â†’ flatten â†’ MLP
#     """
#     def __init__(
#         self,
#         D_model: int,
#         D_HID: int = 512,
#         d_tok: int = 192,
#         k_hid: int = 192,
#         groups: int = 8,
#         K: int = 3,
#         sab_layers: int = 1,
#         sab_heads: int = 4,
#         pdrop: float = 0.10,
#     ):
#         super().__init__()
#         self.k_hid = int(k_hid)
#         self.d_tok = int(d_tok)

#         self.norm = nn.LayerNorm(D_model)
#         self.proj = nn.Linear(D_model, d_tok)

#         g = min(groups, d_tok)
#         # Dilated depthwise conv stack with gating
#         def dw_block(dil):
#             return nn.Sequential(
#                 nn.Conv1d(d_tok, d_tok, 5, padding=2*dil, dilation=dil, groups=g),
#                 nn.GroupNorm(_num_groups(d_tok), d_tok),
#                 nn.GELU(),
#             )
#         self.dw1 = dw_block(1)
#         self.dw2 = dw_block(2)
#         self.dw3 = dw_block(4)
#         self.gate = nn.Parameter(torch.tensor([0.5, 0.3, 0.2]), requires_grad=True)  # learnable mixture
#         self.se1d = nn.Sequential(
#             nn.Conv1d(d_tok, max(8, d_tok//8), 1), nn.GELU(),
#             nn.Conv1d(max(8, d_tok//8), d_tok, 1), nn.Sigmoid()
#         )
#         self.drop = nn.Dropout(pdrop)

#         # Light SAB over time
#         self.pos = nn.Parameter(torch.randn(1, self.k_hid, d_tok) / math.sqrt(d_tok))
#         self.sab = SAB(d_model=d_tok, n_heads=sab_heads, pdrop=pdrop, num_layers=sab_layers)
#         self.pma = PMA(d_model=d_tok, num_seeds=K, n_heads=sab_heads, pdrop=pdrop)

#         # stats head size: FFT (4 bands) + dyn6
#         self.out = nn.Sequential(
#             nn.Linear(K * d_tok + 10, 2 * d_tok), nn.GELU(), nn.Dropout(pdrop),
#             nn.Linear(2 * d_tok, D_HID)
#         )

#     @torch.no_grad()
#     def _dyn_fft_stats(self, seq_mean: torch.Tensor) -> torch.Tensor:
#         """
#         seq_mean: (B,k) fp32 (mean over channels)
#         Returns 10 dims: [Î¼, ÏƒÂ², TV, p90|Î”|, early-late, last, P_low, P_mid, P_high, P_vhigh]
#         """
#         B, k = seq_mean.shape
#         mean = seq_mean.mean(dim=-1, keepdim=True)
#         var  = seq_mean.var(dim=-1, unbiased=False, keepdim=True)
#         if k >= 2:
#             d    = seq_mean[:, 1:] - seq_mean[:, :-1]
#             tv   = d.abs().sum(dim=-1, keepdim=True)
#             p90d = percentile(d.abs(), 0.90, dim=-1, keepdim=True)
#         else:
#             tv   = torch.zeros(B, 1, device=seq_mean.device)
#             p90d = torch.zeros_like(tv)
#         w  = max(1, k // 3)
#         el = (seq_mean[:, :w].mean(dim=-1, keepdim=True) - seq_mean[:, -w:].mean(dim=-1, keepdim=True))
#         last = seq_mean[:, -1:].contiguous()

#         # FFT bands (very coarse)
#         Xf = torch.fft.rfft(seq_mean, dim=-1)
#         P  = (Xf.real**2 + Xf.imag**2)
#         M = P.shape[-1]
#         q1, q2, q3 = M//4, M//2, 3*M//4
#         P_low  = P[:, :q1].mean(dim=-1, keepdim=True)
#         P_mid  = P[:, q1:q2].mean(dim=-1, keepdim=True)
#         P_high = P[:, q2:q3].mean(dim=-1, keepdim=True)
#         P_vhi  = P[:, q3:].mean(dim=-1, keepdim=True)

#         return torch.cat([mean, var, tv, p90d, el, last, P_low, P_mid, P_high, P_vhi], dim=-1)

#     def forward(self, last_hidden: torch.Tensor, mask_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:
#         """
#         last_hidden: (B, S, D_model)
#         mask_tokens: (B, S) with 1=valid
#         """
#         B, S, _ = last_hidden.shape
#         x = self.proj(self.norm(last_hidden))      # (B,S,D)
#         x = x.permute(0, 2, 1).contiguous()        # (B,D,S)

#         if mask_tokens is None:
#             m = torch.ones(B, 1, S, device=last_hidden.device, dtype=last_hidden.dtype)
#         else:
#             m = mask_tokens.detach().unsqueeze(1).to(last_hidden.dtype)

#         with no_amp_fp32(True):
#             x_ds = masked_area_downsample_1d(x.to(torch.float32), m.to(torch.float32), self.k_hid)  # (B,D,k)
#         x_ds = x_ds.to(x.dtype)

#         # Dilated depthwise conv mixture (no fp32 cast here)
#         y1 = self.dw1(x_ds); y2 = self.dw2(x_ds); y3 = self.dw3(x_ds)
#         g  = torch.softmax(self.gate, dim=0)
#         z  = g[0]*y1 + g[1]*y2 + g[2]*y3
#         z  = z * self.se1d(z)
#         z  = self.drop(z + x_ds)

#         # SAB over time
#         tok = z.permute(0, 2, 1).contiguous()                  # (B,k,D)
#         tok = tok + self.pos.to(tok.dtype)                     # dtype-safe pos
#         tok = self.sab(tok)
#         pooled = self.pma(tok).flatten(1)                      # (B,K*D)

#         # stats on channel-mean
#         with no_amp_fp32(True):
#             seq_mean = tok.to(torch.float32).mean(dim=-1)      # (B,k)
#             stats = self._dyn_fft_stats(seq_mean)              # (B,10)

#         vec = torch.cat([pooled, stats.to(pooled.dtype)], dim=-1)
#         return self.out(vec)   # (B, D_HID)

# # ----------------------------
# # ConfidenceFeatureExtractorLite
# # ----------------------------
# def _logit_clamped(x: torch.Tensor, eps: float = 1e-4) -> torch.Tensor:
#     x = x.clamp(eps, 1 - eps)
#     return torch.log(x) - torch.log1p(-x)

# class ConfFeatureExtractorLite(nn.Module):
#     """
#     Input:
#       conf         : (B, S) in [0,1]
#       mask_tokens  : (B, S) with 1=valid (optional)

#     Outputs:
#       z_conf : (B, D_CONF)

#     Design:
#       1) Multi-branch temporal encoding (raw, derivative, logit) â†’ mask-aware downsample to k
#       2) Gated dilated depthwise conv mixture (k-local patterns)
#       3) Light SAB (global mixing over time) + PMA pooling (K seeds)
#       4) Rich stats head (trend/volatility, drawdown, peaks, percentiles, FFT bands, spectral entropy)
#     """
#     def __init__(
#         self,
#         D_CONF: int = 384,
#         d_tok: int = 128,
#         k_conf: int = 192,
#         base_c: int = 64,
#         K: int = 3,
#         sab_layers: int = 1,
#         sab_heads: int = 4,
#         pdrop: float = 0.10,
#     ):
#         super().__init__()
#         self.k_conf = int(k_conf)
#         self.d_tok  = int(d_tok)

#         # channelize: [raw, d(raw), logit(raw)]
#         self.stem = nn.Conv1d(3, base_c, kernel_size=5, padding=2)
#         self.gn0  = nn.GroupNorm(8, base_c)

#         # dilated depthwise conv mixture
#         g = min(8, base_c)
#         def dw(dil):
#             return nn.Sequential(
#                 nn.Conv1d(base_c, base_c, 5, padding=2*dil, dilation=dil, groups=g),
#                 nn.GroupNorm(8, base_c), nn.GELU(),
#             )
#         self.dw1 = dw(1)
#         self.dw2 = dw(2)
#         self.dw3 = dw(4)
#         self.mix_gate = nn.Parameter(torch.tensor([0.5, 0.3, 0.2]), requires_grad=True)

#         # squeeze-excite gating
#         hid = max(8, base_c // 8)
#         self.se = nn.Sequential(nn.Conv1d(base_c, hid, 1), nn.GELU(), nn.Conv1d(hid, base_c, 1), nn.Sigmoid())

#         # project channelsâ†’tokens
#         self.proj_tok = nn.Conv1d(base_c, d_tok, kernel_size=1)

#         # light set-transformer over time + PMA pooling
#         self.pos = nn.Parameter(torch.randn(1, self.k_conf, d_tok) / math.sqrt(d_tok))
#         self.sab = SAB(d_model=d_tok, n_heads=sab_heads, pdrop=pdrop, num_layers=sab_layers)
#         self.pma = PMA(d_model=d_tok, num_seeds=K, n_heads=sab_heads, pdrop=pdrop)

#         # stats head: (Î¼, ÏƒÂ², TV, p90|Î”|, slope, R2, max_drawdown, #peaks,
#         #              p50/p70/p90 mass, FFT low/mid/high/vhigh, spectral entropy) = 14
#         self.out = nn.Sequential(
#             nn.Linear(K * d_tok + 14, 2 * d_tok), nn.GELU(), nn.Dropout(pdrop),
#             nn.Linear(2 * d_tok, D_CONF)
#         )

#     @torch.no_grad()
#     def _rich_stats(self, x: torch.Tensor) -> torch.Tensor:
#         """
#         x: (B, k) fp32  (downsampled confidence)
#         Returns 14-dim vector per batch item.
#         """
#         B, k = x.shape
#         mean = x.mean(dim=-1, keepdim=True)
#         var  = x.var(dim=-1, unbiased=False, keepdim=True)

#         # derivatives
#         dx = x[:, 1:] - x[:, :-1] if k >= 2 else torch.zeros(B, 0, device=x.device)
#         tv  = dx.abs().sum(dim=-1, keepdim=True) if dx.numel() else torch.zeros(B, 1, device=x.device)
#         p90d= percentile(dx.abs(), 0.90, dim=-1, keepdim=True) if dx.numel() else torch.zeros(B, 1, device=x.device)

#         # linear trend (OLS slope + R^2)
#         t = torch.arange(k, device=x.device, dtype=x.dtype).unsqueeze(0)  # (1,k)
#         t = t - t.mean(dim=-1, keepdim=True)
#         denom = (t**2).sum(dim=-1, keepdim=True) + 1e-9
#         slope = (t * (x - mean)).sum(dim=-1, keepdim=True) / denom
#         corr = ((t * (x - mean)).sum(dim=-1) / (torch.sqrt(denom.squeeze(-1) * (var.squeeze(-1) * k + 1e-9)))).clamp(-1,1)
#         r2   = (corr**2).unsqueeze(-1)

#         # max drawdown (peakâ†’trough drop)
#         run_max = torch.cummax(x, dim=-1).values
#         drawdown = (run_max - x).amax(dim=-1, keepdim=True)

#         # rough peak count
#         if dx.numel():
#             th = 0.02
#             up = (dx >  th).to(x.dtype)
#             dn = (dx < -th).to(x.dtype)
#             peaks = ((up[:, :-1] * dn[:, 1:]) > 0).sum(dim=-1, keepdim=True) if dx.size(1) >= 2 else torch.zeros(B,1,device=x.device)
#         else:
#             peaks = torch.zeros(B,1,device=x.device)

#         # mass above thresholds
#         p50 = (x >= 0.50).float().mean(dim=-1, keepdim=True)
#         p70 = (x >= 0.70).float().mean(dim=-1, keepdim=True)
#         p90 = (x >= 0.90).float().mean(dim=-1, keepdim=True)

#         # FFT band powers + spectral entropy
#         Xf = torch.fft.rfft(x, dim=-1)
#         P  = (Xf.real**2 + Xf.imag**2) + 1e-12
#         M  = P.shape[-1]
#         q1, q2, q3 = M//4, M//2, 3*M//4
#         Pl = P[:, :q1].mean(dim=-1, keepdim=True)
#         Pm = P[:, q1:q2].mean(dim=-1, keepdim=True)
#         Ph = P[:, q2:q3].mean(dim=-1, keepdim=True)
#         Pv = P[:, q3:].mean(dim=-1, keepdim=True)
#         Pn = P / P.sum(dim=-1, keepdim=True)
#         sent = (-(Pn * Pn.log()).sum(dim=-1, keepdim=True))

#         return torch.cat([mean, var, tv, p90d, slope, r2, drawdown, peaks, p50, p70, p90, Pl, Pm, Ph + Pv, sent], dim=-1)[:, :14]

#     def forward(self, conf: torch.Tensor, mask_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:
#         """
#         conf: (B, S) in [0,1]
#         """
#         B, S = conf.shape
#         device = conf.device
#         dtype  = conf.dtype

#         # build channels: raw, derivative, logit
#         raw = conf
#         d   = F.pad(conf[:, 1:] - conf[:, :-1], (1,0)) if S >= 2 else torch.zeros_like(conf)
#         lg  = _logit_clamped(conf.to(torch.float32)).to(dtype)

#         x = torch.stack([raw, d, lg], dim=1)  # (B,3,S)
#         if mask_tokens is None:
#             m = torch.ones(B, 1, S, device=device, dtype=dtype)
#         else:
#             m = mask_tokens.detach().unsqueeze(1).to(dtype)

#         # mask-aware downsample to k_conf (fp32 compute; cast back)
#         x = masked_area_downsample_1d(x.to(torch.float32), m.to(torch.float32), self.k_conf).to(dtype)  # (B,3,k)

#         # stem + dilated mixture (NO fp32 casts around learned layers)
#         z  = F.gelu(self.gn0(self.stem(x)))            # (B,C,k)
#         y1 = self.dw1(z); y2 = self.dw2(z); y3 = self.dw3(z)
#         g  = torch.softmax(self.mix_gate, dim=0)
#         y  = g[0]*y1 + g[1]*y2 + g[2]*y3
#         y  = y * self.se(y) + z                        # SE gating + residual

#         # tokens over time â†’ SAB â†’ PMA
#         tok = self.proj_tok(y).permute(0,2,1).contiguous()
#         tok = tok + self.pos.to(tok.dtype)             # dtype-safe pos
#         tok = self.sab(tok)
#         pooled = self.pma(tok).flatten(1)              # (B, K*d_tok)

#         # stats head (fp32 compute; cast back)
#         stats = self._rich_stats(tok.mean(dim=-1).to(torch.float32)).to(pooled.dtype)  # (B,14)

#         vec = torch.cat([pooled, stats], dim=-1)
#         return self.out(vec)  # (B, D_CONF)

# # ----------------------------
# # Correctness head (tri-modal)
# # ----------------------------
# class CorrectnessHeadLite(nn.Module):
#     """
#     Tri-modal fusion with learned modality gates (attention / confidence / hidden).
#     """
#     def __init__(self, D_ATT: int, D_CONF: int, D_HID: int,
#                  use_attn=True, use_conf=True, use_hid=True, pdrop: float = 0.10):
#         super().__init__()
#         self.use_attn, self.use_conf, self.use_hid = use_attn, use_conf, use_hid

#         dims = []
#         if use_attn: dims.append(D_ATT)
#         if use_conf: dims.append(D_CONF)
#         if use_hid:  dims.append(D_HID)
#         D = sum(dims)
#         if D == 0: raise ValueError("Enable at least one modality.")

#         self.g_att = nn.Sequential(nn.LayerNorm(D_ATT), nn.Linear(D_ATT, 1)) if use_attn else None
#         self.g_con = nn.Sequential(nn.LayerNorm(D_CONF), nn.Linear(D_CONF, 1)) if use_conf else None
#         self.g_hid = nn.Sequential(nn.LayerNorm(D_HID),  nn.Linear(D_HID,  1)) if use_hid  else None

#         self.ln = nn.LayerNorm(D)
#         self.mlp = nn.Sequential(
#             nn.Linear(D, 384), nn.GELU(), nn.Dropout(pdrop),
#             nn.Linear(384, 128), nn.GELU(), nn.Dropout(pdrop),
#             nn.Linear(128, 1),
#         )

#     def forward(self, z_att: Optional[torch.Tensor], z_conf: Optional[torch.Tensor], z_hid: Optional[torch.Tensor]):
#         chunks, gates = [], []
#         if self.use_attn: chunks.append(z_att);  gates.append(self.g_att(z_att))
#         if self.use_conf: chunks.append(z_conf); gates.append(self.g_con(z_conf))
#         if self.use_hid:  chunks.append(z_hid);  gates.append(self.g_hid(z_hid))

#         g = torch.softmax(torch.cat(gates, dim=-1), dim=-1)  # (B, n_mod)
#         out_slices = []
#         for i, ch in enumerate(chunks):
#             out_slices.append(ch * g[:, i:i+1])
#         x = torch.cat(out_slices, dim=-1)
#         return self.mlp(self.ln(x))  # (B,1)


# logger = logging.get_logger(__name__)

# _CHECKPOINT_FOR_DOC = "Qwen/Qwen3-8B"
# _CONFIG_FOR_DOC = "Qwen3Config"


# def assert_finite(**named_tensors):
#     """Raise if any tensor contains NaN/Inf."""
#     for k, v in named_tensors.items():
#         if not torch.all(torch.isfinite(v)):
#             raise FloatingPointError(f"{k} has NaN/Inf")


# #myedit
# @dataclass
# class CausalLMOutputWithPast(ModelOutput):
#     """
#     Outputs for causal language modeling with an optional stop-probability head.

#     Args:
#         loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
#             Training loss (e.g., LM loss or stop-head loss).
#         logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
#             Pre-softmax scores from the LM head.
#         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):
#             A nested tuple with length `config.num_hidden_layers`. Each inner tuple contains
#             key/value tensors of shape `(batch_size, num_heads, seq_len, head_dim)`.
#             Can be fed to speed up sequential decoding.
#         hidden_states (`tuple(torch.FloatTensor)`, *optional*):
#             Tuple of hidden states (output of embeddings + each layer), each of shape
#             `(batch_size, sequence_length, hidden_size)`.
#         attentions (`tuple(torch.FloatTensor)`, *optional*):
#             Tuple of attention maps for each layer, each of shape
#             `(batch_size, num_heads, sequence_length, sequence_length)`.
#         stop_prob (`torch.FloatTensor` of shape `(batch_size, sequence_length)` in train
#             or `(batch_size, 1, 1)` at inference, *optional*):
#             Probability from the stop head indicating that thinking should end at each position.
#     """
#     loss: Optional[torch.FloatTensor] = None
#     logits: torch.FloatTensor = None
#     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, ...], ...]] = None
#     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
#     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
#     stop_prob: Optional[torch.FloatTensor] = None



# class Qwen3RMSNorm(nn.Module):
#     def __init__(self, hidden_size, eps=1e-6):
#         """
#         Qwen3RMSNorm is equivalent to T5LayerNorm
#         """
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(hidden_size))
#         self.variance_epsilon = eps

#     def forward(self, hidden_states):
#         input_dtype = hidden_states.dtype
#         hidden_states = hidden_states.to(torch.float32)
#         variance = hidden_states.pow(2).mean(-1, keepdim=True)
#         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
#         return self.weight * hidden_states.to(input_dtype)

#     def extra_repr(self):
#         return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


# class Qwen3MLP(nn.Module):
#     def __init__(self, config):
#         super().__init__()
#         self.config = config
#         self.hidden_size = config.hidden_size
#         self.intermediate_size = config.intermediate_size
#         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
#         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
#         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
#         self.act_fn = ACT2FN[config.hidden_act]

#     def forward(self, x):
#         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
#         return down_proj


# def rotate_half(x):
#     """Rotates half the hidden dims of the input."""
#     x1 = x[..., : x.shape[-1] // 2]
#     x2 = x[..., x.shape[-1] // 2 :]
#     return torch.cat((-x2, x1), dim=-1)


# def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
#     """Applies Rotary Position Embedding to the query and key tensors.

#     Args:
#         q (`torch.Tensor`): The query tensor.
#         k (`torch.Tensor`): The key tensor.
#         cos (`torch.Tensor`): The cosine part of the rotary embedding.
#         sin (`torch.Tensor`): The sine part of the rotary embedding.
#         position_ids (`torch.Tensor`, *optional*):
#             Deprecated and unused.
#         unsqueeze_dim (`int`, *optional*, defaults to 1):
#             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
#             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
#             that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
#             k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
#             cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
#             the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
#     Returns:
#         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
#     """
#     cos = cos.unsqueeze(unsqueeze_dim)
#     sin = sin.unsqueeze(unsqueeze_dim)
#     q_embed = (q * cos) + (rotate_half(q) * sin)
#     k_embed = (k * cos) + (rotate_half(k) * sin)
#     return q_embed, k_embed


# def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
#     """
#     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
#     num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
#     """
#     batch, num_key_value_heads, slen, head_dim = hidden_states.shape
#     if n_rep == 1:
#         return hidden_states
#     hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
#     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


# ## ***************************************************************************************************
# def eager_attention_forward(
#     module: nn.Module,
#     query: torch.Tensor,
#     key: torch.Tensor,
#     value: torch.Tensor,
#     attention_mask: Optional[torch.Tensor],
#     scaling: float,
#     dropout: float = 0.0,
#     **kwargs,
# ):
#     key_states = repeat_kv(key, module.num_key_value_groups)
#     value_states = repeat_kv(value, module.num_key_value_groups)

#     attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

#     if attention_mask is not None:
#         # Accept 2D [B,S] or 4D [B,1,Q,K]
#         if attention_mask.dim() == 2:
#             # 0/1 or bool -> additive mask with -inf for masked keys
#             m = (attention_mask != 0).to(attn_weights.dtype)       # [B,S]
#             add = (1.0 - m) * torch.finfo(attn_weights.dtype).min  # [B,S]
#             attention_mask = add[:, None, None, :]                 # [B,1,1,S]
#     if attention_mask is not None:
#         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
#         attn_weights = attn_weights + causal_mask

#     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
#     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
#     attn_output = torch.matmul(attn_weights, value_states)
#     attn_output = attn_output.transpose(1, 2).contiguous()

#     return attn_output, attn_weights



# # # # Faster memory efficient eager attention, inference only
# # def _build_additive_mask(attention_mask, B, Q, K, dtype, device):
# #     if attention_mask is None:
# #         return None
# #     if attention_mask.dim() == 2:                   # [B, S]
# #         m   = (attention_mask != 0)
# #         add = (~m).to(dtype) * torch.finfo(dtype).min
# #         return add[:, None, None, :].expand(B, 1, Q, K).to(device)
# #     # assume broadcastable [B,1,Q,K] or [B,H,Q,K]
# #     return attention_mask[..., :K].to(dtype).to(device)

# # def eager_attention_forward(
# #     module: nn.Module,
# #     query: torch.Tensor,           # [B, Hq, Q, D]
# #     key: torch.Tensor,             # [B, Hkv, K, D]
# #     value: torch.Tensor,           # [B, Hkv, K, Dv]
# #     attention_mask: Optional[torch.Tensor],
# #     scaling: float,
# #     dropout: float = 0.0,
# #     *,
# #     q_chunk_size: int = 2048,
# #     float32_softmax: bool = False,
# #     # Optional streaming hook to avoid holding full weights in memory:
# #     #   def weights_cb(q_start:int, q_end:int, w_chunk:Tensor[B,H,Qc,K]) -> None
# #     weights_cb: Optional[Callable[[int,int,torch.Tensor], None]] = None,
# #     **kwargs,
# # ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
# #     """
# #     Returns:
# #       attn_output: [B, Q, H, Dv]
# #       attn_weights: [B, H, Q, K]  (or None if using weights_cb to stream)
# #     Exactness:
# #       - With float32_softmax=True and dropout==0, matches eager softmax math.
# #       - With dropout>0, semantics match, bitwise may differ due to RNG order.
# #     """

# #     # Expand K/V heads to match Q heads
# #     key_states   = repeat_kv(key,   module.num_key_value_groups)    # [B,H,K,D]
# #     value_states = repeat_kv(value, module.num_key_value_groups)    # [B,H,K,Dv]
# #     B, H, Q, D   = query.shape
# #     K            = key_states.shape[-2]
# #     Dv           = value_states.shape[-1]
# #     dev          = query.device

# #     # Choose math dtype for softmax (float32 to mirror your original)
# #     sm_dtype = torch.float32 if float32_softmax else query.dtype

# #     # Build additive mask once; slice per Q-chunk
# #     add_mask = _build_additive_mask(attention_mask, B, Q, K, sm_dtype, dev)

# #     # Pre-transpose K for faster GEMM: [B,H,D,K]
# #     kT = key_states.transpose(-2, -1).contiguous()

# #     outs = []
# #     weight_chunks = [] if weights_cb is None else None

# #     training = module.training and (dropout > 0.0)
# #     p_drop   = dropout if training else 0.0

# #     for qs in range(0, Q, q_chunk_size):
# #         qe = min(qs + q_chunk_size, Q)

# #         q_blk = query[:, :, qs:qe, :].to(sm_dtype)      # [B,H,Qc,D]
# #         # scores: [B,H,Qc,K]
# #         s_blk = torch.matmul(q_blk, kT) * scaling
# #         if add_mask is not None:
# #             s_blk = s_blk + add_mask[:, :, qs:qe, :]

# #         # softmax in chosen dtype (float32 for stability/exactness)
# #         w_blk = F.softmax(s_blk, dim=-1)                # [B,H,Qc,K] in sm_dtype

# #         # dropout on probs (efficient and fused per chunk)
# #         if p_drop > 0.0:
# #             w_blk = F.dropout(w_blk, p=p_drop, training=True)

# #         # output chunk: [B,H,Qc,Dv]
# #         v_full = value_states.to(sm_dtype)
# #         out_blk = torch.matmul(w_blk, v_full)

# #         outs.append(out_blk.to(query.dtype))            # keep model dtype for outputs

# #         # collect / stream weights (cast at end)
# #         if weights_cb is None:
# #             weight_chunks.append(w_blk)                 # keep in sm_dtype for now
# #         else:
# #             weights_cb(qs, qe, w_blk)                   # user handles storage; free memory

# #         # free large temps early
# #         del q_blk, s_blk, out_blk

# #     # concat outputs â†’ [B,H,Q,Dv] â†’ [B,Q,H,Dv]
# #     attn_output = torch.cat(outs, dim=2).transpose(1, 2).contiguous()

# #     # assemble weights if requested in-memory
# #     attn_weights = None
# #     if weights_cb is None:
# #         attn_weights = torch.cat(weight_chunks, dim=2).to(query.dtype).contiguous()  # [B,H,Q,K]

# #     return attn_output, attn_weights

# # #***************************************************************************************************

# class Qwen3Attention(nn.Module):
#     """Multi-headed attention from 'Attention Is All You Need' paper"""

#     def __init__(self, config: Qwen3Config, layer_idx: int):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx
#         self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
#         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
#         self.scaling = self.head_dim**-0.5
#         self.attention_dropout = config.attention_dropout
#         self.is_causal = True

#         self.q_proj = nn.Linear(
#             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.k_proj = nn.Linear(
#             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.v_proj = nn.Linear(
#             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.o_proj = nn.Linear(
#             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
#         )
#         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
#         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
#         self.sliding_window = config.sliding_window
#         if not (
#             self.config.use_sliding_window
#             and getattr(self.config, "sliding_window", None) is not None
#             and self.layer_idx >= self.config.max_window_layers
#         ):
#             self.sliding_window = None

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         position_embeddings: Tuple[torch.Tensor, torch.Tensor],
#         attention_mask: Optional[torch.Tensor],
#         past_key_value: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **kwargs: Unpack[FlashAttentionKwargs],
#     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         input_shape = hidden_states.shape[:-1]
#         hidden_shape = (*input_shape, -1, self.head_dim)

#         query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
#         key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
#         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

#         cos, sin = position_embeddings
#         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

#         if past_key_value is not None:
#             # sin and cos are specific to RoPE models; cache_position needed for the static cache
#             cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
#             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

#         #myedit
#         # attention_interface: Callable = eager_attention_forward
#         # if self.config._attn_implementation != "eager":
#         #     if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
#         #         logger.warning_once(
#         #             "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
#         #             'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
#         #         )
#         #     else:
#         #         attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
#         # --- choose kernel -------------------------------------------------------
#         want_weights = kwargs.get("output_attentions", False)

#         if want_weights:                       # <â”€â”€ we need A = softmax(qkáµ€/âˆšd)
#             attention_interface = eager_attention_forward   # eager can return it
#         else:                                   # keep the fast kernel
#             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]


#         attn_output, attn_weights = attention_interface(
#             self,
#             query_states,
#             key_states,
#             value_states,
#             attention_mask,
#             dropout=0.0 if not self.training else self.attention_dropout,
#             scaling=self.scaling,
#             sliding_window=self.sliding_window,  # diff with Llama
#             **kwargs,
#         )

#         attn_output = attn_output.reshape(*input_shape, -1).contiguous()
#         attn_output = self.o_proj(attn_output)
#         return attn_output, attn_weights





# class Qwen3DecoderLayer(nn.Module):
#     def __init__(self, config: Qwen3Config, layer_idx: int):
#         super().__init__()
#         self.hidden_size = config.hidden_size
#         self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)
#         self.mlp = Qwen3MLP(config)
#         self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         if (
#             config.sliding_window and config._attn_implementation != "flash_attention_2"
#         ):  # diff with Llama is this warning
#             logger.warning_once(
#                 f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
#                 "unexpected results may be encountered."
#             )

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_value: Optional[Cache] = None,
#         output_attentions: Optional[bool] = False,
#         use_cache: Optional[bool] = False,
#         cache_position: Optional[torch.LongTensor] = None,
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
#         **kwargs: Unpack[FlashAttentionKwargs],
#     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
#         residual = hidden_states

#         hidden_states = self.input_layernorm(hidden_states)

#         # Self Attention
#         hidden_states, self_attn_weights = self.self_attn(
#             hidden_states=hidden_states,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_value=past_key_value,
#             output_attentions=output_attentions,
#             use_cache=use_cache,
#             cache_position=cache_position,
#             position_embeddings=position_embeddings,
#             **kwargs,
#         )
#         hidden_states = residual + hidden_states

#         # Fully Connected
#         residual = hidden_states
#         hidden_states = self.post_attention_layernorm(hidden_states)
#         hidden_states = self.mlp(hidden_states)
#         hidden_states = residual + hidden_states

#         outputs = (hidden_states,)

#         # #myedit
#         # if output_attentions and self_attn_weights is not None:
#         #     # (B , n_heads , S , S) â†’ (B , 1 , S , S)
#         #     self_attn_weights = self_attn_weights.mean(dim=1, keepdim=True)

#         if output_attentions:
#             outputs += (self_attn_weights,)

#         return outputs


# class Qwen3RotaryEmbedding(nn.Module):
#     def __init__(self, config: Qwen3Config, device=None):
#         super().__init__()
#         # BC: "rope_type" was originally "type"
#         if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
#             self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
#         else:
#             self.rope_type = "default"
#         self.max_seq_len_cached = config.max_position_embeddings
#         self.original_max_seq_len = config.max_position_embeddings

#         self.config = config
#         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

#         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
#         self.register_buffer("inv_freq", inv_freq, persistent=False)
#         self.original_inv_freq = self.inv_freq

#     @torch.no_grad()
#     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
#     def forward(self, x, position_ids):
#         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
#         position_ids_expanded = position_ids[:, None, :].float()

#         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
#         with torch.autocast(device_type=device_type, enabled=False):  # Force float32
#             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
#             emb = torch.cat((freqs, freqs), dim=-1)
#             cos = emb.cos() * self.attention_scaling
#             sin = emb.sin() * self.attention_scaling

#         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


# QWEN3_START_DOCSTRING = r"""
#     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
#     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
#     etc.)

#     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
#     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
#     and behavior.

#     Parameters:
#         config ([`Qwen3Config`]):
#             Model configuration class with all the parameters of the model. Initializing with a config file does not
#             load the weights associated with the model, only the configuration. Check out the
#             [`~PreTrainedModel.from_pretrained`] method to load the model weights.
# """


# @add_start_docstrings(
#     "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3PreTrainedModel(PreTrainedModel):
#     config_class = Qwen3Config
#     base_model_prefix = "model"
#     supports_gradient_checkpointing = True
#     _no_split_modules = ["Qwen3DecoderLayer"]
#     _skip_keys_device_placement = ["past_key_values"]
#     _supports_flash_attn_2 = True
#     _supports_sdpa = True
#     _supports_flex_attn = True
#     _supports_cache_class = True
#     _supports_quantized_cache = True
#     _supports_static_cache = True
#     _supports_attention_backend = True

#     def _init_weights(self, module):
#         std = self.config.initializer_range
#         if isinstance(module, nn.Linear):
#             module.weight.data.normal_(mean=0.0, std=std)
#             if module.bias is not None:
#                 module.bias.data.zero_()
#         elif isinstance(module, nn.Embedding):
#             module.weight.data.normal_(mean=0.0, std=std)
#             if module.padding_idx is not None:
#                 module.weight.data[module.padding_idx].zero_()



# QWEN3_INPUTS_DOCSTRING = r"""
#     Args:
#         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
#             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
#             it.

#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
#             [`PreTrainedTokenizer.__call__`] for details.

#             [What are input IDs?](../glossary#input-ids)
#         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
#             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

#             - 1 for tokens that are **not masked**,
#             - 0 for tokens that are **masked**.

#             [What are attention masks?](../glossary#attention-mask)

#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
#             [`PreTrainedTokenizer.__call__`] for details.

#             If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
#             `past_key_values`).

#             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
#             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
#             information on the default strategy.

#             - 1 indicates the head is **not masked**,
#             - 0 indicates the head is **masked**.
#         position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
#             config.n_positions - 1]`.

#             [What are position IDs?](../glossary#position-ids)
#         past_key_values (`Cache`, *optional*):
#             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
#             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
#             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

#             It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

#             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
#             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
#             of shape `(batch_size, sequence_length)`.
#         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
#             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
#             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
#             model's internal embedding lookup matrix.
#         use_cache (`bool`, *optional*):
#             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
#             `past_key_values`).
#         output_attentions (`bool`, *optional*):
#             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
#             tensors for more detail.
#         output_hidden_states (`bool`, *optional*):
#             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
#             more detail.
#         return_dict (`bool`, *optional*):
#             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
#         cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
#             Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
#             this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
#             the complete sequence length.
# """


# @add_start_docstrings(
#     "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3Model(Qwen3PreTrainedModel):
#     """
#     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Qwen3DecoderLayer`]

#     Args:
#         config: Qwen3Config
#     """

#     def __init__(self, config: Qwen3Config):
#         super().__init__(config)
#         self.padding_idx = config.pad_token_id
#         self.vocab_size = config.vocab_size

#         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
#         self.layers = nn.ModuleList(
#             [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
#         )
#         self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         self.rotary_emb = Qwen3RotaryEmbedding(config=config)
#         self.gradient_checkpointing = False

#         # --- added: target reduced attention grid size (kÃ—k) ---
#         self.k_att = int(getattr(config, "stop_k_att", 48))

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.embed_tokens

#     def set_input_embeddings(self, value):
#         self.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
#     ) -> BaseModelOutputWithPast:
#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
#         output_hidden_states = (
#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
#         )
#         use_cache = use_cache if use_cache is not None else self.config.use_cache

#         if (input_ids is None) ^ (inputs_embeds is not None):
#             raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

#         if self.gradient_checkpointing and self.training and use_cache:
#             logger.warning_once(
#                 "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
#             )
#             use_cache = False

#         # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
#         if not isinstance(past_key_values, (type(None), Cache)):
#             raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

#         if inputs_embeds is None:
#             inputs_embeds = self.embed_tokens(input_ids)

#         if use_cache and past_key_values is None:
#             past_key_values = DynamicCache()

#         if cache_position is None:
#             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
#             cache_position = torch.arange(
#                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
#             )

#         if position_ids is None:
#             position_ids = cache_position.unsqueeze(0)

#         causal_mask = self._update_causal_mask(
#             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
#         )

#         hidden_states = inputs_embeds

#         # create position embeddings to be shared across the decoder layers
#         position_embeddings = self.rotary_emb(hidden_states, position_ids)

#         # decoder layers
#         all_hidden_states = () if output_hidden_states else None
#         all_self_attns = () if output_attentions else None

#         # â”€â”€â”€ main layer loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#         for idx, decoder_layer in enumerate(self.layers):
#             need_attn = output_attentions and (idx % 5 == 4)
#             if output_hidden_states:
#                 all_hidden_states += (hidden_states,)
#             # print(causal_mask.shape)
#             if self.gradient_checkpointing and self.training:
#                 layer_outputs = self._gradient_checkpointing_func(
#                     partial(decoder_layer.__call__, **flash_attn_kwargs),
#                     hidden_states,
#                     causal_mask,
#                     position_ids,
#                     past_key_values,
#                     # output_attentions,
#                     need_attn,
#                     use_cache,
#                     cache_position,
#                     position_embeddings,
#                 )
#             else:
#                 layer_outputs = decoder_layer(
#                     hidden_states,
#                     attention_mask=causal_mask,
#                     position_ids=position_ids,
#                     past_key_value=past_key_values,
#                     output_attentions=output_attentions,
#                     use_cache=use_cache,
#                     cache_position=cache_position,
#                     position_embeddings=position_embeddings,
#                     **flash_attn_kwargs,
#                 )

#             hidden_states = layer_outputs[0]

#             if need_attn:
#                 # (B, H, S, S)
#                 attn_full = layer_outputs[1]
#                 # all_self_attns += (attn_full,)

#                 # --- added: reduction logic (mask-aware area downsample to kÃ—k) ---
#                 with torch.no_grad():
#                     # downsample to (B, H, k, k) in fp32 for numerical stability
#                     A_small = F.interpolate(attn_full.to(torch.float16), size=(self.k_att, self.k_att), mode="area")

#                     # if a 2D attention_mask is provided (B, S), build a (B,1,k,k) grid mask and apply
#                     if attention_mask is not None and attention_mask.dim() == 2:
#                         mt = attention_mask.to(A_small.dtype).unsqueeze(1)  # (B,1,S)
#                         m_ds = F.interpolate(mt, size=self.k_att, mode="area").clamp_(0, 1)  # (B,1,k)
#                         grid = torch.bmm(m_ds.transpose(1, 2), m_ds).unsqueeze(1)            # (B,1,k,k)
#                         A_small = A_small * grid                                             # broadcast over H

#                     # sanitize
#                     A_small = A_small.clamp_min_(0)

#                 # free big tensors ASAP
#                 del attn_full
#                 all_self_attns += (A_small.to(hidden_states.dtype).detach(),)

#         hidden_states = self.norm(hidden_states)

#         # add hidden states from the last decoder layer
#         if output_hidden_states:
#             all_hidden_states += (hidden_states,)

#         return BaseModelOutputWithPast(
#             last_hidden_state=hidden_states,
#             past_key_values=past_key_values if use_cache else None,
#             hidden_states=all_hidden_states,
#             attentions=all_self_attns,
#         )

#     def _update_causal_mask(
#         self,
#         attention_mask: torch.Tensor,
#         input_tensor: torch.Tensor,
#         cache_position: torch.Tensor,
#         past_key_values: Cache,
#         output_attentions: bool = False,
#     ):
#         if self.config._attn_implementation == "flash_attention_2":
#             if attention_mask is not None and past_key_values is not None:
#                 is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
#                 if is_padding_right:
#                     raise ValueError(
#                         "You are attempting to perform batched generation with padding_side='right'"
#                         " this may lead to unexpected behaviour for Flash Attention version of Qwen3. Make sure to "
#                         " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
#                     )
#             if attention_mask is not None and 0.0 in attention_mask:
#                 return attention_mask
#             return None

#         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in
#         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail
#         # to infer the attention mask.
#         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
#         using_static_cache = isinstance(past_key_values, StaticCache)
#         using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)

#         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward
#         if (
#             self.config._attn_implementation == "sdpa"
#             and not (using_static_cache or using_sliding_window_cache)
#             and not output_attentions
#         ):
#             if AttentionMaskConverter._ignore_causal_mask_sdpa(
#                 attention_mask,
#                 inputs_embeds=input_tensor,
#                 past_key_values_length=past_seen_tokens,
#                 sliding_window=self.config.sliding_window,
#                 is_training=self.training,
#             ):
#                 return None

#         dtype, device = input_tensor.dtype, input_tensor.device
#         min_dtype = torch.finfo(dtype).min
#         sequence_length = input_tensor.shape[1]
#         # SlidingWindowCache or StaticCache
#         if using_sliding_window_cache or using_static_cache:
#             target_length = past_key_values.get_max_cache_shape()
#         # DynamicCache or no cache
#         else:
#             target_length = (
#                 attention_mask.shape[-1]
#                 if isinstance(attention_mask, torch.Tensor)
#                 else past_seen_tokens + sequence_length + 1
#             )

#         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).
#         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
#             attention_mask,
#             sequence_length=sequence_length,
#             target_length=target_length,
#             dtype=dtype,
#             device=device,
#             cache_position=cache_position,
#             batch_size=input_tensor.shape[0],
#             config=self.config,
#             past_key_values=past_key_values,
#         )

#         if (
#             self.config._attn_implementation == "sdpa"
#             and attention_mask is not None
#             and attention_mask.device.type in ["cuda", "xpu"]
#             and not output_attentions
#         ):
#             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
#             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
#             # Details: https://github.com/pytorch/pytorch/issues/110213
#             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

#         return causal_mask

#     @staticmethod
#     def _prepare_4d_causal_attention_mask_with_cache_position(
#         attention_mask: torch.Tensor,
#         sequence_length: int,
#         target_length: int,
#         dtype: torch.dtype,
#         device: torch.device,
#         cache_position: torch.Tensor,
#         batch_size: int,
#         config: Qwen3Config,
#         past_key_values: Cache,
#     ):
#         """
#         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
#         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

#         Args:
#             attention_mask (`torch.Tensor`):
#                 A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
#             sequence_length (`int`):
#                 The sequence length being processed.
#             target_length (`int`):
#                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
#             dtype (`torch.dtype`):
#                 The dtype to use for the 4D attention mask.
#             device (`torch.device`):
#                 The device to place the 4D attention mask on.
#             cache_position (`torch.Tensor`):
#                 Indices depicting the position of the input sequence tokens in the sequence.
#             batch_size (`torch.Tensor`):
#                 Batch size.
#             config (`Qwen3Config`):
#                 The model's configuration class
#             past_key_values (`Cache`):
#                 The cache class that is being used currently to generate
#         """
#         if attention_mask is not None and attention_mask.dim() == 4:
#             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
#             causal_mask = attention_mask
#         else:
#             min_dtype = torch.finfo(dtype).min
#             causal_mask = torch.full(
#                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device
#             )
#             diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
#             if config.sliding_window is not None:
#                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also
#                 # the check is needed to verify is current checkpoint was trained with sliding window or not
#                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:
#                     sliding_attend_mask = torch.arange(target_length, device=device) <= (
#                         cache_position.reshape(-1, 1) - config.sliding_window
#                     )
#                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)
#             causal_mask *= diagonal_attend_mask
#             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
#             if attention_mask is not None:
#                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
#                 if attention_mask.shape[-1] > target_length:
#                     attention_mask = attention_mask[:, :target_length]
#                 mask_length = attention_mask.shape[-1]
#                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
#                     causal_mask.device
#                 )
#                 padding_mask = padding_mask == 0
#                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
#                     padding_mask, min_dtype
#                 )
#         return causal_mask



# class KwargsForCausalLM(FlashAttentionKwargs): ...



# # =========================
# # Model with togglable features
# # =========================
# class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
#     _tied_weights_keys = ["lm_head.weight"]
#     _tp_plan = {"lm_head": "colwise_rep"}
#     _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

#     def __init__(self, config):
#         super().__init__(config)
#         self.model = Qwen3Model(config)
#         self.vocab_size = config.vocab_size
#         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

#         # ---- feature toggles (simple booleans) ----
#         self.use_stop_attn = bool(getattr(config, "use_stop_attn", True))
#         self.use_stop_conf = bool(getattr(config, "use_stop_conf", False))
#         self.use_stop_hid  = bool(getattr(config, "use_stop_hid",  False))

#         # ---- dims (safe defaults) ----
#         D_ATT  = int(getattr(config, "stop_att_dim", 384))
#         D_CONF = int(getattr(config, "stop_conf_dim", 128))
#         D_HID  = int(getattr(config, "stop_hid_dim", 256))

#         k_conf = int(getattr(config, "stop_k_conf", 192))
#         k_hid  = int(getattr(config, "stop_k_hid", 192))

#         max_layers = int(getattr(config, "num_hidden_layers", 32))
#         max_heads  = int(getattr(config, "num_attention_heads", 32))


#         if self.use_stop_attn:
#             self.attn_extractor = AttnFeatureExtractorLite(
#                 D_ATT=D_ATT,
#                 d_tok=192,          # token dim for (layerÃ—head) tokens
#                 cnn_c=64,           # channels in CNN stems (s0/s1/s2)
#                 top_r=8,            # # of top singular values to keep
#                 K=4,                # PMA seeds
#                 max_layers=max_layers,
#                 max_heads=max_heads,
#                 sab_layers=3,       # SAB depth
#                 sab_heads=4,
#                 pdrop=0.10,
#             )
#         else:
#             self.attn_extractor = None

#         if self.use_stop_conf:
#             self.conf_extractor = ConfFeatureExtractorLite(
#                 D_CONF=D_CONF,
#                 d_tok=128,          # token dim over time
#                 k_conf=k_conf,      # downsampled time steps
#                 base_c=64,          # channels in 1D conv stem
#                 K=3,                # PMA seeds
#                 sab_layers=2,
#                 sab_heads=4,
#                 pdrop=0.10,
#             )
#         else:
#             self.conf_extractor = None

#         if self.use_stop_hid:
#             self.hid_extractor = HiddenFeatureExtractorLite(
#                 D_model=config.hidden_size,
#                 D_HID=D_HID,
#                 d_tok=192,          # token dim over time
#                 k_hid=k_hid,        # downsampled time steps
#                 groups=8,           # depthwise grouping for 1D convs
#                 K=3,                # PMA seeds
#                 sab_layers=3,
#                 sab_heads=4,
#                 pdrop=0.10,
#             )
#         else:
#             self.hid_extractor = None



#         # correctness head sized to the *enabled* features only
#         self.stop_head = CorrectnessHeadLite(
#             D_ATT=D_ATT, D_CONF=D_CONF, D_HID=D_HID, pdrop=0.10,
#             use_attn=self.use_stop_attn, use_conf=self.use_stop_conf, use_hid=self.use_stop_hid,
#         )
#         # self.stop_head = CorrectnessHeadLite(
#         #     D_ATT=D_ATT, D_CONF=D_CONF, D_HID=D_HID,
#         #     use_attn=self.use_stop_attn,
#         #     use_conf=self.use_stop_conf,
#         #     use_hid=self.use_stop_hid,
#         #     width=512, depth=3, expansion=2,
#         #     ca_reduction=8, eca_ks=5, pdrop=0.10,
#         # )


#         # thresholds (optional)
#         self.stop_threshold   = float(getattr(config, "stop_threshold", 0.5))
#         self.stop_token_id    = int(getattr(config, "stop_token_id", 151668))  # "</think>" default
#         self.newline_token_id = int(getattr(config, "newline_token_id", 198))

#         # expose for init/freeze utilities (only the ones that exist)
#         self._custom_head_names = []
#         if self.attn_extractor is not None: self._custom_head_names.append("attn_extractor")
#         if self.conf_extractor is not None: self._custom_head_names.append("conf_extractor")
#         if self.hid_extractor  is not None: self._custom_head_names.append("hid_extractor")
#         self._custom_head_names.append("stop_head")

#         # init weights -> will be properly (re)initialized by your post-load routine
#         self.post_init()

#         # freeze base; train only aux modules that exist
#         trainable_prefixes = tuple(self._custom_head_names)
#         for n, p in self.named_parameters():
#             if n.startswith(trainable_prefixes):
#                 p.requires_grad_(True)
#             else:
#                 p.requires_grad_(False)

#     # ----------------------------------------------------------------------------------
#     # Aux correctness scorer (returns sequence-level probability; shape (B,1))
#     # ----------------------------------------------------------------------------------
#     def _should_stop(
#         self,
#         last_hidden: torch.Tensor,                 # (B,S,Hd)
#         attn_stack: Optional[List[torch.Tensor]],  # length L: each is (B,H,k,k)  <-- already reduced
#         token_probs: torch.Tensor,                 # (B,S)
#         mask: torch.Tensor,                        # (B,S) 1=valid
#         input_ids: Optional[torch.Tensor] = None,  # unused in lite head
#     ) -> torch.Tensor:
#         """
#         Returns a single scalar probability per sequence: (B,1).
#         Assumes attention maps are already reduced to (B,L,H,k,k).
#         """
#         B, S, _ = last_hidden.shape
#         out_dtype = _safe_dtype_param(self.stop_head)

#         # sanitize inputs early
#         token_probs_s = torch.clamp(torch.nan_to_num(token_probs, nan=1.0, posinf=1.0, neginf=1e-8), 1e-8, 1.0)
#         last_hidden_s = torch.nan_to_num(last_hidden, nan=0.0, posinf=0.0, neginf=0.0)

#         # validate attention stack only if attention feature is enabled
#         A = None
#         if self.use_stop_attn:
#             if attn_stack is None or len(attn_stack) == 0:
#                 raise RuntimeError("use_stop_attn=True but no reduced attentions were provided.")
#             with torch.no_grad():
#                 A = torch.stack(attn_stack, dim=1).detach()  # (B,L,H,k,k)
#                 if A.dim() != 5 or A.shape[-1] != A.shape[-2]:
#                     raise RuntimeError(f"Expected reduced attn (B,L,H,k,k), got {tuple(A.shape)}")
#                 A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)

#         with torch.amp.autocast("cuda", dtype=torch.bfloat16, enabled=True):
#             # build features ONLY for enabled extractors (no zero fill)
#             z_att = None
#             z_conf = None
#             z_hid = None

#             if self.use_stop_attn:
#                 z_att = self.attn_extractor(A.to(_safe_dtype_param(self.attn_extractor)))

#             if self.use_stop_conf:
#                 z_conf = self.conf_extractor(token_probs_s.to(_safe_dtype_param(self.conf_extractor)))

#             if self.use_stop_hid:
#                 z_hid  = self.hid_extractor(last_hidden_s.to(_safe_dtype_param(self.hid_extractor)))

#             # sanitize produced features (if any)
#             if z_att  is not None: z_att  = torch.nan_to_num(z_att,  nan=0.0, posinf=0.0, neginf=0.0)
#             if z_conf is not None: z_conf = torch.nan_to_num(z_conf, nan=0.0, posinf=0.0, neginf=0.0)
#             if z_hid  is not None: z_hid  = torch.nan_to_num(z_hid,  nan=0.0, posinf=0.0, neginf=0.0)

#             logits = self.stop_head(
#                 z_att.to(out_dtype)  if z_att  is not None else None,
#                 z_conf.to(out_dtype) if z_conf is not None else None,
#                 z_hid.to(out_dtype)  if z_hid  is not None else None,
#             )  # (B,1)

#         # stable sigmoid in fp32, sanitize & clamp to (0,1)
#         logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)
#         with torch.amp.autocast("cuda", enabled=False):
#             probs = torch.sigmoid(logits.to(torch.float32))  # (B,1)
#         probs = torch.clamp(torch.nan_to_num(probs, nan=0.5, posinf=1.0, neginf=0.0), 1e-6, 1.0 - 1e-6).to(out_dtype)
#         # return probs, z_att, z_conf, z_hid
#         return probs

#     # (rest of your class â€” forward(), compute_stop_loss(), get/set embeddings, etc. â€” unchanged)



#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     def get_output_embeddings(self):
#         return self.lm_head

#     def set_output_embeddings(self, new_embeddings):
#         self.lm_head = new_embeddings

#     def set_decoder(self, decoder):
#         self.model = decoder

#     def get_decoder(self):
#         return self.model

#     # #hazard loss
#     # def compute_stop_loss(self, p, target, mask, eps=1e-6):
#     #     """
#     #     p      : (B,S) stop probabilities in (0,1)
#     #     target : (B,S) binary 1 at ground-truth </think> position
#     #     mask   : (B,S) 1 for real tokens, 0 for pads
#     #     """
#     #     # ---- sanitize & fp32 ----
#     #     p      = torch.nan_to_num(p.float(),    nan=0.5)
#     #     target = torch.nan_to_num(target.float(), nan=0.0)
#     #     mask   = torch.nan_to_num(mask.float(),   nan=0.0)

#     #     # Keep strictly inside (0,1) to avoid log(0) and log1p(-1)
#     #     p = p.clamp(eps, 1.0 - eps)

#     #     # ---- hazard -> event pmf q via MASKED log-softmax (no manual divide) ----
#     #     # log q_t = log p_t + sum_{i<t} log(1 - p_i)
#     #     log1m_p  = torch.log1p(-p) * mask                          # (B,S)
#     #     log_surv = torch.cumsum(log1m_p, dim=1) - log1m_p          # exclusive cumsum
#     #     logp     = torch.where(mask > 0, torch.log(p), torch.zeros_like(p))
#     #     logq_raw = logp + log_surv                                  # (B,S)

#     #     neg_inf = torch.finfo(logq_raw.dtype).min
#     #     logq_masked = torch.where(mask > 0, logq_raw, torch.full_like(logq_raw, neg_inf))
#     #     logq = torch.log_softmax(logq_masked, dim=1)                # (B,S), normalized over valid tokens
#     #     q    = torch.exp(logq)                                      # (B,S)

#     #     # ---- your distance objective ----
#     #     B, S = p.shape
#     #     t = torch.arange(S, device=p.device).unsqueeze(0)
#     #     t_star = target.argmax(dim=1, keepdim=True)                 # (B,1)
#     #     dist = (t - t_star).abs() * mask
#     #     loss_vec = (q * dist).sum(dim=1)

#     #     valid = target.sum(dim=1) > 0
#     #     loss = loss_vec[valid].mean() if valid.any() else loss_vec.mean()

#     #     # final safety: replace non-finite (keeps scaler alive on edge cases)
#     #     if not torch.isfinite(loss):
#     #         loss = torch.zeros((), device=p.device, dtype=torch.float32)
#     #     return loss

#     #Correctness BCE loss
#     def compute_stop_loss(
#         self,
#         probs_seq: torch.Tensor,           # (B,1) or (B,)
#         correctness_label: torch.Tensor,  # (B,) or (B,1); supports -1 to skip
#         eps: float = 1e-6,
#     ) -> torch.Tensor:
#         """
#         BCE on a single sequence-level probability per example.
#         - probs_seq: model's probability that the sequence is "correct" (0..1)
#         - correctness_label: float/long in {-1, 0, 1}; -1 rows are skipped
#         """
#         # Shape & dtype hygiene
#         if probs_seq.dim() == 2 and probs_seq.size(-1) == 1:
#             probs_seq = probs_seq.squeeze(-1)                # (B,)
#         probs = torch.nan_to_num(probs_seq.float(), nan=0.5)
#         probs = probs.clamp(min=eps, max=1.0 - eps)

#         labels = correctness_label
#         if labels.dim() == 2 and labels.size(-1) == 1:
#             labels = labels.squeeze(-1)                      # (B,)
#         labels = torch.nan_to_num(labels.float(), nan=0.0)

#         # Skip rows with label == -1
#         keep = labels.ne(-1.0)
#         if not torch.any(keep):
#             # preserve graph
#             return probs.sum() * 0.0

#         y = labels[keep].clamp_(0.0, 1.0)
#         p = probs[keep]

#         loss = F.binary_cross_entropy(p, y, reduction="mean")
#         if not torch.isfinite(loss):
#             # ultra-conservative fallback to keep scaler alive
#             loss = probs.sum() * 0.0
#         return loss


#     #New forward with updated compute loss to be used in SFT, everything else same as before
#     @can_return_tuple
#     @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         logits_to_keep: Union[int, torch.Tensor] = 0,
#         # ---- NEW kwargs for stop-head ----
#         apply_budget: Optional[bool] = None,
#         correctness_label: Optional[torch.Tensor] = None,   # (B,) or (B,1) in {-1,0,1}
#         **kwargs: Unpack[KwargsForCausalLM],
#     ) -> CausalLMOutputWithPast:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#             Standard LM labels (unused for correctness loss).

#         logits_to_keep (`int` or `torch.Tensor`, *optional*):
#             Kept for API parity; logits are computed for the whole sequence here.

#         Returns:
#             CausalLMOutputWithPast with an extra field `stop_prob` (B,1) when computed.
#         """
#         # We need attentions during training / budgeted inference
#         output_attentions = (
#             True if (self.training or apply_budget)
#             else (self.config.output_attentions if output_attentions is None else output_attentions)
#         )
#         output_hidden_states = False  # we don't request per-layer hiddens from the backbone

#         # ---- 1) Run the frozen backbone under no_grad; project to logits (for metrics/compat) ----
#         with torch.no_grad():
#             outputs: BaseModelOutputWithPast = self.model(
#                 input_ids=input_ids,
#                 attention_mask=attention_mask,
#                 position_ids=position_ids,
#                 past_key_values=past_key_values,
#                 inputs_embeds=inputs_embeds,
#                 use_cache=use_cache,
#                 output_attentions=output_attentions,
#                 output_hidden_states=False,
#                 cache_position=cache_position,
#                 **kwargs,
#             )
#             hidden_full = outputs.last_hidden_state                   # (B, L, H)
#             logits      = self.lm_head(hidden_full)                  # (B, L, V)

#         # For the correctness head we follow your GRPO pass: drop the final next-token row to align tokensâ†”features.
#         hidden_for_head = hidden_full[:, :-1, :]                     # (B, S_hid, H) with S_hid = L-1
#         B, S_hid, _ = hidden_for_head.shape

#         # Mask aligned to S_hid (full prompt+completion span, minus last row)
#         if attention_mask is not None:
#             mask = attention_mask[:, :S_hid].float()
#         else:
#             mask = torch.ones((B, S_hid), device=hidden_for_head.device, dtype=torch.float)


#         stop_prob = None
#         # ===============================
#         # 2) TRAINING: sequence-level BCE
#         # ===============================
#         if self.training:
#             if correctness_label is None:
#                 raise ValueError("`correctness_labels` (shape (B,) or (B,1), values in {-1,0,1}) is required for training.")

#             #stop_prob
#             logits_step = logits[:, :-1, :] if logits.size(1) == labels.size(1) + 1 else logits
#             logp = torch.log_softmax(logits_step.float(), dim=-1)                # (B,S,V)
#             tgt = labels.clamp(min=0)
#             log_tok_p = logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)           # (B,S)
#             log_tok_p = torch.where(labels.eq(-100), torch.zeros_like(log_tok_p), log_tok_p)
#             token_probs_so_far = torch.clamp(log_tok_p.exp(), min=1e-8).to(logits_step.dtype).detach()
            
#             # Use already-reduced attentions from the backbone as-is; no trimming or extra processing.
#             attn_stack = outputs.attentions  # expected list[L] of (B, H, k, k) if self.use_stop_attn else possibly None

#             stop_prob = self._should_stop(
#                 last_hidden = hidden_for_head,        # (B, S_hid, H)
#                 attn_stack  = attn_stack,             # list[L] or None (already reduced by your backbone)
#                 token_probs = token_probs_so_far[:,:-1],       # (B, S_hid)  â€” neutral, since we removed token_probs_so_far
#                 mask        = mask,                    # (B, S_hid)
#                 input_ids   = input_ids[:, :S_hid] if input_ids is not None else None,
#             )  # (B,1)

#             # Only correctness loss (head-only training). BCE handles -1 rows internally.
#             loss = self.compute_stop_loss(stop_prob, correctness_label)

#             return CausalLMOutputWithPast(
#                 loss=loss,
#                 logits=logits,
#                 past_key_values=outputs.past_key_values,
#                 hidden_states=outputs.hidden_states,
#                 attentions=outputs.attentions,
#                 stop_prob=stop_prob.detach(),
#             )

#         # ===============================
#         # 3) INFERENCE (optional stop prob)
#         # ===============================

#         # if apply_budget:
#         #     # Single pass after generation is done â€” use full-sequence features (minus last row) and attentions as-is.
#         #     attn_stack = outputs.attentions
#         #     stop_prob = self._should_stop(
#         #         last_hidden = hidden_for_head,        # (B, S_hid, H)
#         #         attn_stack  = attn_stack,             # list[L] or None
#         #         token_probs = token_probs_so_far,       # neutral
#         #         mask        = mask,                    # (B, S_hid)
#         #         input_ids   = input_ids[:, :S_hid] if input_ids is not None else None,
#         #     )  # (B,1)
#         #     stop_prob = stop_prob.clamp_(min=1e-6, max=1.0 - 1e-6)

#         return CausalLMOutputWithPast(
#             logits=logits,
#             past_key_values=outputs.past_key_values,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#             stop_prob=stop_prob,  # (B,1) or None
#         )


#     #original forwrd used in t3, t4
#     # @can_return_tuple
#     # @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
#     # @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     # @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
#     # def forward(
#     #     self,
#     #     input_ids: Optional[torch.LongTensor] = None,
#     #     attention_mask: Optional[torch.Tensor] = None,
#     #     position_ids: Optional[torch.LongTensor] = None,
#     #     past_key_values: Optional[Cache] = None,
#     #     inputs_embeds: Optional[torch.FloatTensor] = None,
#     #     labels: Optional[torch.LongTensor] = None,
#     #     use_cache: Optional[bool] = None,
#     #     output_attentions: Optional[bool] = None,
#     #     output_hidden_states: Optional[bool] = None,
#     #     cache_position: Optional[torch.LongTensor] = None,
#     #     logits_to_keep: Union[int, torch.Tensor] = 0,
#     #     # ---- NEW kwargs for stop-head ----
#     #     token_probs_so_far: Optional[torch.Tensor] = None,   # (B,S) float in (0,1], used by conf extractor
#     #     apply_budget: Optional[bool] = None,
#     #     correctness_labels: Optional[torch.Tensor] = None,   # (B,) or (B,1), {-1,0,1}
#     #     **kwargs: Unpack[KwargsForCausalLM],
#     # ) -> CausalLMOutputWithPast:
#     #     r"""
#     #         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#     #             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
#     #             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
#     #             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

#     #         logits_to_keep (`int` or `torch.Tensor`, *optional*):
#     #             If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
#     #             `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
#     #             token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
#     #             If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
#     #             This is useful when using packed tensor format (single dimension for batch and sequence length).

#     #     Returns:

#     #     Example:

#     #     ```python
#     #     >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

#     #     >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
#     #     >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

#     #     >>> prompt = "Hey, are you conscious? Can you talk to me?"
#     #     >>> inputs = tokenizer(prompt, return_tensors="pt")

#     #     >>> # Generate
#     #     >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
#     #     >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
#     #     "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
#     #     ```"""
#     #     output_attentions = (
#     #         True if (self.training or apply_budget)
#     #         else (self.config.output_attentions if output_attentions is None else output_attentions)
#     #     )
#     #     output_hidden_states = False

#     #     # ---- run base model under no_grad to keep base frozen & cheap ----
#     #     with torch.no_grad():
#     #         outputs: BaseModelOutputWithPast = self.model(
#     #             input_ids=input_ids,
#     #             attention_mask=attention_mask,
#     #             position_ids=position_ids,
#     #             past_key_values=past_key_values,
#     #             inputs_embeds=inputs_embeds,
#     #             use_cache=use_cache,
#     #             output_attentions=output_attentions,
#     #             output_hidden_states=False,
#     #             cache_position=cache_position,
#     #             **kwargs,
#     #         )

#     #         hidden_states = outputs.last_hidden_state          # (B,S,H)
#     #         logits        = self.lm_head(hidden_states)        # (B,S,V)

#     #         # Build token_probs_so_far if needed (for conf extractor)
#     #         if self.training and token_probs_so_far is None and labels is not None:
#     #             logits_step = logits[:, :-1, :] if logits.size(1) == labels.size(1) + 1 else logits
#     #             logp = torch.log_softmax(logits_step.float(), dim=-1)                # (B,S,V)
#     #             tgt = labels.clamp(min=0)
#     #             log_tok_p = logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)           # (B,S)
#     #             log_tok_p = torch.where(labels.eq(-100), torch.zeros_like(log_tok_p), log_tok_p)
#     #             token_probs_so_far = torch.clamp(log_tok_p.exp(), min=1e-8).to(logits_step.dtype).detach()

#     #     # ===============================
#     #     # TRAINING: sequence-level BCE
#     #     # ===============================
#     #     if self.training:
#     #         if correctness_labels is None:
#     #             raise ValueError("`correctness_labels` (shape (B,) or (B,1), values in {-1,0,1}) is required for training.")

#     #         # Prepare inputs to _should_stop (expects reduced attn per layer: (B,H,k,k))
#     #         # Your backbone already returns reduced attentions; just pass them through.
#     #         mask = attention_mask.float() if attention_mask is not None else \
#     #             torch.ones(hidden_states.size()[:2], device=hidden_states.device, dtype=torch.float)

#     #         stop_prob = self._should_stop(
#     #             last_hidden = hidden_states,             # (B,S,H)
#     #             attn_stack  = outputs.attentions,        # list[L] of (B,H,k,k)  (already reduced)
#     #             token_probs = token_probs_so_far if token_probs_so_far is not None else
#     #                         torch.ones_like(mask, dtype=hidden_states.dtype),
#     #             mask        = mask,                      # (B,S)
#     #             input_ids   = input_ids,
#     #         )                                            # (B,1)

#     #         # BCE against sequence label (supports -1 skipping inside compute_stop_loss)
#     #         loss = self.compute_stop_loss(stop_prob, correctness_labels)

#     #         return CausalLMOutputWithPast(
#     #             loss=loss,
#     #             logits=logits,
#     #             past_key_values=outputs.past_key_values,
#     #             hidden_states=outputs.hidden_states,
#     #             attentions=outputs.attentions,
#     #             stop_prob=stop_prob.detach(),
#     #         )

#     #     # ===============================
#     #     # INFERENCE
#     #     # ===============================
#     #     stop_prob = None
#     #     if apply_budget:
#     #         if token_probs_so_far is None:
#     #             raise ValueError("`token_probs_so_far` must be provided when `apply_budget=True` in inference.")

#     #         B, S, _ = hidden_states.shape
#     #         tok_p = token_probs_so_far
#     #         if tok_p.size(1) < S:
#     #             pad = tok_p.new_full((B, S - tok_p.size(1)), 1.0)
#     #             tok_p = torch.cat([tok_p, pad], dim=1)
#     #         elif tok_p.size(1) > S:
#     #             tok_p = tok_p[:, :S]

#     #         mask = attention_mask[:, :S].float() if attention_mask is not None else \
#     #             torch.ones(B, S, device=hidden_states.device, dtype=torch.float)

#     #         # Use full-sequence features & already-reduced attentions
#     #         stop_prob = self._should_stop(
#     #             last_hidden = hidden_states,             # (B,S,H)
#     #             attn_stack  = outputs.attentions,        # list[L] of (B,H,k,k) (already reduced)
#     #             token_probs = tok_p,                     # (B,S)
#     #             mask        = mask,                      # (B,S)
#     #             input_ids   = input_ids[:, :S] if input_ids is not None else None,
#     #         )                                            # (B,1)
#     #         stop_prob = stop_prob.clamp_(min=1e-6, max=1-1e-6)

#     #     return CausalLMOutputWithPast(
#     #         logits=logits,
#     #         past_key_values=outputs.past_key_values,
#     #         hidden_states=outputs.hidden_states,
#     #         attentions=outputs.attentions,
#     #         stop_prob=stop_prob,   # (B,1) or None
#     #     )



# @add_start_docstrings(
#     """
#     The Qwen3 Model transformer with a sequence classification head on top (linear layer).

#     [`Qwen3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models
#     (e.g. GPT-2) do.

#     Since it does classification on the last token, it requires to know the position of the last token. If a
#     `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
#     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
#     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
#     each row of the batch).
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForSequenceClassification(Qwen3PreTrainedModel):
#     def __init__(self, config):
#         super().__init__(config)
#         self.num_labels = config.num_labels
#         self.model = Qwen3Model(config)
#         self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#     ) -> SequenceClassifierOutputWithPast:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
#             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
#             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
#         """

#         transformer_outputs: BaseModelOutputWithPast = self.model(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             use_cache=use_cache,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )
#         hidden_states = transformer_outputs.last_hidden_state
#         logits = self.score(hidden_states)

#         if input_ids is not None:
#             batch_size = input_ids.shape[0]
#         else:
#             batch_size = inputs_embeds.shape[0]

#         if self.config.pad_token_id is None and batch_size != 1:
#             raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
#         if self.config.pad_token_id is None:
#             last_non_pad_token = -1
#         elif input_ids is not None:
#             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
#             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
#             token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
#             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
#         else:
#             last_non_pad_token = -1
#             logger.warning_once(
#                 f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
#                 "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
#             )

#         pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

#         loss = None
#         if labels is not None:
#             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

#         return SequenceClassifierOutputWithPast(
#             loss=loss,
#             logits=pooled_logits,
#             past_key_values=transformer_outputs.past_key_values,
#             hidden_states=transformer_outputs.hidden_states,
#             attentions=transformer_outputs.attentions,
#         )


# @add_start_docstrings(
#     """
#     The Qwen3 Model transformer with a token classification head on top (a linear layer on top of the hidden-states
#     output) e.g. for Named-Entity-Recognition (NER) tasks.
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForTokenClassification(Qwen3PreTrainedModel):
#     def __init__(self, config):
#         super().__init__(config)
#         self.num_labels = config.num_labels
#         self.model = Qwen3Model(config)
#         if getattr(config, "classifier_dropout", None) is not None:
#             classifier_dropout = config.classifier_dropout
#         elif getattr(config, "hidden_dropout", None) is not None:
#             classifier_dropout = config.hidden_dropout
#         else:
#             classifier_dropout = 0.1
#         self.dropout = nn.Dropout(classifier_dropout)
#         self.score = nn.Linear(config.hidden_size, config.num_labels)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     @add_code_sample_docstrings(
#         checkpoint=_CHECKPOINT_FOR_DOC,
#         output_type=TokenClassifierOutput,
#         config_class=_CONFIG_FOR_DOC,
#     )
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#     ) -> TokenClassifierOutput:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
#             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
#             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
#         """

#         outputs: BaseModelOutputWithPast = self.model(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             use_cache=use_cache,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )
#         sequence_output = outputs.last_hidden_state
#         sequence_output = self.dropout(sequence_output)
#         logits = self.score(sequence_output)

#         loss = None
#         if labels is not None:
#             loss = self.loss_function(logits, labels, self.config)

#         return TokenClassifierOutput(
#             loss=loss,
#             logits=logits,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#         )


# @add_start_docstrings(
#     """
# The Qwen3 Model transformer with a span classification head on top for extractive question-answering tasks like
# SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForQuestionAnswering(Qwen3PreTrainedModel):
#     base_model_prefix = "transformer"

#     def __init__(self, config):
#         super().__init__(config)
#         self.transformer = Qwen3Model(config)
#         self.qa_outputs = nn.Linear(config.hidden_size, 2)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.transformer.embed_tokens

#     def set_input_embeddings(self, value):
#         self.transformer.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.FloatTensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         start_positions: Optional[torch.LongTensor] = None,
#         end_positions: Optional[torch.LongTensor] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         **kwargs,
#     ) -> QuestionAnsweringModelOutput:
#         r"""
#         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for position (index) of the start of the labelled span for computing the token classification loss.
#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
#             are not taken into account for computing the loss.
#         end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for position (index) of the end of the labelled span for computing the token classification loss.
#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
#             are not taken into account for computing the loss.
#         """

#         outputs: BaseModelOutputWithPast = self.transformer(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )

#         sequence_output = outputs.last_hidden_state

#         logits = self.qa_outputs(sequence_output)
#         start_logits, end_logits = logits.split(1, dim=-1)
#         start_logits = start_logits.squeeze(-1).contiguous()
#         end_logits = end_logits.squeeze(-1).contiguous()

#         loss = None
#         if start_positions is not None and end_positions is not None:
#             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)

#         return QuestionAnsweringModelOutput(
#             loss=loss,
#             start_logits=start_logits,
#             end_logits=end_logits,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#         )


# __all__ = [
#     "Qwen3ForCausalLM",
#     "Qwen3ForQuestionAnswering",
#     "Qwen3Model",
#     "Qwen3PreTrainedModel",
#     "Qwen3ForSequenceClassification",
#     "Qwen3ForTokenClassification",
# ]
