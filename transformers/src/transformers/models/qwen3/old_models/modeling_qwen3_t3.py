# # modeling_qwen3_t3_final_final_trivia

# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# #           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
# #               Do NOT edit this file manually as any edits will be overwritten by the generation of
# #             the file from the modular. If any change should be done, please apply the change to the
# #                          modular_qwen3.py file directly. One of our CI enforces this.
# #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# # coding=utf-8
# # Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
# #
# # Licensed under the Apache License, Version 2.0 (the "License");
# # you may not use this file except in compliance with the License.
# # You may obtain a copy of the License at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.
# from __future__ import annotations

# import math
# from typing import Optional, List

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from torch.amp import autocast

# from functools import partial
# from typing import Callable, Optional, Tuple, Union

# import torch
# from torch import nn

# from ...activations import ACT2FN
# from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
# from ...generation import GenerationMixin
# from ...modeling_attn_mask_utils import AttentionMaskConverter
# from ...modeling_flash_attention_utils import FlashAttentionKwargs
# from ...modeling_outputs import (
#     BaseModelOutputWithPast,
#     QuestionAnsweringModelOutput,
#     SequenceClassifierOutputWithPast,
#     TokenClassifierOutput,
# )
# from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
# from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
# from ...processing_utils import Unpack
# from ...utils import (
#     # LossKwargs,
#     add_code_sample_docstrings,
#     add_start_docstrings,
#     add_start_docstrings_to_model_forward,
#     can_return_tuple,
#     logging,
#     replace_return_docstrings,
# )
# from ...utils.deprecation import deprecate_kwarg
# from .configuration_qwen3 import Qwen3Config
# from ...utils import ModelOutput
# from dataclasses import dataclass
# from torch.distributions import Gamma
# import numpy as np
# from transformers import BertConfig, BertModel
# import torch, torch.nn.functional as F
# from typing import List


# def initialize_weights(module):
#     """
#     Recursively initializes weights for Linear, Conv, and Norm layers.
#     """
#     if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
#         # Use Kaiming normal initialization for weights
#         nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
#         # Initialize biases to zero, if they exist
#         if module.bias is not None:
#             nn.init.constant_(module.bias, 0)
#     elif isinstance(module, (nn.LayerNorm, nn.modules.normalization.RMSNorm)):
#         # Initialize normalization layers: weight to 1, bias to 0
#         if hasattr(module, 'weight') and module.weight is not None:
#             nn.init.constant_(module.weight, 1.0)
#         if hasattr(module, 'bias') and module.bias is not None:
#             nn.init.constant_(module.bias, 0)



# # #myedit, Visualization************************************************************************************************
# # # stop_viz.py (no cropping, small figs, no ticks)
# # import os, uuid, math
# # import numpy as np
# # import torch
# # import torch.nn.functional as F
# # import matplotlib
# # matplotlib.use("Agg")
# # import matplotlib.pyplot as plt
# # from numbers import Integral
# # from typing import Optional, List, Sequence, Tuple
# # from matplotlib import colors  # <- already added above

# # # ---------- robust limits ----------
# # def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> tuple[float, float]:
# #     """Percentile-based vmin/vmax with safe fallback so we never get a flat colormap."""
# #     v = np.asarray(M)
# #     v = v[np.isfinite(v)]
# #     if v.size == 0:
# #         return 0.0, 1.0
# #     vmin = float(np.percentile(v, lo))
# #     vmax = float(np.percentile(v, hi))
# #     if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
# #         vmin = float(v.min()) if v.size else 0.0
# #         vmax = float(v.max()) if v.size else 1.0
# #         if vmin == vmax:
# #             vmax = vmin + 1e-6
# #     return vmin, vmax

# # # ---------- small, enhanced token map (keep only ONE definition of this) ----------
# # def _plot_token_single_full(M, think_positions, path):
# #     S = M.shape[0]
# #     vmin, vmax = _robust_limits(M)
# #     norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)  # boost mid/low contrast

# #     plt.figure(figsize=(4, 4))  # small, fixed
# #     plt.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
# #     for t in (think_positions or []):
# #         if 0 <= t < S:
# #             plt.axvline(t, ls="--", lw=0.8, color="w")
# #             plt.axhline(t, ls="--", lw=0.8, color="w")
# #             plt.scatter([t], [t], s=10, c="red", marker="o")
# #     plt.xticks([]); plt.yticks([])  # no ticks to keep it light
# #     plt.title("Token attention (full, enhanced)", fontsize=9)
# #     plt.tight_layout()
# #     plt.savefig(path, dpi=200)
# #     plt.close()

# # def _ensure_dir(p: str):
# #     os.makedirs(p, exist_ok=True)

# # @torch.no_grad()
# # def _unify_attentions(attn_stack):
# #     """
# #     attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
# #     Returns A_all: (B,L,S,S) after head-avg and square-izing.
# #     """
# #     if attn_stack is None or len(attn_stack) == 0:
# #         return None
# #     layers = []
# #     for A in attn_stack:
# #         A = A.float()  # (B,H,q,S)
# #         if A.dim() != 4:
# #             continue
# #         B, H, q, S = A.shape
# #         A = A.mean(dim=1)  # (B,q,S)
# #         if q == 1:
# #             sq = A.new_zeros(B, S, S)
# #             sq[:, -1, :] = A[:, 0, :]
# #             A_sq = sq
# #         elif q == S:
# #             A_sq = A
# #         else:
# #             if q < S:
# #                 A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
# #             A_sq = A[:, :S, :S]
# #         layers.append(A_sq)  # (B,S,S)
# #     if not layers:
# #         return None
# #     return torch.stack(layers, dim=1)  # (B,L,S,S)

# # @torch.no_grad()
# # def _token_to_sentence_attention(A_tok: torch.Tensor, input_ids: torch.Tensor, sep_id: int):
# #     """
# #     A_tok: (B,L,S,S). Returns (B,L,Ns,Ns), seg, Ns.
# #     """
# #     B, L, S, _ = A_tok.shape
# #     seg = torch.cumsum((input_ids == sep_id).long(), dim=1)  # (B,S)
# #     Ns = int(seg.max().item()) + 1
# #     H = F.one_hot(seg, Ns).to(dtype=A_tok.dtype)             # (B,S,Ns)
# #     A_sum = torch.einsum("bqi, blqt, btj -> blij", H, A_tok, H)
# #     counts = H.sum(dim=1).clamp(min=1.0)                     # (B,Ns)
# #     denom = counts.unsqueeze(1).unsqueeze(3) * counts.unsqueeze(1).unsqueeze(2)
# #     return (A_sum / denom), seg, Ns

# # def _grid_dims(n: int) -> Tuple[int, int]:
# #     cols = int(np.ceil(np.sqrt(n)))
# #     rows = int(np.ceil(n / cols))
# #     return rows, cols

# # # ---------- outlier smoothing (confidence) ----------
# # @torch.no_grad()
# # def interp_outliers_inplace_token_probs(token_probs: torch.Tensor,
# #                                         mask: torch.Tensor,
# #                                         low: float = 1.0, high: float = 99.0) -> None:
# #     """
# #     In-place linear interpolation of outliers in token_probs per batch item.
# #     Outliers = values outside [low, high] percentiles over valid (mask>0) positions.
# #     """
# #     B, S = token_probs.shape
# #     for b in range(B):
# #         valid = (mask[b] > 0).cpu().numpy()
# #         if valid.sum() < 3:
# #             continue
# #         y = token_probs[b, valid].detach().float().cpu().numpy()
# #         x = np.arange(y.shape[0])

# #         lo, hi = np.percentile(y, [low, high])
# #         bad = (y < lo) | (y > hi)
# #         if not bad.any():
# #             continue

# #         x_good = x[~bad]
# #         y_good = y[~bad]
# #         if x_good.size >= 2:
# #             y[bad] = np.interp(x[bad], x_good, y_good)
# #         else:
# #             y = np.clip(y, lo, hi)

# #         token_probs[b, valid] = torch.as_tensor(y, device=token_probs.device, dtype=token_probs.dtype)

# # # ---------- marker finding (</think>) ----------
# # @torch.no_grad()
# # def _find_marker_positions(ids_row: torch.Tensor,
# #                            think_id: Optional[int] = None,
# #                            think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
# #     """
# #     Returns start indices where the marker occurs in input_ids.
# #     - If think_token_seq is provided, it searches for that subsequence.
# #     - Else falls back to single-token search using think_id.
# #     """
# #     ids = ids_row.tolist()
# #     positions: List[int] = []

# #     if think_token_seq and len(think_token_seq) > 0:
# #         pat = list(think_token_seq)
# #         m, n = len(pat), len(ids)
# #         if m <= n:
# #             for i in range(0, n - m + 1):
# #                 if ids[i:i+m] == pat:
# #                     positions.append(i)
# #         return positions

# #     if think_id is not None:
# #         for i, tok in enumerate(ids):
# #             if tok == think_id:
# #                 positions.append(i)
# #     return positions

# # # ---------- plotting helpers (small figs) ----------
# # def _plot_conf_full(probs, mask, think_positions, path):
# #     K = probs.shape[0]
# #     x = np.arange(K)
# #     plt.figure(figsize=(30, 5))
# #     plt.plot(x[mask > 0], probs[mask > 0], linewidth=1.0)
# #     for pos in think_positions:
# #         tmk = pos - 1  # probs[t] predicts token t+1
# #         if 0 <= tmk < K:
# #             plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
# #     plt.xticks([]); plt.yticks([])
# #     plt.title("Confidence (full)", fontsize=9)
# #     plt.tight_layout()
# #     plt.savefig(path, dpi=200)
# #     plt.close()

# # # ---------- NEW: uncertainty plots ----------
# # def _plot_uncertainty_full(unc, mask, think_positions, path, title="Uncertainty (full)"):
# #     """
# #     unc: numpy array shape (K,) with per-token uncertainty (e.g., -log p_gold or entropy).
# #     mask: numpy array shape (K,) in {0,1}
# #     """
# #     K = unc.shape[0]
# #     x = np.arange(K)
# #     plt.figure(figsize=(30, 5))
# #     plt.plot(x[mask > 0], unc[mask > 0], linewidth=1.0)
# #     for pos in think_positions:
# #         tmk = pos - 1  # aligns with next-token prediction index
# #         if 0 <= tmk < K:
# #             plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
# #     plt.xticks([]); plt.yticks([])
# #     plt.title(title, fontsize=9)
# #     plt.tight_layout()
# #     plt.savefig(path, dpi=200)
# #     plt.close()

# # def _plot_token_grid_full(A_layers, think_positions, path, max_layers=16):
# #     """
# #     Show exactly the last up-to-3 layers (3Ã—1). Each subplot uses robust normalization.
# #     """
# #     L, S, _ = A_layers.shape
# #     if L == 0:
# #         return

# #     idxs = list(range(max(0, L - 3), L))  # [L-3, L-2, L-1]
# #     panels = len(idxs)

# #     fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
# #     if panels == 1:
# #         axes = [axes]

# #     for ax, li in zip(axes, idxs):
# #         M = A_layers[li]
# #         vmin, vmax = _robust_limits(M)
# #         norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)
# #         ax.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
# #         for t in (think_positions or []):
# #             if 0 <= t < S:
# #                 ax.axvline(t, ls="--", lw=0.8, color="w")
# #                 ax.axhline(t, ls="--", lw=0.8, color="w")
# #                 ax.scatter([t], [t], s=8, c="red", marker="o")
# #         ax.set_title(f"L{li}", fontsize=8)
# #         # keep ticks default off to stay light

# #     fig.tight_layout()
# #     fig.savefig(path, dpi=180)
# #     plt.close(fig)

# # def _plot_sentence_single(Ms, s_star, path):
# #     plt.figure(figsize=(4, 4))
# #     plt.imshow(Ms, aspect="auto", interpolation="nearest")
# #     if s_star is not None and 0 <= s_star < Ms.shape[0]:
# #         plt.axvline(s_star, ls="--", lw=0.8, color="w")
# #         plt.axhline(s_star, ls="--", lw=0.8, color="w")
# #     plt.xticks([]); plt.yticks([])
# #     plt.title("Sentence attention (full)", fontsize=9)
# #     plt.tight_layout()
# #     plt.savefig(path, dpi=200)
# #     plt.close()

# # def _plot_sentence_grid(M_layers, s_star, path, max_layers=16):
# #     L = M_layers.shape[0]
# #     L_show = min(L, max_layers)
# #     rows, cols = _grid_dims(L_show)
# #     vmax = float(M_layers[:L_show].max()) if L_show > 0 else 1.0
# #     vmax = max(vmax, 1e-12)

# #     fig, axes = plt.subplots(rows, cols, figsize=(2.0*cols, 2.0*rows))
# #     axes = np.atleast_2d(axes)
# #     idx = 0
# #     for r in range(rows):
# #         for c in range(cols):
# #             ax = axes[r, c]
# #             if idx < L_show:
# #                 Ms = M_layers[idx]
# #                 ax.imshow(Ms, aspect="auto", interpolation="nearest", vmin=0.0, vmax=vmax)
# #                 if s_star is not None and 0 <= s_star < Ms.shape[0]:
# #                     ax.axvline(s_star, ls="--", lw=0.8, color="w")
# #                     ax.axhline(s_star, ls="--", lw=0.8, color="w")
# #                 ax.set_title(f"L{idx}", fontsize=8)
# #                 ax.set_xticks([]); ax.set_yticks([])
# #             else:
# #                 ax.axis("off")
# #             idx += 1
# #     fig.tight_layout()
# #     fig.savefig(path, dpi=180)
# #     plt.close(fig)

# # # ---------- main (NO cropping) ----------
# # @torch.no_grad()
# # def plot_stop_viz(
# #     input_ids: torch.Tensor,          # (B,S)
# #     token_probs: torch.Tensor,        # (B,S)
# #     mask: torch.Tensor,               # (B,S)
# #     attn_stack,                       # list[L] of (B,H,S,S) or (B,H,1,S)
# #     save_root: str,
# #     sep_id: int = 198,
# #     think_id: Optional[int] = 151668,
# #     think_token_seq: Optional[Sequence[int]] = None,  # pass if </think> is multi-token
# #     batch_index: int = 0,
# #     max_layers_grid: int = 16,
# #     tag: Optional[str] = None,
# #     per_token_uncertainty: Optional[torch.Tensor] = None,  # <-- NEW (optional entropy/SCE)
# #     plot_uncertainty_delta: bool = False,                  # <-- NEW
# # ):
# #     """
# #     Saves (FULL views, small figs, no ticks):
# #       - confidence/stop_conf_{tag}.png
# #       - uncertainty/stop_unc_{tag}.png                (NEW; surprisal or provided entropy)
# #       - uncertainty/stop_unc_delta_{tag}.png         (NEW; Î” uncertainty, optional)
# #       - token_attn/single/stop_tok_attn_{tag}.png
# #       - token_attn/grid/stop_tok_attn_grid_{tag}.png
# #       - sentence_attn/single/stop_sent_attn_{tag}.png
# #       - sentence_attn/grid/stop_sent_attn_grid_{tag}.png
# #     """
# #     if not isinstance(sep_id, Integral):
# #         sep_id = 198

# #     _ensure_dir(save_root)
# #     d_conf        = os.path.join(save_root, "confidence")
# #     d_unc         = os.path.join(save_root, "uncertainty")  # NEW
# #     d_tok_single  = os.path.join(save_root, "token_attn", "single")
# #     d_tok_grid    = os.path.join(save_root, "token_attn", "grid")
# #     d_sent_single = os.path.join(save_root, "sentence_attn", "single")
# #     d_sent_grid   = os.path.join(save_root, "sentence_attn", "grid")
# #     for d in [d_conf, d_unc, d_tok_single, d_tok_grid, d_sent_single, d_sent_grid]:
# #         _ensure_dir(d)

# #     tag = tag or uuid.uuid4().hex[:8]
# #     b = batch_index
# #     S = int(input_ids.size(1))
# #     K = S  # FULL length; discard any kept-token logic

# #     # find </think> occurrences
# #     think_positions: List[int] = _find_marker_positions(
# #         input_ids[b], think_id=think_id, think_token_seq=think_token_seq
# #     )

# #     # confidence (full) â€” smooth outliers then plot
# #     interp_outliers_inplace_token_probs(token_probs, mask)
# #     probs_np = token_probs[b, :K].detach().float().cpu().numpy()
# #     mask_np  = mask[b, :K].detach().float().cpu().numpy()
# #     _plot_conf_full(
# #         probs=probs_np,
# #         mask=mask_np,
# #         think_positions=think_positions,
# #         path=os.path.join(d_conf, f"stop_conf_{tag}.png"),
# #     )

# #     # ---------- NEW: uncertainty (full sequence) ----------
# #     # If an uncertainty tensor is provided (e.g., entropy), plot that.
# #     # Otherwise, use surprisal: u = -log p_gold.
# #     if per_token_uncertainty is not None:
# #         U = per_token_uncertainty[b, :K].detach().float().cpu().numpy()
# #         unc_title = "Uncertainty (provided)"
# #     else:
# #         eps = 1e-8
# #         U = np.log(np.clip(probs_np, eps, 1.0))
# #         unc_title = "Uncertainty = -log p_gold"

# #     _plot_uncertainty_full(
# #         unc=U,
# #         mask=mask_np,
# #         think_positions=think_positions,
# #         path=os.path.join(d_unc, f"stop_unc_{tag}.png"),
# #         title=unc_title
# #     )

# #     if plot_uncertainty_delta:
# #         # simple first difference to visualize "change" in uncertainty
# #         dU = np.diff(U, prepend=U[:1])
# #         _plot_uncertainty_full(
# #             unc=dU,
# #             mask=mask_np,
# #             think_positions=think_positions,
# #             path=os.path.join(d_unc, f"stop_unc_delta_{tag}.png"),
# #             title="Î” Uncertainty (first difference)"
# #         )

# #     # token attention (full)
# #     # print(attn_stack)
# #     # A_all = _unify_attentions(attn_stack)  # (B,L,S,S) or None
# #     A_all = attn_stack
# #     if A_all is not None and A_all.size(1) > 0:
# #         A_all_b = torch.nan_to_num(A_all[b].float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0)
# #         A_layers_full = A_all_b.cpu().numpy()     # (L,S,S)
# #         A_last_full   = A_layers_full[-1]         # (S,S)

# #         _plot_token_single_full(
# #             M=A_last_full,
# #             think_positions=think_positions,
# #             path=os.path.join(d_tok_single, f"stop_tok_attn_{tag}.png"),
# #         )

# #         _plot_token_grid_full(
# #             A_layers=A_layers_full,
# #             think_positions=think_positions,
# #             path=os.path.join(d_tok_grid, f"stop_tok_attn_grid_{tag}.png"),
# #             max_layers=max_layers_grid,
# #         )

# #         # sentence attention (full)
# #         A_sent, seg, Ns = _token_to_sentence_attention(A_all[b:b+1], input_ids[b:b+1], sep_id)
# #         A_sent_np = A_sent[0].float().clamp_min(0).cpu().numpy()

# #         s_mark = None
# #         if think_positions:
# #             last_pos = think_positions[-1]
# #             s_mark = int(seg[0, last_pos].item())

# #         _plot_sentence_single(
# #             Ms=A_sent_np[-1],
# #             s_star=s_mark,
# #             path=os.path.join(d_sent_single, f"stop_sent_attn_{tag}.png"),
# #         )
# #         _plot_sentence_grid(
# #             M_layers=A_sent_np,
# #             s_star=s_mark,
# #             path=os.path.join(d_sent_grid, f"stop_sent_attn_grid_{tag}.png"),
# #             max_layers=max_layers_grid,
# #         )

# # #**********************************************************************************************************************
# # #Plot attn maps
# # import os, uuid
# # import numpy as np
# # import torch
# # import torch.nn.functional as F
# # import matplotlib
# # matplotlib.use("Agg")
# # import matplotlib.pyplot as plt
# # from matplotlib import colors
# # from typing import Optional, Sequence, List, Tuple


# # # =========================
# # # utils
# # # =========================
# # def _ensure_dir(p: str):
# #     os.makedirs(p, exist_ok=True)

# # def _unique_path(path: str) -> str:
# #     """Return path if free; otherwise append _001, _002, ... before extension."""
# #     if not os.path.exists(path):
# #         return path
# #     base, ext = os.path.splitext(path)
# #     i = 1
# #     while True:
# #         cand = f"{base}_{i:03d}{ext}"
# #         if not os.path.exists(cand):
# #             return cand
# #         i += 1

# # @torch.no_grad()
# # def _unify_attentions(attn_stack):
# #     """
# #     attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
# #     â†’ returns (B,L,S,S) with heads averaged and 'q' expanded to S (last query row if q=1).
# #     """
# #     if attn_stack is None or len(attn_stack) == 0:
# #         print("Kir")
# #         return None
# #     layers = []
# #     for A in attn_stack:
# #         if A is None or A.dim() != 4:
# #             continue
# #         A = A.float()                   # (B,H,q,S)
# #         B, H, q, S = A.shape
# #         A = A.mean(dim=1)               # (B,q,S)
# #         if q == 1:
# #             sq = A.new_zeros(B, S, S)
# #             sq[:, -1, :] = A[:, 0, :]
# #             A_sq = sq
# #         elif q == S:
# #             A_sq = A
# #         else:
# #             if q < S:
# #                 A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
# #             A_sq = A[:, :S, :S]
# #         layers.append(A_sq)              # (B,S,S)
# #     if not layers:
# #         return None
# #     return torch.stack(layers, dim=1)   # (B,L,S,S)

# # def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> Tuple[float, float]:
# #     v = np.asarray(M)
# #     v = v[np.isfinite(v)]
# #     if v.size == 0:
# #         return 0.0, 1.0
# #     vmin = float(np.percentile(v, lo))
# #     vmax = float(np.percentile(v, hi))
# #     if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
# #         vmin = float(v.min()) if v.size else 0.0
# #         vmax = float(v.max()) if v.size else 1.0
# #     if vmin == vmax:
# #         vmax = vmin + 1e-6
# #     return vmin, vmax

# # @torch.no_grad()
# # def _find_marker_positions(ids_row: torch.Tensor,
# #                            think_id: Optional[int] = None,
# #                            think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
# #     """
# #     Return start positions where the marker occurs in input_ids row.
# #     - If think_token_seq is provided, match that subsequence.
# #     - Else use single-token think_id.
# #     """
# #     ids = ids_row.tolist()
# #     pos: List[int] = []
# #     if think_token_seq and len(think_token_seq) > 0:
# #         pat = list(think_token_seq)
# #         m, n = len(pat), len(ids)
# #         for i in range(0, n - m + 1):
# #             if ids[i:i+m] == pat:
# #                 pos.append(i)
# #         return pos
# #     if think_id is not None:
# #         for i, t in enumerate(ids):
# #             if t == think_id:
# #                 pos.append(i)
# #     return pos

# # def _draw_think_marks(ax, positions: Sequence[int], S: int):
# #     for t in positions or []:
# #         if 0 <= t < S:
# #             ax.axvline(t, ls="--", lw=0.8, color="w")
# #             ax.axhline(t, ls="--", lw=0.8, color="w")
# #             ax.scatter([t], [t], s=8, c="red", marker="o")

# # def _project_marks_to_size(positions: Sequence[int], S_src: int, S_tgt: int) -> List[int]:
# #     if S_src <= 0 or S_tgt <= 0:
# #         return []
# #     out = []
# #     for p in positions or []:
# #         q = int(np.floor(p * S_tgt / max(1, S_src)))
# #         if 0 <= q < S_tgt:
# #             out.append(q)
# #     return sorted(set(out))

# # def _save_grid(fig, path, dpi=180) -> str:
# #     fig.tight_layout()
# #     path = _unique_path(path)
# #     fig.savefig(path, dpi=dpi)
# #     plt.close(fig)
# #     print(f"[attn_reductions] saved: {path}")
# #     return path

# # def _save_line(fig, path, dpi=200) -> str:
# #     fig.tight_layout()
# #     path = _unique_path(path)
# #     fig.savefig(path, dpi=dpi)
# #     plt.close(fig)
# #     print(f"[attn_reductions] saved: {path}")
# #     return path


# # # =========================
# # # reductions (CPU-safe)
# # # =========================
# # def _strided_pool_maps(A_last: torch.Tensor, sizes=(8,16,32)) -> List[torch.Tensor]:
# #     # A_last: (B,S,S) on CPU
# #     assert A_last.device.type == "cpu"
# #     X = A_last.unsqueeze(1)  # (B,1,S,S)
# #     out = []
# #     B, S, _ = A_last.shape
# #     for k in sizes:
# #         stride = max(1, S // k)
# #         out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
# #     return out

# # def _gauss_blur_pool_maps(A_last: torch.Tensor, sizes=(8,16,32), sigma=1.0) -> List[torch.Tensor]:
# #     assert A_last.device.type == "cpu"
# #     B, S, _ = A_last.shape
# #     X = A_last.unsqueeze(1)  # (B,1,S,S)
# #     rad = int(3*sigma + 0.5)
# #     t = torch.arange(-rad, rad+1, dtype=A_last.dtype)
# #     g = torch.exp(-(t**2)/(2*sigma**2)); g = g / g.sum()
# #     k2 = (g[:,None] @ g[None,:]).unsqueeze(0).unsqueeze(0)  # (1,1,K,K)
# #     X = F.conv2d(X, k2, padding=rad)
# #     out = []
# #     for k in sizes:
# #         stride = max(1, S // k)
# #         out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
# #     return out

# # def _distance_binned_profile(A_last: torch.Tensor, bins=64) -> torch.Tensor:
# #     # (B,S,S) â†’ (B,bins), mean attention per |i-j| bin (CPU)
# #     assert A_last.device.type == "cpu"
# #     B, S, _ = A_last.shape
# #     dists = torch.arange(S, dtype=torch.long)
# #     D = (dists[None,:] - dists[:,None]).abs().unsqueeze(0)  # (1,S,S)
# #     A_use = A_last
# #     edges = torch.linspace(0, S-1, bins+1)
# #     idx = torch.bucketize(D, edges) - 1                     # (1,S,S) in [0,bins-1]
# #     profiles = []
# #     for b in range(bins):
# #         m = (idx == b)
# #         num = (A_use * m).sum(dim=(1,2))
# #         den = m.sum(dim=(1,2)).clamp_min(1)
# #         profiles.append(num / den)
# #     return torch.stack(profiles, dim=-1)  # (B,bins)

# # def _topk_sparse_pool(A_last: torch.Tensor, ratios=(0.01, 0.05, 0.10), size=32) -> List[torch.Tensor]:
# #     # CPU top-k per row â†’ sparse â†’ pooled
# #     assert A_last.device.type == "cpu"
# #     B, S, _ = A_last.shape
# #     out = []
# #     for r in ratios:
# #         k = max(1, int(S * r))
# #         vals, idx = torch.topk(A_last, k, dim=-1)           # (B,S,k) long indices
# #         M = torch.zeros_like(A_last)
# #         M.scatter_(-1, idx.long(), 1.0)                     # 1 at top-k keys
# #         Sparse = A_last * M
# #         stride = max(1, S // size)
# #         out.append(F.avg_pool2d(Sparse.unsqueeze(1), stride, stride, ceil_mode=True).squeeze(1))
# #     return out

# # def _kmeans1d_assign(S: int, K: int, iters=8) -> torch.Tensor:
# #     # CPU 1D k-means on positions
# #     x = torch.linspace(0, 1, S, dtype=torch.float32).unsqueeze(-1)   # (S,1)
# #     centers = torch.linspace(0, 1, K, dtype=torch.float32).unsqueeze(-1)
# #     for _ in range(iters):
# #         d = (x - centers.T)**2                                       # (S,K)
# #         assign = d.argmin(dim=1)                                     # (S,) long
# #         for k in range(K):
# #             pts = x[assign == k]
# #             centers[k] = pts.mean(dim=0) if pts.numel() else centers[k]
# #     return assign.long()

# # def _cluster_pool(A_last: torch.Tensor, Ks=(16,32,64)) -> List[torch.Tensor]:
# #     # CPU positional clusters â†’ clusterÃ—cluster map
# #     assert A_last.device.type == "cpu"
# #     B, S, _ = A_last.shape
# #     out = []
# #     for K in Ks:
# #         K = min(K, S)
# #         assign = _kmeans1d_assign(S, K)                   # (S,) long
# #         H = F.one_hot(assign, K).to(A_last.dtype)         # (S,K)
# #         Ht = H.t().unsqueeze(0).expand(B, -1, -1)         # (B,K,S)
# #         Hb = H.unsqueeze(0).expand(B, -1, -1)             # (B,S,K)
# #         Cluster = torch.bmm(Ht, torch.bmm(A_last, Hb))    # (B,K,K)
# #         cnt = H.sum(dim=0).clamp_min(1.0)                 # (K,)
# #         Cluster = Cluster / (cnt.unsqueeze(0).unsqueeze(-1) * cnt.unsqueeze(0).unsqueeze(1))
# #         out.append(Cluster)
# #     return out

# # def _landmark_pool(A_last: torch.Tensor, Ms=(16,32,64)) -> List[torch.Tensor]:
# #     # CPU landmarks by out-degree
# #     assert A_last.device.type == "cpu"
# #     B, S, _ = A_last.shape
# #     out = []
# #     outdeg = A_last.sum(-1)                            # (B,S)
# #     for M in Ms:
# #         M = min(M, S)
# #         idx = torch.topk(outdeg, M, dim=-1).indices.long()   # (B,M)
# #         batch = torch.arange(B, dtype=torch.long)[:, None]   # (B,1)
# #         L = A_last[batch, idx][:, :, idx]                    # (B,M,M)
# #         sort_idx = idx.sort(dim=-1).indices
# #         L = L[batch, sort_idx][:, :, sort_idx]
# #         out.append(L)
# #     return out

# # def _pyramid_maps(A_last: torch.Tensor, scales=(8,16,32)) -> List[torch.Tensor]:
# #     # same as strided pool, just named as pyramid (multi-scale)
# #     return _strided_pool_maps(A_last, sizes=scales)


# # # =========================
# # # Internal/External (span-based) helpers
# # # =========================
# # def _find_internal_span(input_ids_row: torch.Tensor,
# #                         close_positions: List[int],
# #                         think_open_seq: Optional[Sequence[int]] = None,
# #                         internal_window: int = 128) -> Optional[Tuple[int, int]]:
# #     """
# #     Return (start, end) token indices for the internal region.
# #     - Prefer the last '<think>' ... '</think>' span if think_open_seq is provided.
# #     - Else fallback to window: [max(0, t_close - internal_window), t_close)
# #     Excludes the close token itself.
# #     """
# #     if not close_positions:
# #         return None
# #     t_close = close_positions[-1]  # last close
# #     if think_open_seq and len(think_open_seq) > 0:
# #         ids = input_ids_row.tolist()
# #         pat = list(think_open_seq)
# #         last_open = -1
# #         for i in range(0, t_close - len(pat) + 1):
# #             if ids[i:i+len(pat)] == pat:
# #                 last_open = i
# #         if last_open >= 0 and last_open < t_close:
# #             return (last_open, t_close)
# #     s = max(0, t_close - int(internal_window))
# #     return (s, t_close)

# # def _int_ext_fractions(A: torch.Tensor, start: int, end: int) -> Tuple[float, float, float, float]:
# #     """
# #     A: (S,S) CPU attention (nonnegative). Internal = [start, end) ; External = complement.
# #     Returns fractions (II, IE, EI, EE) that sum to 1.
# #     """
# #     S = A.size(0)
# #     I = torch.zeros(S, dtype=torch.float32)
# #     I[start:end] = 1.0
# #     E = 1.0 - I
# #     M_II = torch.ger(I, I)
# #     M_IE = torch.ger(I, E)
# #     M_EI = torch.ger(E, I)
# #     M_EE = torch.ger(E, E)
# #     tot = A.sum().item() + 1e-12
# #     II = float((A * M_II).sum().item() / tot)
# #     IE = float((A * M_IE).sum().item() / tot)
# #     EI = float((A * M_EI).sum().item() / tot)
# #     EE = float((A * M_EE).sum().item() / tot)
# #     return II, IE, EI, EE

# # def _plot_int_ext_layers(A_layers: torch.Tensor,  # (L,S,S) CPU
# #                          start: int, end: int,
# #                          path: str):
# #     L, S, _ = A_layers.shape
# #     II_list, IE_list, EI_list = [], [], []
# #     for li in range(L):
# #         II, IE, EI, EE = _int_ext_fractions(A_layers[li], start, end)
# #         II_list.append(II); IE_list.append(IE); EI_list.append(EI)
# #     x = np.arange(L)
# #     fig = plt.figure(figsize=(6.5, 2.0))
# #     plt.plot(x, II_list, label="Iâ†’I", linewidth=1.2)
# #     plt.plot(x, IE_list, label="Iâ†’E", linewidth=1.0)
# #     plt.plot(x, EI_list, label="Eâ†’I", linewidth=1.0)
# #     plt.legend(loc="upper right", fontsize=7, framealpha=0.25)
# #     plt.title("Internal vs External attention (per layer)", fontsize=9)
# #     _save_line(fig, path, dpi=200)

# # def _plot_int_ext_last_bar(A_last: torch.Tensor,  # (S,S) CPU
# #                            start: int, end: int,
# #                            path: str):
# #     II, IE, EI, EE = _int_ext_fractions(A_last, start, end)
# #     vals = [II, IE, EI, EE]
# #     labels = ["Iâ†’I", "Iâ†’E", "Eâ†’I", "Eâ†’E"]
# #     fig = plt.figure(figsize=(4.5, 2.2))
# #     xpos = np.arange(len(vals))
# #     plt.bar(xpos, vals, width=0.6)
# #     for i, v in enumerate(vals):
# #         plt.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
# #     plt.title("Internal/External fractions (last layer)", fontsize=9)
# #     _save_line(fig, path, dpi=200)


# # # =========================
# # # NEW: Grouped Internal/External helpers
# # # =========================
# # def _group_intra_external(A_layers: torch.Tensor, K: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
# #     """
# #     A_layers: (L,S,S) CPU, nonnegative
# #     K: number of groups
# #     Returns:
# #       intra[L]   : sum diag(H^T A H) / sum(A)
# #       external[L]: 1 - intra[L]
# #       last_B(K,K): group block attention for last layer, normalised by total mass
# #     """
# #     L, S, _ = A_layers.shape
# #     K = max(1, min(int(K), S))

# #     # fixed positional k-means assignment (deterministic, CPU)
# #     assign = _kmeans1d_assign(S, K)                   # (S,)
# #     H = F.one_hot(assign, K).to(A_layers.dtype)       # (S,K)
# #     Ht = H.t()                                        # (K,S)

# #     intra = np.zeros(L, dtype=np.float32)
# #     external = np.zeros(L, dtype=np.float32)
# #     last_B = None

# #     for li in range(L):
# #         A = A_layers[li]                              # (S,S)
# #         B = Ht @ (A @ H)                              # (K,K)
# #         tot = float(A.sum().item() + 1e-12)
# #         intra_li = float(torch.trace(B).item()) / tot
# #         intra[li] = intra_li
# #         external[li] = max(0.0, 1.0 - intra_li)
# #         if li == L - 1:
# #             last_B = (B / tot).cpu().numpy()

# #     return intra, external, last_B

# # def _plot_group_intra_external_lines(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
# #     """
# #     3Ã—1 panels: for each K, plot intra vs external vs layers.
# #     """
# #     fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
# #     if len(Ks) == 1:
# #         axes = [axes]
# #     for ax, K in zip(axes, Ks):
# #         intra, ext, _ = _group_intra_external(A_layers, K)
# #         x = np.arange(len(intra))
# #         ax.plot(x, intra, label="intra", linewidth=1.2)
# #         ax.plot(x, ext,   label="external", linewidth=1.0)
# #         ax.set_title(f"group K={K}", fontsize=9)
# #         ax.legend(loc="upper right", fontsize=7, framealpha=0.25)
# #     _save_grid(fig, path, dpi=200)

# # def _plot_group_intra_external_bars(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
# #     """
# #     3Ã—1 panels: for each K, bar of last layer intra vs external.
# #     """
# #     L, S, _ = A_layers.shape
# #     fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
# #     if len(Ks) == 1:
# #         axes = [axes]
# #     for ax, K in zip(axes, Ks):
# #         intra, ext, _ = _group_intra_external(A_layers, K)
# #         vals = [float(intra[-1]), float(ext[-1])]
# #         xpos = np.arange(2)
# #         ax.bar(xpos, vals, width=0.6)
# #         ax.set_title(f"last layer (K={K})", fontsize=9)
# #         for i, v in enumerate(vals):
# #             ax.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
# #     _save_grid(fig, path, dpi=200)


# # # =========================
# # # plotting helpers
# # # =========================
# # def _imshow_small(ax, M: np.ndarray, title: str, think_pos: Sequence[int]):
# #     vmin, vmax = _robust_limits(M)
# #     ax.imshow(M, interpolation="nearest", aspect="auto",
# #               norm=colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax))
# #     S = M.shape[0]
# #     _draw_think_marks(ax, think_pos, S)
# #     ax.set_title(title, fontsize=8)

# # def _imshow_small_project(ax, M: np.ndarray, title: str,
# #                           think_pos_tokens: Sequence[int], S_src_tokens: int):
# #     # Project token indices into this mapâ€™s axis length
# #     S_tgt = M.shape[0]
# #     marks = _project_marks_to_size(think_pos_tokens, S_src_tokens, S_tgt)
# #     _imshow_small(ax, M, title, marks)

# # def _plot_line_profile(vec: np.ndarray, path: str, title="Distance-binned attention"):
# #     fig = plt.figure(figsize=(5, 1.6))
# #     x = np.arange(vec.shape[-1])
# #     plt.plot(x, vec, linewidth=1.0)
# #     plt.title(title, fontsize=9)
# #     _save_line(fig, path, dpi=200)


# # # =========================
# # # main API
# # # =========================
# # @torch.no_grad()
# # def plot_all_token_attn_reductions(
# #     input_ids: torch.Tensor,     # (B,S)
# #     attn_stack,                  # list[L] of (B,H,S,S) or (B,H,1,S)
# #     save_root: str,
# #     think_id: Optional[int] = 151668,
# #     think_token_seq: Optional[Sequence[int]] = None,
# #     think_open_seq: Optional[Sequence[int]] = None,   # optional "<think>" opener
# #     internal_window: int = 128,                        # fallback window
# #     batch_index: int = 0,
# #     tag: Optional[str] = None,
# # ):
# #     """
# #     Writes PNGs under {save_root}/attn_reductions/<method>/...  (small figs, 3Ã—1 where applicable).
# #     Shows </think> position(s) in all map-based plots (projected where needed). Unique filenames.
# #     Also plots Internal vs External (span-based) and Grouped Intra/External (Kâˆˆ{8,16,32}).
# #     """
# #     _ensure_dir(save_root)
# #     base = os.path.join(save_root, "attn_reductions")
# #     subdirs = {
# #         "token_full_single": os.path.join(base, "token_full", "single"),
# #         "token_full_last3":  os.path.join(base, "token_full", "last3"),
# #         "strided_pool":      os.path.join(base, "strided_pool"),
# #         "gauss_pool":        os.path.join(base, "gauss_pool"),
# #         "dist_binned":       os.path.join(base, "dist_binned"),
# #         "topk_pool":         os.path.join(base, "topk_pool"),
# #         "cluster_pool":      os.path.join(base, "cluster_pool"),
# #         "landmarks":         os.path.join(base, "landmarks"),
# #         "pyramid":           os.path.join(base, "pyramid"),
# #         "int_ext_ratio":     os.path.join(base, "int_ext_ratio"),
# #         "group_int_ext":     os.path.join(base, "group_int_ext"),
# #     }
# #     for d in subdirs.values():
# #         _ensure_dir(d)

# #     tag = tag or uuid.uuid4().hex[:8]
# #     b = batch_index

# #     # ---- unify attentions and select last layer ----
# #     A_all = _unify_attentions(attn_stack)  # (B,L,S,S)
# #     if A_all is None or A_all.size(1) == 0:
# #         print("[attn_reductions] no attention to plot")
# #         return

# #     # Move to CPU once; sanitize; keep shapes
# #     A_all_cpu = torch.nan_to_num(A_all.detach().float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0).cpu()
# #     A_last = A_all_cpu[b, -1].unsqueeze(0)  # (1,S,S) CPU
# #     B_, S, _ = A_last.shape

# #     # find all </think> positions (token indices)
# #     think_pos_tokens = _find_marker_positions(input_ids[b], think_id=think_id, think_token_seq=think_token_seq)

# #     print(f"[attn_reductions] S={S}, Layers={int(A_all_cpu.shape[1])}, think_positions={think_pos_tokens}")

# #     # === Normal token attention (comparison) ===
# #     try:
# #         print("[attn_reductions] plotting: token_full_single")
# #         M = A_last[0].numpy()
# #         fig, ax = plt.subplots(1, 1, figsize=(4, 4))
# #         _imshow_small(ax, M, "Token attention (last layer, full)", think_pos_tokens)
# #         _save_grid(fig, os.path.join(subdirs["token_full_single"], f"token_full_single_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:token_full_single] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: token_full_last3")
# #         L = int(A_all_cpu.shape[1])
# #         idxs = list(range(max(0, L - 3), L))
# #         panels = len(idxs)
# #         fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
# #         if panels == 1:
# #             axes = [axes]
# #         for ax, li in zip(axes, idxs):
# #             M = A_all_cpu[b, li].numpy()
# #             _imshow_small(ax, M, f"Layer {li}", think_pos_tokens)
# #         _save_grid(fig, os.path.join(subdirs["token_full_last3"], f"token_full_last3_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:token_full_last3] skipped: {e}")

# #     # === Reductions (3Ã—1 each where applicable) ===
# #     try:
# #         print("[attn_reductions] plotting: strided_pool (8,16,32)")
# #         maps = _strided_pool_maps(A_last, sizes=(8,16,32))
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         for ax, m, k in zip(axes, maps, (8,16,32)):
# #             M = m[0].numpy()
# #             _imshow_small_project(ax, M, f"avg k={k}", think_pos_tokens, S)
# #         _save_grid(fig, os.path.join(subdirs["strided_pool"], f"strided_pool_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:strided_pool] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: gauss_pool (8,16,32)")
# #         maps = _gauss_blur_pool_maps(A_last, sizes=(8,16,32), sigma=1.0)
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         for ax, m, k in zip(axes, maps, (8,16,32)):
# #             M = m[0].numpy()
# #             _imshow_small_project(ax, M, f"gauss k={k}", think_pos_tokens, S)
# #         _save_grid(fig, os.path.join(subdirs["gauss_pool"], f"gauss_pool_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:gauss_pool] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: dist_binned (bins=64)")
# #         prof = _distance_binned_profile(A_last, bins=64)[0].numpy()
# #         fig = plt.figure(figsize=(5, 1.6))
# #         x = np.arange(prof.shape[-1])
# #         plt.plot(x, prof, linewidth=1.0)
# #         plt.title("Distance-binned attention", fontsize=9)
# #         _save_line(fig, os.path.join(subdirs["dist_binned"], f"dist_binned_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:dist_binned] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: topk_pool (1%,5%,10%)")
# #         maps = _topk_sparse_pool(A_last, ratios=(0.01, 0.05, 0.10), size=32)
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         for ax, m, label in zip(axes, maps, ("top-1%", "top-5%", "top-10%")):
# #             M = m[0].numpy()
# #             _imshow_small_project(ax, M, label, think_pos_tokens, S)
# #         _save_grid(fig, os.path.join(subdirs["topk_pool"], f"topk_pool_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:topk_pool] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: cluster_pool (K=16,32,64)")
# #         Ks = (16,32,64)
# #         maps = _cluster_pool(A_last, Ks=Ks)
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         for ax, m, K in zip(axes, maps, Ks):
# #             assign = _kmeans1d_assign(S, K)
# #             marks = [int(assign[p].item()) for p in think_pos_tokens if 0 <= p < S]
# #             _imshow_small(ax, m[0].numpy(), f"cluster K={K}", marks)
# #         _save_grid(fig, os.path.join(subdirs["cluster_pool"], f"cluster_pool_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:cluster_pool] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: landmarks (M=16,32,64)")
# #         Ms_ = (16,32,64)
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         outdeg = A_last.sum(-1)[0]  # (S,)
# #         for ax, Msize in zip(axes, Ms_):
# #             Msize = min(Msize, S)
# #             idx = torch.topk(outdeg, Msize, dim=-1).indices.long()
# #             idx_sorted, order = idx.sort()
# #             pos_map = {int(tok.item()): int(i) for i, tok in enumerate(idx_sorted)}
# #             marks = [pos_map[p] for p in think_pos_tokens if p in pos_map]
# #             Lm = A_last[0][idx][:, idx]
# #             Lm = Lm[order][:, order]
# #             _imshow_small(ax, Lm.numpy(), f"landmarks M={Msize}", marks)
# #         _save_grid(fig, os.path.join(subdirs["landmarks"], f"landmarks_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:landmarks] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: pyramid (8,16,32)")
# #         maps = _pyramid_maps(A_last, scales=(8,16,32))
# #         fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
# #         for ax, m, k in zip(axes, maps, (8,16,32)):
# #             M = m[0].numpy()
# #             _imshow_small_project(ax, M, f"pyr k={k}", think_pos_tokens, S)
# #         _save_grid(fig, os.path.join(subdirs["pyramid"], f"pyramid_{tag}.png"))
# #     except Exception as e:
# #         print(f"[attn_reductions:pyramid] skipped: {e}")

# #     # === Span-based Internal vs External ===
# #     try:
# #         print("[attn_reductions] computing: internal/external ratios (span)")
# #         close_positions = think_pos_tokens
# #         span = _find_internal_span(input_ids[b], close_positions,
# #                                    think_open_seq=think_open_seq,
# #                                    internal_window=internal_window)
# #         if span is None:
# #             print("[attn_reductions:int_ext_ratio] skipped (no </think> found)")
# #         else:
# #             s_int, e_int = span
# #             print(f"[attn_reductions:int_ext_ratio] internal span: [{s_int}, {e_int})")
# #             print("[attn_reductions] plotting: int_ext_ratio per-layer line")
# #             _plot_int_ext_layers(
# #                 A_layers=A_all_cpu[b],
# #                 start=s_int, end=e_int,
# #                 path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_layers_{tag}.png")
# #             )
# #             print("[attn_reductions] plotting: int_ext_ratio last-layer bar")
# #             _plot_int_ext_last_bar(
# #                 A_last=A_last[0],
# #                 start=s_int, end=e_int,
# #                 path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_last_{tag}.png")
# #             )
# #     except Exception as e:
# #         print(f"[attn_reductions:int_ext_ratio] skipped: {e}")

# #     # === Grouped Internal vs External ===
# #     try:
# #         print("[attn_reductions] plotting: group_int_ext lines (K=8,16,32)")
# #         _plot_group_intra_external_lines(
# #             A_layers=A_all_cpu[b],
# #             path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_lines_{tag}.png"),
# #             Ks=(8,16,32),
# #         )
# #     except Exception as e:
# #         print(f"[attn_reductions:group_int_ext_lines] skipped: {e}")

# #     try:
# #         print("[attn_reductions] plotting: group_int_ext bars (K=8,16,32)")
# #         _plot_group_intra_external_bars(
# #             A_layers=A_all_cpu[b],
# #             path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_bars_{tag}.png"),
# #             Ks=(8,16,32),
# #         )
# #     except Exception as e:
# #         print(f"[attn_reductions:group_int_ext_bars] skipped: {e}")


# #**********************************************************************************************************************
# #myedit

# # ===========================================
# # Strong-but-compact correctness head stack
# # ===========================================


# # ----------------------------
# # Small helpers (stable & safe)
# # ----------------------------
# def _safe_dtype_param(module: nn.Module):
#     for p in module.parameters():
#         return p.dtype
#     return torch.bfloat16

# def _num_groups(c: int, g: int = 8) -> int:
#     for k in [g, 6, 4, 3, 2, 1]:
#         if c % k == 0:
#             return k
#     return 1

# class LinearLN(nn.Module):
#     def __init__(self, d_in: int, d_out: int, pdrop: float = 0.0):
#         super().__init__()
#         self.ln = nn.LayerNorm(d_in)
#         self.fc = nn.Linear(d_in, d_out)
#         self.act = nn.GELU()
#         self.drop = nn.Dropout(pdrop)
#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         return self.drop(self.act(self.fc(self.ln(x))))

# def percentile(x: torch.Tensor, q: float, dim: Optional[int] = None, keepdim: bool = False) -> torch.Tensor:
#     # q in [0,1]
#     n = x.shape[dim] if dim is not None else x.numel()
#     k = max(1, int(n * q))
#     if dim is None:
#         vals, _ = torch.kthvalue(x.view(-1), k)
#         return vals
#     vals, _ = torch.kthvalue(x, k, dim=dim, keepdim=keepdim)
#     return vals

# def masked_area_downsample_1d(x: torch.Tensor, mask: torch.Tensor, k: int, eps: float = 1e-6) -> torch.Tensor:
#     """
#     x:    (B,C,N)
#     mask: (B,1,N)  with 1=valid
#     return: (B,C,k)  area interpolation weighted by valid counts
#     """
#     B, C, N = x.shape
#     assert mask.shape == (B, 1, N), f"mask shape must be (B,1,N), got {mask.shape}"
#     num = F.interpolate(x * mask, size=k, mode="area")
#     den = F.interpolate(mask,   size=k, mode="area")
#     return num / (den + eps)


# # ----------------------------
# # Squeeze-and-Excitation blocks
# # ----------------------------
# class SE2d(nn.Module):
#     def __init__(self, c: int, r: int = 8):
#         super().__init__()
#         hidden = max(4, c // r)
#         self.fc1 = nn.Conv2d(c, hidden, kernel_size=1)
#         self.fc2 = nn.Conv2d(hidden, c, kernel_size=1)
#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         s = F.adaptive_avg_pool2d(x, 1)
#         s = F.gelu(self.fc1(s))
#         s = torch.sigmoid(self.fc2(s))
#         return x * s

# class SE1d(nn.Module):
#     def __init__(self, c: int, r: int = 8):
#         super().__init__()
#         hidden = max(4, c // r)
#         self.fc1 = nn.Conv1d(c, hidden, kernel_size=1)
#         self.fc2 = nn.Conv1d(hidden, c, kernel_size=1)
#     def forward(self, x: torch.Tensor) -> torch.Tensor:
#         s = F.adaptive_avg_pool1d(x, 1)
#         s = F.gelu(self.fc1(s))
#         s = torch.sigmoid(self.fc2(s))
#         return x * s


# # ======================================================
# # Feature Extractor: ATTENTION (B,L,H,k,k) -> (B, D_ATT)
# # ======================================================
# class AttnFeatureExtractorLite(nn.Module):
#     """
#     Stronger variant:
#       - 2Ã—Conv2d GN+GELU + residual + SE2d over each (l,h) map
#       - project per-(l,h) token via global avg/max + fp32 stats6
#       - optional TransformerEncoder token mixer across T=(L*H) tokens
#       - multi-query attentive pooling â†’ z_att
#     """
#     def __init__(
#         self,
#         D_ATT: int = 384,
#         d_tok: int = 128,
#         c: int = 64,
#         pool_queries: int = 4,
#         max_layers: int = 64,
#         max_heads: int = 128,
#         pdrop: float = 0.10,
#         mixer_layers: int = 1,           # 0/1/2; auto-disabled if T too big
#         mixer_heads: int = 4,
#         mixer_enable_T_max: int = 2048,  # disable mixer if L*H > this
#     ):
#         super().__init__()
#         self.c = int(c)
#         self.d_tok = int(d_tok)
#         self.K = int(pool_queries)
#         self.mixer_layers = int(mixer_layers)
#         self.mixer_enable_T_max = int(mixer_enable_T_max)

#         # 2D conv stem with residual + SE
#         self.conv1 = nn.Conv2d(1, c, 3, padding=1)
#         self.gn1   = nn.GroupNorm(_num_groups(c), c)
#         self.conv2 = nn.Conv2d(c, c, 3, padding=1)
#         self.gn2   = nn.GroupNorm(_num_groups(c), c)
#         self.se2d  = SE2d(c, r=8)

#         # projection to tokens (avg/max + stats6 â†’ d_tok)
#         self.proj = LinearLN(2 * c + 6, d_tok)

#         # positional embeddings across (layer, head)
#         self.layer_emb = nn.Embedding(max_layers, d_tok)
#         self.head_emb  = nn.Embedding(max_heads, d_tok)

#         # optional token mixer across T=L*H
#         if self.mixer_layers > 0:
#             enc = nn.TransformerEncoderLayer(
#                 d_model=d_tok, nhead=mixer_heads, dim_feedforward=2 * d_tok,
#                 dropout=pdrop * 0.5, activation="gelu", batch_first=True
#             )
#             self.tok_mixer = nn.TransformerEncoder(
#                 enc, num_layers=self.mixer_layers, norm=nn.LayerNorm(d_tok)
#             )
#         else:
#             self.tok_mixer = None

#         # multi-query attentive pooling
#         self.query = nn.Parameter(torch.randn(self.K, d_tok) * (1.0 / math.sqrt(d_tok)))
#         self.out   = nn.Sequential(
#             LinearLN(self.K * d_tok, D_ATT, pdrop=pdrop),
#             nn.Dropout(pdrop),
#         )

#     @torch.no_grad()
#     def _stats6(self, A: torch.Tensor) -> torch.Tensor:
#         """
#         A: (B,1,k,k)  in fp32
#         returns (B,6): mean, var, entropy(rowwise), diag_ratio, p90, sparsity
#         """
#         A2   = A.flatten(2)                    # (B,1,k*k)
#         mean = A2.mean(dim=-1)                 # (B,1)
#         var  = A2.var(dim=-1, unbiased=False)  # (B,1)

#         rows = A.squeeze(1)                    # (B,k,k)
#         p    = F.softmax(rows, dim=-1).clamp_min(1e-8)
#         ent  = (-(p * p.log()).sum(dim=-1).mean(dim=-1)).unsqueeze(-1)  # (B,1)

#         diag = torch.diagonal(rows, dim1=-2, dim2=-1)   # (B,k)
#         diag_ratio = (diag.abs().sum(dim=-1) / (rows.abs().sum(dim=(-1, -2)) + 1e-6)).unsqueeze(-1)

#         p90  = percentile(A2, 0.90, dim=-1)             # (B,1)

#         A2_abs = A2.abs()
#         thr    = (A2_abs.mean(dim=-1) * 1e-3)           # (B,1)
#         sparse = (A2_abs < thr.unsqueeze(-1)).float().mean(dim=-1)  # (B,1)

#         return torch.cat([mean, var, ent, diag_ratio, p90, sparse], dim=-1)  # (B,6)

#     def _encode_maps_to_tokens(self, maps_blhkk: torch.Tensor, l_idx: torch.Tensor, h_idx: torch.Tensor) -> torch.Tensor:
#         """
#         maps_blhkk: (B*L*H, 1, k, k)  already reduced
#         return toks: (B, L*H, d_tok)
#         """
#         B_L_H = maps_blhkk.shape[0]
#         T = l_idx.numel()  # L*H
#         assert B_L_H % T == 0, "maps batch does not match L*H"
#         B = int(B_L_H // T)

#         # conv stem with residual + SE (compute in fp32, keep dtype)
#         x = maps_blhkk
#         y = F.gelu(self.gn1(self.conv1(x).to(torch.float32))).to(x.dtype)
#         z = F.gelu(self.gn2(self.conv2(y).to(torch.float32))).to(x.dtype)
#         z = self.se2d(z)
#         z = z + y  # residual (same channels)

#         gavg = F.adaptive_avg_pool2d(z, 1).flatten(1)  # (B*L*H, c)
#         gmax = F.adaptive_max_pool2d(z, 1).flatten(1)  # (B*L*H, c)

#         with autocast("cuda", enabled=False):
#             stats = self._stats6(maps_blhkk.to(torch.float32))  # (B*L*H, 6)

#         tok = torch.cat([gavg, gmax, stats.to(gavg.dtype)], dim=-1)  # (B*L*H, 2c+6)
#         tok = self.proj(tok)                                         # (B*L*H, d_tok)

#         # add (layer, head) pos embeddings (broadcast over batch)
#         device = maps_blhkk.device
#         lpe = self.layer_emb(l_idx.to(device))      # (T, d_tok)
#         hpe = self.head_emb(h_idx.to(device))       # (T, d_tok)
#         pe  = (lpe + hpe).unsqueeze(0).expand(B, -1, -1)  # (B, T, d_tok)

#         toks = tok.view(B, T, self.d_tok) + pe      # (B, T, d_tok)

#         # optional token mixer (guard for very large T)
#         if self.tok_mixer is not None and T <= self.mixer_enable_T_max:
#             toks = self.tok_mixer(toks)

#         return toks

#     def forward(self, attn: torch.Tensor, mask_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:
#         """
#         attn: (B,L,H,k,k)  already reduced by backbone
#         """
#         assert attn.dim() == 5, f"expected (B,L,H,k,k), got {tuple(attn.shape)}"
#         B, L, H, k, k2 = attn.shape
#         assert k == k2, "attention must be square"
#         device = attn.device

#         maps = attn.reshape(B * L * H, 1, k, k)

#         l_idx = torch.arange(L, device=device).repeat_interleave(H)  # (L*H,)
#         h_idx = torch.arange(H, device=device).repeat(L)             # (L*H,)

#         toks   = self._encode_maps_to_tokens(maps, l_idx, h_idx)  # (B, T, d_tok)
#         q      = self.query                                       # (K, d_tok)
#         scores = torch.einsum('btd,kd->bkt', toks, q) / math.sqrt(self.d_tok)  # (B,K,T)
#         weights= F.softmax(scores, dim=2)
#         pooled = torch.einsum('bkt,btd->bkd', weights, toks).reshape(B, self.K * self.d_tok)

#         return self.out(pooled)  # (B, D_ATT)


# # ==================================================
# # Feature Extractor: CONF (B,S) -> (B, D_CONF)
# # ==================================================
# class ConfFeatureExtractorLite(nn.Module):
#     """
#     Stronger variant:
#       - mask-aware area downsample to k_conf
#       - 3Ã—Conv1d (GN+GELU) with residual + SE1d
#       - global avg/max + fp32 stats5
#       - LinearLN to D_CONF
#     """
#     def __init__(self, D_CONF: int = 128, k_conf: int = 192, c: int = 48, pdrop: float = 0.10):
#         super().__init__()
#         self.k_conf = int(k_conf)
#         self.c = int(c)

#         self.conv1 = nn.Conv1d(1,   c, 5, padding=2)
#         self.gn1   = nn.GroupNorm(_num_groups(c), c)
#         self.conv2 = nn.Conv1d(c,   c, 5, padding=2)
#         self.gn2   = nn.GroupNorm(_num_groups(c), c)
#         self.conv3 = nn.Conv1d(c,   c, 5, padding=2)
#         self.gn3   = nn.GroupNorm(_num_groups(c), c)
#         self.se1d  = SE1d(c, r=8)
#         self.drop  = nn.Dropout(pdrop)

#         self.proj  = LinearLN(2 * c + 5, D_CONF, pdrop=pdrop)

#     @torch.no_grad()
#     def _stats5(self, x: torch.Tensor) -> torch.Tensor:
#         """
#         x: (B,1,k) fp32
#         returns (B,5): mean, var, total variation, max slope, p90
#         """
#         mean = x.mean(dim=-1)                        # (B,1)
#         var  = x.var(dim=-1, unbiased=False)         # (B,1)
#         dx   = x[..., 1:] - x[..., :-1]              # (B,1,k-1)
#         tv   = dx.abs().sum(dim=-1)                  # (B,1)
#         mxs  = dx.abs().amax(dim=-1) if dx.shape[-1] > 0 else torch.zeros_like(mean)
#         p90  = percentile(x, 0.90, dim=-1)           # (B,1)
#         return torch.cat([mean, var, tv, mxs, p90], dim=-1)  # (B,5)

#     def forward(self, conf: torch.Tensor, mask_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:
#         """
#         conf: (B,S)
#         mask_tokens: (B,S)  1=valid
#         """
#         B, S = conf.shape
#         x = conf.detach().unsqueeze(1)  # (B,1,S)
#         if mask_tokens is None:
#             m = torch.ones(B, 1, S, device=conf.device, dtype=conf.dtype)
#         else:
#             m = mask_tokens.detach().unsqueeze(1).to(conf.dtype)

#         with autocast("cuda", enabled=False):
#             x_ds = masked_area_downsample_1d(x.to(torch.float32), m.to(torch.float32), self.k_conf)  # (B,1,k)
#         x_ds = x_ds.to(conf.dtype)

#         # 3Ã—Conv1d with residual + SE
#         y = F.gelu(self.gn1(self.conv1(x_ds).to(torch.float32))).to(x_ds.dtype)
#         z = F.gelu(self.gn2(self.conv2(y).to(torch.float32))).to(x_ds.dtype)
#         z = F.gelu(self.gn3(self.conv3(z).to(torch.float32))).to(x_ds.dtype)
#         z = self.se1d(z)
#         z = self.drop(z + y)

#         gavg = F.adaptive_avg_pool1d(z, 1).flatten(1)  # (B,c)
#         gmax = F.adaptive_max_pool1d(z, 1).flatten(1)  # (B,c)

#         with autocast("cuda", enabled=False):
#             stats = self._stats5(x_ds.to(torch.float32))  # (B,5)

#         vec = torch.cat([gavg, gmax, stats.to(gavg.dtype)], dim=-1)  # (B,2c+5)
#         return self.proj(vec)  # (B, D_CONF)


# # ==========================================================
# # Feature Extractor: LAST HIDDEN (B,S,Hd) -> (B, D_HID)
# # ==========================================================
# class HiddenFeatureExtractorLite(nn.Module):
#     """
#     Stronger variant:
#       - LN + Linear to d_tok
#       - mask-aware area downsample to k_hid
#       - 2Ã—Conv1d (depthwise-ish via groups) + residual + SE1d
#       - optional tiny token mixer across time (disabled by default)
#       - 3 learnable queries pool â†’ pooled vector
#       - add fp32 dynamic stats6 â†’ LinearLN to D_HID
#     """
#     def __init__(
#         self,
#         D_model: int,
#         D_HID: int = 256,
#         k_hid: int = 192,
#         d_tok: int = 192,
#         groups: int = 8,
#         pdrop: float = 0.10,
#         mixer_layers: int = 0,        # keep 0 by default; set 1 if you want
#         mixer_heads: int = 4,
#         mixer_enable_k_max: int = 512,
#     ):
#         super().__init__()
#         self.k_hid = int(k_hid)
#         self.d_tok = int(d_tok)
#         self.norm  = nn.LayerNorm(D_model)
#         self.proj  = nn.Linear(D_model, d_tok)

#         g = min(groups, d_tok)
#         self.conv1 = nn.Conv1d(d_tok, d_tok, 5, padding=2, groups=g)
#         self.gn1   = nn.GroupNorm(_num_groups(d_tok), d_tok)
#         self.conv2 = nn.Conv1d(d_tok, d_tok, 5, padding=2, groups=g)
#         self.gn2   = nn.GroupNorm(_num_groups(d_tok), d_tok)
#         self.se1d  = SE1d(d_tok, r=8)
#         self.drop  = nn.Dropout(pdrop)

#         # optional small mixer across time tokens (k)
#         self.mixer_enable_k_max = int(mixer_enable_k_max)
#         if mixer_layers > 0:
#             enc = nn.TransformerEncoderLayer(
#                 d_model=d_tok, nhead=mixer_heads, dim_feedforward=2 * d_tok,
#                 dropout=pdrop * 0.5, activation="gelu", batch_first=True
#             )
#             self.tok_mixer = nn.TransformerEncoder(
#                 enc, num_layers=int(mixer_layers), norm=nn.LayerNorm(d_tok)
#             )
#         else:
#             self.tok_mixer = None

#         # learnable queries
#         self.query = nn.Parameter(torch.randn(3, d_tok) * (1.0 / math.sqrt(d_tok)))

#         self.out = nn.Sequential(
#             LinearLN(3 * d_tok + 6, D_HID, pdrop=pdrop),
#             nn.Dropout(pdrop),
#         )

#     @torch.no_grad()
#     def _dyn6(self, seq_mean: torch.Tensor) -> torch.Tensor:
#         """
#         seq_mean: (B,k) fp32  (mean over channels)
#         returns (B,6): mean, var, TV, p90|Î”|, early-late diff, last val
#         """
#         B, k = seq_mean.shape
#         mean = seq_mean.mean(dim=-1, keepdim=True)
#         var  = seq_mean.var(dim=-1, unbiased=False, keepdim=True)
#         if k >= 2:
#             d    = seq_mean[:, 1:] - seq_mean[:, :-1]
#             tv   = d.abs().sum(dim=-1, keepdim=True)
#             p90d = percentile(d.abs(), 0.90, dim=-1, keepdim=True)
#         else:
#             tv   = torch.zeros(B, 1, device=seq_mean.device, dtype=seq_mean.dtype)
#             p90d = torch.zeros_like(tv)
#         w  = max(1, k // 3)
#         el = (seq_mean[:, :w].mean(dim=-1, keepdim=True) - seq_mean[:, -w:].mean(dim=-1, keepdim=True))
#         last = seq_mean[:, -1:].contiguous()
#         return torch.cat([mean, var, tv, p90d, el, last], dim=-1)

#     def forward(self, last_hidden: torch.Tensor, mask_tokens: Optional[torch.Tensor] = None) -> torch.Tensor:
#         """
#         last_hidden: (B,S,D_model)
#         mask_tokens: (B,S)  1=valid
#         """
#         B, S, Dm = last_hidden.shape
#         x = self.proj(self.norm(last_hidden.detach()))  # (B,S,d_tok)
#         x = x.permute(0, 2, 1).contiguous()             # (B,d_tok,S)

#         if mask_tokens is None:
#             m = torch.ones(B, 1, S, device=last_hidden.device, dtype=last_hidden.dtype)
#         else:
#             m = mask_tokens.detach().unsqueeze(1).to(last_hidden.dtype)

#         with autocast("cuda", enabled=False):
#             x_ds = masked_area_downsample_1d(x.to(torch.float32), m.to(torch.float32), self.k_hid)  # (B,d_tok,k)
#         x_ds = x_ds.to(last_hidden.dtype)

#         y = F.gelu(self.gn1(self.conv1(x_ds).to(torch.float32))).to(x_ds.dtype)
#         z = F.gelu(self.gn2(self.conv2(y).to(torch.float32))).to(x_ds.dtype)
#         z = self.se1d(z)
#         z = self.drop(z + y)   # residual

#         # optional tiny mixer across time tokens
#         tokens = z.permute(0, 2, 1).contiguous()  # (B,k,d_tok)
#         if self.tok_mixer is not None and tokens.shape[1] <= self.mixer_enable_k_max:
#             tokens = self.tok_mixer(tokens)

#         scores = torch.einsum('btd,kd->bkt', tokens, self.query) / math.sqrt(self.d_tok)
#         weights = F.softmax(scores, dim=2)
#         pooled = torch.einsum('bkt,btd->bkd', weights, tokens).reshape(B, 3 * self.d_tok)

#         with autocast("cuda", enabled=False):
#             seq_mean = tokens.to(torch.float32).mean(dim=-1)  # (B,k)
#             dyn = self._dyn6(seq_mean)                        # (B,6)

#         vec = torch.cat([pooled, dyn.to(pooled.dtype)], dim=-1)  # (B, 3*d_tok + 6)
#         return self.out(vec)  # (B, D_HID)


# # =========================
# # Correctness head (toggable)
# # =========================
# class CorrectnessHeadLite(nn.Module):
#     """
#     Slightly deeper MLP with PreNorm and two hidden layers.
#     - Chooses input dim from *enabled* features only.
#     - Does NOT accept missing tensors for enabled features.
#     """
#     def __init__(
#         self,
#         D_ATT: int,
#         D_CONF: int,
#         D_HID: int,
#         pdrop: float = 0.10,
#         *,
#         use_attn: bool = True,
#         use_conf: bool = True,
#         use_hid:  bool = True,
#     ):
#         super().__init__()
#         self.use_attn = use_attn
#         self.use_conf = use_conf
#         self.use_hid  = use_hid

#         D = (D_ATT if use_attn else 0) + (D_CONF if use_conf else 0) + (D_HID if use_hid else 0)
#         if D <= 0:
#             raise ValueError("CorrectnessHeadLite: at least one feature must be enabled.")

#         self.ln = nn.LayerNorm(D)
#         self.net = nn.Sequential(
#             nn.Linear(D, 256),
#             nn.GELU(),
#             nn.Dropout(pdrop),
#             nn.Linear(256, 128),
#             nn.GELU(),
#             nn.Dropout(pdrop),
#             nn.Linear(128, 1),
#         )

#     def forward(
#         self,
#         z_att: Optional[torch.Tensor],
#         z_conf: Optional[torch.Tensor],
#         z_hid: Optional[torch.Tensor],
#     ) -> torch.Tensor:
#         chunks = []
#         if self.use_attn:
#             if z_att is None:
#                 raise RuntimeError("CorrectnessHeadLite: use_attn=True but z_att is None (no zeros are fed).")
#             chunks.append(z_att)
#         if self.use_conf:
#             if z_conf is None:
#                 raise RuntimeError("CorrectnessHeadLite: use_conf=True but z_conf is None (no zeros are fed).")
#             chunks.append(z_conf)
#         if self.use_hid:
#             if z_hid is None:
#                 raise RuntimeError("CorrectnessHeadLite: use_hid=True but z_hid is None (no zeros are fed).")
#             chunks.append(z_hid)

#         x = torch.cat(chunks, dim=-1)
#         # x_rand = torch.randn_like(x)                       # same shape/device/dtype as x
#         # return self.net(self.ln(x_rand))                   # (B, 1)
#         return self.net(self.ln(x))  # (B,1)
















# logger = logging.get_logger(__name__)

# _CHECKPOINT_FOR_DOC = "Qwen/Qwen3-8B"
# _CONFIG_FOR_DOC = "Qwen3Config"


# def assert_finite(**named_tensors):
#     """Raise if any tensor contains NaN/Inf."""
#     for k, v in named_tensors.items():
#         if not torch.all(torch.isfinite(v)):
#             raise FloatingPointError(f"{k} has NaN/Inf")


# #myedit
# @dataclass
# class CausalLMOutputWithPast(ModelOutput):
#     """
#     Outputs for causal language modeling with an optional stop-probability head.

#     Args:
#         loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
#             Training loss (e.g., LM loss or stop-head loss).
#         logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
#             Pre-softmax scores from the LM head.
#         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):
#             A nested tuple with length `config.num_hidden_layers`. Each inner tuple contains
#             key/value tensors of shape `(batch_size, num_heads, seq_len, head_dim)`.
#             Can be fed to speed up sequential decoding.
#         hidden_states (`tuple(torch.FloatTensor)`, *optional*):
#             Tuple of hidden states (output of embeddings + each layer), each of shape
#             `(batch_size, sequence_length, hidden_size)`.
#         attentions (`tuple(torch.FloatTensor)`, *optional*):
#             Tuple of attention maps for each layer, each of shape
#             `(batch_size, num_heads, sequence_length, sequence_length)`.
#         stop_prob (`torch.FloatTensor` of shape `(batch_size, sequence_length)` in train
#             or `(batch_size, 1, 1)` at inference, *optional*):
#             Probability from the stop head indicating that thinking should end at each position.
#     """
#     loss: Optional[torch.FloatTensor] = None
#     logits: torch.FloatTensor = None
#     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, ...], ...]] = None
#     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
#     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
#     stop_prob: Optional[torch.FloatTensor] = None



# class Qwen3RMSNorm(nn.Module):
#     def __init__(self, hidden_size, eps=1e-6):
#         """
#         Qwen3RMSNorm is equivalent to T5LayerNorm
#         """
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(hidden_size))
#         self.variance_epsilon = eps

#     def forward(self, hidden_states):
#         input_dtype = hidden_states.dtype
#         hidden_states = hidden_states.to(torch.float32)
#         variance = hidden_states.pow(2).mean(-1, keepdim=True)
#         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
#         return self.weight * hidden_states.to(input_dtype)

#     def extra_repr(self):
#         return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


# class Qwen3MLP(nn.Module):
#     def __init__(self, config):
#         super().__init__()
#         self.config = config
#         self.hidden_size = config.hidden_size
#         self.intermediate_size = config.intermediate_size
#         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
#         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
#         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
#         self.act_fn = ACT2FN[config.hidden_act]

#     def forward(self, x):
#         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
#         return down_proj


# def rotate_half(x):
#     """Rotates half the hidden dims of the input."""
#     x1 = x[..., : x.shape[-1] // 2]
#     x2 = x[..., x.shape[-1] // 2 :]
#     return torch.cat((-x2, x1), dim=-1)


# def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
#     """Applies Rotary Position Embedding to the query and key tensors.

#     Args:
#         q (`torch.Tensor`): The query tensor.
#         k (`torch.Tensor`): The key tensor.
#         cos (`torch.Tensor`): The cosine part of the rotary embedding.
#         sin (`torch.Tensor`): The sine part of the rotary embedding.
#         position_ids (`torch.Tensor`, *optional*):
#             Deprecated and unused.
#         unsqueeze_dim (`int`, *optional*, defaults to 1):
#             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
#             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
#             that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
#             k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
#             cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
#             the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
#     Returns:
#         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
#     """
#     cos = cos.unsqueeze(unsqueeze_dim)
#     sin = sin.unsqueeze(unsqueeze_dim)
#     q_embed = (q * cos) + (rotate_half(q) * sin)
#     k_embed = (k * cos) + (rotate_half(k) * sin)
#     return q_embed, k_embed


# def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
#     """
#     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
#     num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
#     """
#     batch, num_key_value_heads, slen, head_dim = hidden_states.shape
#     if n_rep == 1:
#         return hidden_states
#     hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
#     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


# def eager_attention_forward(
#     module: nn.Module,
#     query: torch.Tensor,
#     key: torch.Tensor,
#     value: torch.Tensor,
#     attention_mask: Optional[torch.Tensor],
#     scaling: float,
#     dropout: float = 0.0,
#     **kwargs,
# ):
#     key_states = repeat_kv(key, module.num_key_value_groups)
#     value_states = repeat_kv(value, module.num_key_value_groups)

#     attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

#     if attention_mask is not None:
#         # Accept 2D [B,S] or 4D [B,1,Q,K]
#         if attention_mask.dim() == 2:
#             # 0/1 or bool -> additive mask with -inf for masked keys
#             m = (attention_mask != 0).to(attn_weights.dtype)       # [B,S]
#             add = (1.0 - m) * torch.finfo(attn_weights.dtype).min  # [B,S]
#             attention_mask = add[:, None, None, :]                 # [B,1,1,S]
#     if attention_mask is not None:
#         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
#         attn_weights = attn_weights + causal_mask

#     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
#     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
#     attn_output = torch.matmul(attn_weights, value_states)
#     attn_output = attn_output.transpose(1, 2).contiguous()

#     return attn_output, attn_weights


# class Qwen3Attention(nn.Module):
#     """Multi-headed attention from 'Attention Is All You Need' paper"""

#     def __init__(self, config: Qwen3Config, layer_idx: int):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx
#         self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
#         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
#         self.scaling = self.head_dim**-0.5
#         self.attention_dropout = config.attention_dropout
#         self.is_causal = True

#         self.q_proj = nn.Linear(
#             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.k_proj = nn.Linear(
#             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.v_proj = nn.Linear(
#             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
#         )
#         self.o_proj = nn.Linear(
#             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
#         )
#         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
#         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
#         self.sliding_window = config.sliding_window
#         if not (
#             self.config.use_sliding_window
#             and getattr(self.config, "sliding_window", None) is not None
#             and self.layer_idx >= self.config.max_window_layers
#         ):
#             self.sliding_window = None

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         position_embeddings: Tuple[torch.Tensor, torch.Tensor],
#         attention_mask: Optional[torch.Tensor],
#         past_key_value: Optional[Cache] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **kwargs: Unpack[FlashAttentionKwargs],
#     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         input_shape = hidden_states.shape[:-1]
#         hidden_shape = (*input_shape, -1, self.head_dim)

#         query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
#         key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
#         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

#         cos, sin = position_embeddings
#         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

#         if past_key_value is not None:
#             # sin and cos are specific to RoPE models; cache_position needed for the static cache
#             cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
#             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

#         #myedit
#         # attention_interface: Callable = eager_attention_forward
#         # if self.config._attn_implementation != "eager":
#         #     if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
#         #         logger.warning_once(
#         #             "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
#         #             'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
#         #         )
#         #     else:
#         #         attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
#         # --- choose kernel -------------------------------------------------------
#         want_weights = kwargs.get("output_attentions", False)

#         if want_weights:                       # <â”€â”€ we need A = softmax(qkáµ€/âˆšd)
#             attention_interface = eager_attention_forward   # eager can return it
#         else:                                   # keep the fast kernel
#             attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]


#         attn_output, attn_weights = attention_interface(
#             self,
#             query_states,
#             key_states,
#             value_states,
#             attention_mask,
#             dropout=0.0 if not self.training else self.attention_dropout,
#             scaling=self.scaling,
#             sliding_window=self.sliding_window,  # diff with Llama
#             **kwargs,
#         )

#         attn_output = attn_output.reshape(*input_shape, -1).contiguous()
#         attn_output = self.o_proj(attn_output)
#         return attn_output, attn_weights

# # class Qwen3Attention(nn.Module):
# #     """Multi-headed attention from 'Attention Is All You Need' paper"""

# #     def __init__(self, config: Qwen3Config, layer_idx: int):
# #         super().__init__()
# #         self.config = config
# #         self.layer_idx = layer_idx
# #         self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
# #         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
# #         self.scaling = self.head_dim**-0.5
# #         self.attention_dropout = config.attention_dropout
# #         self.is_causal = True

# #         self.q_proj = nn.Linear(
# #             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
# #         )
# #         self.k_proj = nn.Linear(
# #             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
# #         )
# #         self.v_proj = nn.Linear(
# #             config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
# #         )
# #         self.o_proj = nn.Linear(
# #             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
# #         )
# #         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
# #         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
# #         self.sliding_window = config.sliding_window
# #         if not (
# #             self.config.use_sliding_window
# #             and getattr(self.config, "sliding_window", None) is not None
# #             and self.layer_idx >= self.config.max_window_layers
# #         ):
# #             self.sliding_window = None

# #     def forward(
# #         self,
# #         hidden_states: torch.Tensor,
# #         position_embeddings: Tuple[torch.Tensor, torch.Tensor],
# #         attention_mask: Optional[torch.Tensor],
# #         past_key_value: Optional[Cache] = None,
# #         cache_position: Optional[torch.LongTensor] = None,
# #         **kwargs: Unpack[FlashAttentionKwargs],
# #     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
# #         input_shape = hidden_states.shape[:-1]
# #         hidden_shape = (*input_shape, -1, self.head_dim)

# #         query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)  # [B,H,S,D]
# #         key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)    # [B,H_kv,S,D]
# #         value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)               # [B,H_kv,S,D]

# #         cos, sin = position_embeddings
# #         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

# #         if past_key_value is not None:
# #             # sin and cos are specific to RoPE models; cache_position needed for the static cache
# #             cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
# #             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

# #         # --- choose kernel (always non-eager) ---------------------------------
# #         attn_impl = getattr(self.config, "_attn_implementation", "sdpa")
# #         attention_interface = ALL_ATTENTION_FUNCTIONS[attn_impl]

# #         attn_output, _ = attention_interface(
# #             self,
# #             query_states,
# #             key_states,
# #             value_states,
# #             attention_mask,
# #             dropout=0.0 if not self.training else self.attention_dropout,
# #             scaling=self.scaling,
# #             sliding_window=self.sliding_window,  # diff with Llama
# #             **kwargs,
# #         )

# #         # --- manual attn weights from QK^T, no softmax, bf16 L1 normalization -
# #         attn_weights = None
# #         if kwargs.get("output_attentions", False):
# #             # Expand KV heads to match Q heads
# #             k = repeat_kv(key_states, self.num_key_value_groups)  # [B,H,S_k,D]
# #             q = query_states                                      # [B,H,S_q,D]

# #             # scores in fp32, scale; we won't softmax
# #             scores = torch.matmul(q.float(), k.transpose(-2, -1).float()) * float(self.scaling)  # [B,H,S_q,S_k]

# #             # Build allowed (True=keep) from masks
# #             allowed = None
# #             if attention_mask is not None:
# #                 if attention_mask.dim() == 2:
# #                     keep = (attention_mask != 0)[:, None, None, :k.size(-2)]  # [B,1,1,S_k]
# #                 else:
# #                     if attention_mask.dtype == torch.bool:
# #                         # SDPA-style: True means masked; we want keep
# #                         keep = ~attention_mask[..., :k.size(-2)]
# #                     else:
# #                         # Additive mask: 0 for keep, large -ve for masked
# #                         keep = (attention_mask[..., :k.size(-2)].float() > -1e30)
# #                 allowed = keep

# #             # causal lower-tri
# #             if self.is_causal:
# #                 S_q, S_k = scores.size(-2), scores.size(-1)
# #                 causal = torch.ones((S_q, S_k), dtype=torch.bool, device=scores.device).triu(1)
# #                 causal = ~causal[None, None, :, :]  # True=keep
# #                 allowed = causal if allowed is None else (allowed & causal)

# #             # optional sliding window band
# #             if self.sliding_window is not None:
# #                 S_q, S_k = scores.size(-2), scores.size(-1)
# #                 i = torch.arange(S_q, device=scores.device)[:, None]
# #                 j = torch.arange(S_k, device=scores.device)[None, :]
# #                 band = (i - j) <= self.sliding_window
# #                 band = band & (i >= j)
# #                 band = band[None, None, :, :]
# #                 allowed = band if allowed is None else (allowed & band)

# #             if allowed is not None:
# #                 scores = scores.masked_fill(~allowed, 0.0)

# #             # bf16 row-wise L1 normalization (memory/compute friendly)
# #             eps = torch.tensor(1e-6, device=scores.device, dtype=torch.bfloat16)
# #             scores_bf16 = scores.to(torch.bfloat16)
# #             denom = scores_bf16.abs().sum(dim=-1, keepdim=True).clamp_min(eps)  # [B,H,S_q,1]
# #             attn_weights = (scores_bf16 / denom).to(q.dtype)  # [B,H,S_q,S_k]

# #         attn_output = attn_output.reshape(*input_shape, -1).contiguous()
# #         attn_output = self.o_proj(attn_output)
# #         return attn_output, attn_weights



# class Qwen3DecoderLayer(nn.Module):
#     def __init__(self, config: Qwen3Config, layer_idx: int):
#         super().__init__()
#         self.hidden_size = config.hidden_size
#         self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)
#         self.mlp = Qwen3MLP(config)
#         self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         if (
#             config.sliding_window and config._attn_implementation != "flash_attention_2"
#         ):  # diff with Llama is this warning
#             logger.warning_once(
#                 f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
#                 "unexpected results may be encountered."
#             )

#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_value: Optional[Cache] = None,
#         output_attentions: Optional[bool] = False,
#         use_cache: Optional[bool] = False,
#         cache_position: Optional[torch.LongTensor] = None,
#         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
#         **kwargs: Unpack[FlashAttentionKwargs],
#     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
#         residual = hidden_states

#         hidden_states = self.input_layernorm(hidden_states)

#         # Self Attention
#         hidden_states, self_attn_weights = self.self_attn(
#             hidden_states=hidden_states,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_value=past_key_value,
#             output_attentions=output_attentions,
#             use_cache=use_cache,
#             cache_position=cache_position,
#             position_embeddings=position_embeddings,
#             **kwargs,
#         )
#         hidden_states = residual + hidden_states

#         # Fully Connected
#         residual = hidden_states
#         hidden_states = self.post_attention_layernorm(hidden_states)
#         hidden_states = self.mlp(hidden_states)
#         hidden_states = residual + hidden_states

#         outputs = (hidden_states,)

#         # #myedit
#         # if output_attentions and self_attn_weights is not None:
#         #     # (B , n_heads , S , S) â†’ (B , 1 , S , S)
#         #     self_attn_weights = self_attn_weights.mean(dim=1, keepdim=True)

#         if output_attentions:
#             outputs += (self_attn_weights,)

#         return outputs


# class Qwen3RotaryEmbedding(nn.Module):
#     def __init__(self, config: Qwen3Config, device=None):
#         super().__init__()
#         # BC: "rope_type" was originally "type"
#         if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
#             self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
#         else:
#             self.rope_type = "default"
#         self.max_seq_len_cached = config.max_position_embeddings
#         self.original_max_seq_len = config.max_position_embeddings

#         self.config = config
#         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

#         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
#         self.register_buffer("inv_freq", inv_freq, persistent=False)
#         self.original_inv_freq = self.inv_freq

#     @torch.no_grad()
#     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
#     def forward(self, x, position_ids):
#         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
#         position_ids_expanded = position_ids[:, None, :].float()

#         device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
#         with torch.autocast(device_type=device_type, enabled=False):  # Force float32
#             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
#             emb = torch.cat((freqs, freqs), dim=-1)
#             cos = emb.cos() * self.attention_scaling
#             sin = emb.sin() * self.attention_scaling

#         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


# QWEN3_START_DOCSTRING = r"""
#     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
#     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
#     etc.)

#     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
#     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
#     and behavior.

#     Parameters:
#         config ([`Qwen3Config`]):
#             Model configuration class with all the parameters of the model. Initializing with a config file does not
#             load the weights associated with the model, only the configuration. Check out the
#             [`~PreTrainedModel.from_pretrained`] method to load the model weights.
# """


# @add_start_docstrings(
#     "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3PreTrainedModel(PreTrainedModel):
#     config_class = Qwen3Config
#     base_model_prefix = "model"
#     supports_gradient_checkpointing = True
#     _no_split_modules = ["Qwen3DecoderLayer"]
#     _skip_keys_device_placement = ["past_key_values"]
#     _supports_flash_attn_2 = True
#     _supports_sdpa = True
#     _supports_flex_attn = True
#     _supports_cache_class = True
#     _supports_quantized_cache = True
#     _supports_static_cache = True
#     _supports_attention_backend = True

#     def _init_weights(self, module):
#         std = self.config.initializer_range
#         if isinstance(module, nn.Linear):
#             module.weight.data.normal_(mean=0.0, std=std)
#             if module.bias is not None:
#                 module.bias.data.zero_()
#         elif isinstance(module, nn.Embedding):
#             module.weight.data.normal_(mean=0.0, std=std)
#             if module.padding_idx is not None:
#                 module.weight.data[module.padding_idx].zero_()



# QWEN3_INPUTS_DOCSTRING = r"""
#     Args:
#         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
#             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
#             it.

#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
#             [`PreTrainedTokenizer.__call__`] for details.

#             [What are input IDs?](../glossary#input-ids)
#         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
#             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

#             - 1 for tokens that are **not masked**,
#             - 0 for tokens that are **masked**.

#             [What are attention masks?](../glossary#attention-mask)

#             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
#             [`PreTrainedTokenizer.__call__`] for details.

#             If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
#             `past_key_values`).

#             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
#             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
#             information on the default strategy.

#             - 1 indicates the head is **not masked**,
#             - 0 indicates the head is **masked**.
#         position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#             Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
#             config.n_positions - 1]`.

#             [What are position IDs?](../glossary#position-ids)
#         past_key_values (`Cache`, *optional*):
#             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
#             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
#             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

#             It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

#             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
#             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
#             of shape `(batch_size, sequence_length)`.
#         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
#             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
#             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
#             model's internal embedding lookup matrix.
#         use_cache (`bool`, *optional*):
#             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
#             `past_key_values`).
#         output_attentions (`bool`, *optional*):
#             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
#             tensors for more detail.
#         output_hidden_states (`bool`, *optional*):
#             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
#             more detail.
#         return_dict (`bool`, *optional*):
#             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
#         cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
#             Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
#             this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
#             the complete sequence length.
# """


# @add_start_docstrings(
#     "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3Model(Qwen3PreTrainedModel):
#     """
#     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Qwen3DecoderLayer`]

#     Args:
#         config: Qwen3Config
#     """

#     def __init__(self, config: Qwen3Config):
#         super().__init__(config)
#         self.padding_idx = config.pad_token_id
#         self.vocab_size = config.vocab_size

#         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
#         self.layers = nn.ModuleList(
#             [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
#         )
#         self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
#         self.rotary_emb = Qwen3RotaryEmbedding(config=config)
#         self.gradient_checkpointing = False

#         # --- added: target reduced attention grid size (kÃ—k) ---
#         self.k_att = int(getattr(config, "stop_k_att", 48))

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.embed_tokens

#     def set_input_embeddings(self, value):
#         self.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
#     ) -> BaseModelOutputWithPast:
#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
#         output_hidden_states = (
#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
#         )
#         use_cache = use_cache if use_cache is not None else self.config.use_cache

#         if (input_ids is None) ^ (inputs_embeds is not None):
#             raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

#         if self.gradient_checkpointing and self.training and use_cache:
#             logger.warning_once(
#                 "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
#             )
#             use_cache = False

#         # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
#         if not isinstance(past_key_values, (type(None), Cache)):
#             raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

#         if inputs_embeds is None:
#             inputs_embeds = self.embed_tokens(input_ids)

#         if use_cache and past_key_values is None:
#             past_key_values = DynamicCache()

#         if cache_position is None:
#             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
#             cache_position = torch.arange(
#                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
#             )

#         if position_ids is None:
#             position_ids = cache_position.unsqueeze(0)

#         causal_mask = self._update_causal_mask(
#             attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
#         )

#         hidden_states = inputs_embeds

#         # create position embeddings to be shared across the decoder layers
#         position_embeddings = self.rotary_emb(hidden_states, position_ids)

#         # decoder layers
#         all_hidden_states = () if output_hidden_states else None
#         all_self_attns = () if output_attentions else None

#         # â”€â”€â”€ main layer loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#         for idx, decoder_layer in enumerate(self.layers):
#             need_attn = output_attentions and (idx % 5 == 4)
#             if output_hidden_states:
#                 all_hidden_states += (hidden_states,)
#             # print(causal_mask.shape)
#             if self.gradient_checkpointing and self.training:
#                 layer_outputs = self._gradient_checkpointing_func(
#                     partial(decoder_layer.__call__, **flash_attn_kwargs),
#                     hidden_states,
#                     causal_mask,
#                     position_ids,
#                     past_key_values,
#                     # output_attentions,
#                     need_attn,
#                     use_cache,
#                     cache_position,
#                     position_embeddings,
#                 )
#             else:
#                 layer_outputs = decoder_layer(
#                     hidden_states,
#                     attention_mask=causal_mask,
#                     position_ids=position_ids,
#                     past_key_value=past_key_values,
#                     output_attentions=output_attentions,
#                     use_cache=use_cache,
#                     cache_position=cache_position,
#                     position_embeddings=position_embeddings,
#                     **flash_attn_kwargs,
#                 )

#             hidden_states = layer_outputs[0]

#             if need_attn:
#                 # (B, H, S, S)
#                 attn_full = layer_outputs[1]

#                 # --- added: reduction logic (mask-aware area downsample to kÃ—k) ---
#                 with torch.no_grad():
#                     # downsample to (B, H, k, k) in fp32 for numerical stability
#                     A_small = F.interpolate(attn_full.to(torch.float32), size=(self.k_att, self.k_att), mode="area")

#                     # if a 2D attention_mask is provided (B, S), build a (B,1,k,k) grid mask and apply
#                     if attention_mask is not None and attention_mask.dim() == 2:
#                         mt = attention_mask.to(A_small.dtype).unsqueeze(1)  # (B,1,S)
#                         m_ds = F.interpolate(mt, size=self.k_att, mode="area").clamp_(0, 1)  # (B,1,k)
#                         grid = torch.bmm(m_ds.transpose(1, 2), m_ds).unsqueeze(1)            # (B,1,k,k)
#                         A_small = A_small * grid                                             # broadcast over H

#                     # sanitize and cast back to original dtype
#                     A_small = A_small.clamp_min_(0).to(attn_full.dtype)

#                 # free big tensors ASAP
#                 del attn_full
#                 all_self_attns += (A_small.to(hidden_states.dtype).detach(),)

#         hidden_states = self.norm(hidden_states)

#         # add hidden states from the last decoder layer
#         if output_hidden_states:
#             all_hidden_states += (hidden_states,)

#         return BaseModelOutputWithPast(
#             last_hidden_state=hidden_states,
#             past_key_values=past_key_values if use_cache else None,
#             hidden_states=all_hidden_states,
#             attentions=all_self_attns,
#         )

#     def _update_causal_mask(
#         self,
#         attention_mask: torch.Tensor,
#         input_tensor: torch.Tensor,
#         cache_position: torch.Tensor,
#         past_key_values: Cache,
#         output_attentions: bool = False,
#     ):
#         if self.config._attn_implementation == "flash_attention_2":
#             if attention_mask is not None and past_key_values is not None:
#                 is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
#                 if is_padding_right:
#                     raise ValueError(
#                         "You are attempting to perform batched generation with padding_side='right'"
#                         " this may lead to unexpected behaviour for Flash Attention version of Qwen3. Make sure to "
#                         " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
#                     )
#             if attention_mask is not None and 0.0 in attention_mask:
#                 return attention_mask
#             return None

#         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in
#         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail
#         # to infer the attention mask.
#         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
#         using_static_cache = isinstance(past_key_values, StaticCache)
#         using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)

#         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward
#         if (
#             self.config._attn_implementation == "sdpa"
#             and not (using_static_cache or using_sliding_window_cache)
#             and not output_attentions
#         ):
#             if AttentionMaskConverter._ignore_causal_mask_sdpa(
#                 attention_mask,
#                 inputs_embeds=input_tensor,
#                 past_key_values_length=past_seen_tokens,
#                 sliding_window=self.config.sliding_window,
#                 is_training=self.training,
#             ):
#                 return None

#         dtype, device = input_tensor.dtype, input_tensor.device
#         min_dtype = torch.finfo(dtype).min
#         sequence_length = input_tensor.shape[1]
#         # SlidingWindowCache or StaticCache
#         if using_sliding_window_cache or using_static_cache:
#             target_length = past_key_values.get_max_cache_shape()
#         # DynamicCache or no cache
#         else:
#             target_length = (
#                 attention_mask.shape[-1]
#                 if isinstance(attention_mask, torch.Tensor)
#                 else past_seen_tokens + sequence_length + 1
#             )

#         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).
#         causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
#             attention_mask,
#             sequence_length=sequence_length,
#             target_length=target_length,
#             dtype=dtype,
#             device=device,
#             cache_position=cache_position,
#             batch_size=input_tensor.shape[0],
#             config=self.config,
#             past_key_values=past_key_values,
#         )

#         if (
#             self.config._attn_implementation == "sdpa"
#             and attention_mask is not None
#             and attention_mask.device.type in ["cuda", "xpu"]
#             and not output_attentions
#         ):
#             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
#             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
#             # Details: https://github.com/pytorch/pytorch/issues/110213
#             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

#         return causal_mask

#     @staticmethod
#     def _prepare_4d_causal_attention_mask_with_cache_position(
#         attention_mask: torch.Tensor,
#         sequence_length: int,
#         target_length: int,
#         dtype: torch.dtype,
#         device: torch.device,
#         cache_position: torch.Tensor,
#         batch_size: int,
#         config: Qwen3Config,
#         past_key_values: Cache,
#     ):
#         """
#         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
#         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

#         Args:
#             attention_mask (`torch.Tensor`):
#                 A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
#             sequence_length (`int`):
#                 The sequence length being processed.
#             target_length (`int`):
#                 The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
#             dtype (`torch.dtype`):
#                 The dtype to use for the 4D attention mask.
#             device (`torch.device`):
#                 The device to place the 4D attention mask on.
#             cache_position (`torch.Tensor`):
#                 Indices depicting the position of the input sequence tokens in the sequence.
#             batch_size (`torch.Tensor`):
#                 Batch size.
#             config (`Qwen3Config`):
#                 The model's configuration class
#             past_key_values (`Cache`):
#                 The cache class that is being used currently to generate
#         """
#         if attention_mask is not None and attention_mask.dim() == 4:
#             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
#             causal_mask = attention_mask
#         else:
#             min_dtype = torch.finfo(dtype).min
#             causal_mask = torch.full(
#                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device
#             )
#             diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
#             if config.sliding_window is not None:
#                 # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also
#                 # the check is needed to verify is current checkpoint was trained with sliding window or not
#                 if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:
#                     sliding_attend_mask = torch.arange(target_length, device=device) <= (
#                         cache_position.reshape(-1, 1) - config.sliding_window
#                     )
#                     diagonal_attend_mask.bitwise_or_(sliding_attend_mask)
#             causal_mask *= diagonal_attend_mask
#             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
#             if attention_mask is not None:
#                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
#                 if attention_mask.shape[-1] > target_length:
#                     attention_mask = attention_mask[:, :target_length]
#                 mask_length = attention_mask.shape[-1]
#                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
#                     causal_mask.device
#                 )
#                 padding_mask = padding_mask == 0
#                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
#                     padding_mask, min_dtype
#                 )
#         return causal_mask



# class KwargsForCausalLM(FlashAttentionKwargs): ...



# # =========================
# # Model with togglable features
# # =========================
# class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
#     _tied_weights_keys = ["lm_head.weight"]
#     _tp_plan = {"lm_head": "colwise_rep"}
#     _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

#     def __init__(self, config):
#         super().__init__(config)
#         self.model = Qwen3Model(config)
#         self.vocab_size = config.vocab_size
#         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

#         # ---- feature toggles (simple booleans) ----
#         self.use_stop_attn = bool(getattr(config, "use_stop_attn", True))
#         self.use_stop_conf = bool(getattr(config, "use_stop_conf", True))
#         self.use_stop_hid  = bool(getattr(config, "use_stop_hid",  True))

#         # ---- dims (safe defaults) ----
#         D_ATT  = int(getattr(config, "stop_att_dim", 384))
#         D_CONF = int(getattr(config, "stop_conf_dim", 128))
#         D_HID  = int(getattr(config, "stop_hid_dim", 256))

#         k_conf = int(getattr(config, "stop_k_conf", 192))
#         k_hid  = int(getattr(config, "stop_k_hid", 192))

#         max_layers = int(getattr(config, "num_hidden_layers", 32))
#         max_heads  = int(getattr(config, "num_attention_heads", 32))

#         # ---- aux modules (only init those that are enabled) ----
#         if self.use_stop_attn:
#             self.attn_extractor = AttnFeatureExtractorLite(
#                 D_ATT=D_ATT, d_tok=128, c=64, pool_queries=4,
#                 max_layers=max_layers, max_heads=max_heads, pdrop=0.10,
#                 mixer_layers=1, mixer_heads=4, mixer_enable_T_max=2048
#             )
#         else:
#             self.attn_extractor = None

#         if self.use_stop_conf:
#             self.conf_extractor = ConfFeatureExtractorLite(D_CONF=D_CONF, k_conf=k_conf, c=48, pdrop=0.10)
#         else:
#             self.conf_extractor = None

#         if self.use_stop_hid:
#             self.hid_extractor  = HiddenFeatureExtractorLite(
#                 D_model=config.hidden_size, D_HID=D_HID,
#                 k_hid=k_hid, d_tok=192, groups=8, pdrop=0.10,
#                 mixer_layers=0, mixer_heads=4, mixer_enable_k_max=512
#             )
#         else:
#             self.hid_extractor = None

#         # correctness head sized to the *enabled* features only
#         self.stop_head = CorrectnessHeadLite(
#             D_ATT=D_ATT, D_CONF=D_CONF, D_HID=D_HID, pdrop=0.10,
#             use_attn=self.use_stop_attn, use_conf=self.use_stop_conf, use_hid=self.use_stop_hid,
#         )

#         # thresholds (optional)
#         self.stop_threshold   = float(getattr(config, "stop_threshold", 0.5))
#         self.stop_token_id    = int(getattr(config, "stop_token_id", 151668))  # "</think>" default
#         self.newline_token_id = int(getattr(config, "newline_token_id", 198))

#         # expose for init/freeze utilities (only the ones that exist)
#         self._custom_head_names = []
#         if self.attn_extractor is not None: self._custom_head_names.append("attn_extractor")
#         if self.conf_extractor is not None: self._custom_head_names.append("conf_extractor")
#         if self.hid_extractor  is not None: self._custom_head_names.append("hid_extractor")
#         self._custom_head_names.append("stop_head")

#         # init weights -> will be properly (re)initialized by your post-load routine
#         self.post_init()

#         # freeze base; train only aux modules that exist
#         trainable_prefixes = tuple(self._custom_head_names)
#         for n, p in self.named_parameters():
#             if n.startswith(trainable_prefixes):
#                 p.requires_grad_(True)
#             else:
#                 p.requires_grad_(False)

#     # ----------------------------------------------------------------------------------
#     # Aux correctness scorer (returns sequence-level probability; shape (B,1))
#     # ----------------------------------------------------------------------------------
#     def _should_stop(
#         self,
#         last_hidden: torch.Tensor,                 # (B,S,Hd)
#         attn_stack: Optional[List[torch.Tensor]],  # length L: each is (B,H,k,k)  <-- already reduced
#         token_probs: torch.Tensor,                 # (B,S)
#         mask: torch.Tensor,                        # (B,S) 1=valid
#         input_ids: Optional[torch.Tensor] = None,  # unused in lite head
#     ) -> torch.Tensor:
#         """
#         Returns a single scalar probability per sequence: (B,1).
#         Assumes attention maps are already reduced to (B,L,H,k,k).
#         """
#         B, S, _ = last_hidden.shape
#         out_dtype = _safe_dtype_param(self.stop_head)

#         # sanitize inputs early
#         token_probs_s = torch.clamp(torch.nan_to_num(token_probs, nan=1.0, posinf=1.0, neginf=1e-8), 1e-8, 1.0)
#         last_hidden_s = torch.nan_to_num(last_hidden, nan=0.0, posinf=0.0, neginf=0.0)

#         # validate attention stack only if attention feature is enabled
#         A = None
#         if self.use_stop_attn:
#             if attn_stack is None or len(attn_stack) == 0:
#                 raise RuntimeError("use_stop_attn=True but no reduced attentions were provided.")
#             with torch.no_grad():
#                 A = torch.stack(attn_stack, dim=1).detach()  # (B,L,H,k,k)
#                 if A.dim() != 5 or A.shape[-1] != A.shape[-2]:
#                     raise RuntimeError(f"Expected reduced attn (B,L,H,k,k), got {tuple(A.shape)}")
#                 A = torch.nan_to_num(A, nan=0.0, posinf=0.0, neginf=0.0)

#         with torch.amp.autocast("cuda", dtype=torch.bfloat16, enabled=True):
#             # build features ONLY for enabled extractors (no zero fill)
#             z_att = None
#             z_conf = None
#             z_hid = None

#             if self.use_stop_attn:
#                 z_att = self.attn_extractor(A.to(_safe_dtype_param(self.attn_extractor)), mask_tokens=mask)

#             if self.use_stop_conf:
#                 z_conf = self.conf_extractor(token_probs_s.to(_safe_dtype_param(self.conf_extractor)),
#                                              mask_tokens=mask)

#             if self.use_stop_hid:
#                 z_hid  = self.hid_extractor(last_hidden_s.to(_safe_dtype_param(self.hid_extractor)),
#                                             mask_tokens=mask)

#             # sanitize produced features (if any)
#             if z_att  is not None: z_att  = torch.nan_to_num(z_att,  nan=0.0, posinf=0.0, neginf=0.0)
#             if z_conf is not None: z_conf = torch.nan_to_num(z_conf, nan=0.0, posinf=0.0, neginf=0.0)
#             if z_hid  is not None: z_hid  = torch.nan_to_num(z_hid,  nan=0.0, posinf=0.0, neginf=0.0)

#             logits = self.stop_head(
#                 z_att.to(out_dtype)  if z_att  is not None else None,
#                 z_conf.to(out_dtype) if z_conf is not None else None,
#                 z_hid.to(out_dtype)  if z_hid  is not None else None,
#             )  # (B,1)

#         # stable sigmoid in fp32, sanitize & clamp to (0,1)
#         logits = torch.nan_to_num(logits, nan=0.0, posinf=0.0, neginf=0.0)
#         with torch.amp.autocast("cuda", enabled=False):
#             probs = torch.sigmoid(logits.to(torch.float32))  # (B,1)
#         probs = torch.clamp(torch.nan_to_num(probs, nan=0.5, posinf=1.0, neginf=0.0), 1e-6, 1.0 - 1e-6).to(out_dtype)
#         return probs  # (B,1)

#     # (rest of your class â€” forward(), compute_stop_loss(), get/set embeddings, etc. â€” unchanged)



#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     def get_output_embeddings(self):
#         return self.lm_head

#     def set_output_embeddings(self, new_embeddings):
#         self.lm_head = new_embeddings

#     def set_decoder(self, decoder):
#         self.model = decoder

#     def get_decoder(self):
#         return self.model

#     # #hazard loss
#     # def compute_stop_loss(self, p, target, mask, eps=1e-6):
#     #     """
#     #     p      : (B,S) stop probabilities in (0,1)
#     #     target : (B,S) binary 1 at ground-truth </think> position
#     #     mask   : (B,S) 1 for real tokens, 0 for pads
#     #     """
#     #     # ---- sanitize & fp32 ----
#     #     p      = torch.nan_to_num(p.float(),    nan=0.5)
#     #     target = torch.nan_to_num(target.float(), nan=0.0)
#     #     mask   = torch.nan_to_num(mask.float(),   nan=0.0)

#     #     # Keep strictly inside (0,1) to avoid log(0) and log1p(-1)
#     #     p = p.clamp(eps, 1.0 - eps)

#     #     # ---- hazard -> event pmf q via MASKED log-softmax (no manual divide) ----
#     #     # log q_t = log p_t + sum_{i<t} log(1 - p_i)
#     #     log1m_p  = torch.log1p(-p) * mask                          # (B,S)
#     #     log_surv = torch.cumsum(log1m_p, dim=1) - log1m_p          # exclusive cumsum
#     #     logp     = torch.where(mask > 0, torch.log(p), torch.zeros_like(p))
#     #     logq_raw = logp + log_surv                                  # (B,S)

#     #     neg_inf = torch.finfo(logq_raw.dtype).min
#     #     logq_masked = torch.where(mask > 0, logq_raw, torch.full_like(logq_raw, neg_inf))
#     #     logq = torch.log_softmax(logq_masked, dim=1)                # (B,S), normalized over valid tokens
#     #     q    = torch.exp(logq)                                      # (B,S)

#     #     # ---- your distance objective ----
#     #     B, S = p.shape
#     #     t = torch.arange(S, device=p.device).unsqueeze(0)
#     #     t_star = target.argmax(dim=1, keepdim=True)                 # (B,1)
#     #     dist = (t - t_star).abs() * mask
#     #     loss_vec = (q * dist).sum(dim=1)

#     #     valid = target.sum(dim=1) > 0
#     #     loss = loss_vec[valid].mean() if valid.any() else loss_vec.mean()

#     #     # final safety: replace non-finite (keeps scaler alive on edge cases)
#     #     if not torch.isfinite(loss):
#     #         loss = torch.zeros((), device=p.device, dtype=torch.float32)
#     #     return loss

#     #Correctness BCE loss
#     def compute_stop_loss(
#         self,
#         probs_seq: torch.Tensor,           # (B,1) or (B,)
#         correctness_labels: torch.Tensor,  # (B,) or (B,1); supports -1 to skip
#         eps: float = 1e-6,
#     ) -> torch.Tensor:
#         """
#         BCE on a single sequence-level probability per example.
#         - probs_seq: model's probability that the sequence is "correct" (0..1)
#         - correctness_labels: float/long in {-1, 0, 1}; -1 rows are skipped
#         """
#         # Shape & dtype hygiene
#         if probs_seq.dim() == 2 and probs_seq.size(-1) == 1:
#             probs_seq = probs_seq.squeeze(-1)                # (B,)
#         probs = torch.nan_to_num(probs_seq.float(), nan=0.5)
#         probs = probs.clamp(min=eps, max=1.0 - eps)

#         labels = correctness_labels
#         if labels.dim() == 2 and labels.size(-1) == 1:
#             labels = labels.squeeze(-1)                      # (B,)
#         labels = torch.nan_to_num(labels.float(), nan=0.0)

#         # Skip rows with label == -1
#         keep = labels.ne(-1.0)
#         if not torch.any(keep):
#             # preserve graph
#             return probs.sum() * 0.0

#         y = labels[keep].clamp_(0.0, 1.0)
#         p = probs[keep]

#         loss = F.binary_cross_entropy(p, y, reduction="mean")
#         if not torch.isfinite(loss):
#             # ultra-conservative fallback to keep scaler alive
#             loss = probs.sum() * 0.0
#         return loss

#     @can_return_tuple
#     @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         cache_position: Optional[torch.LongTensor] = None,
#         logits_to_keep: Union[int, torch.Tensor] = 0,
#         # ---- NEW kwargs for stop-head ----
#         token_probs_so_far: Optional[torch.Tensor] = None,   # (B,S) float in (0,1], used by conf extractor
#         apply_budget: Optional[bool] = None,
#         correctness_labels: Optional[torch.Tensor] = None,   # (B,) or (B,1), {-1,0,1}
#         **kwargs: Unpack[KwargsForCausalLM],
#     ) -> CausalLMOutputWithPast:
#         r"""
#             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
#                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
#                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

#             logits_to_keep (`int` or `torch.Tensor`, *optional*):
#                 If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
#                 `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
#                 token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
#                 If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
#                 This is useful when using packed tensor format (single dimension for batch and sequence length).

#         Returns:

#         Example:

#         ```python
#         >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

#         >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
#         >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

#         >>> prompt = "Hey, are you conscious? Can you talk to me?"
#         >>> inputs = tokenizer(prompt, return_tensors="pt")

#         >>> # Generate
#         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
#         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
#         "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
#         ```"""
#         output_attentions = (
#             True if (self.training or apply_budget)
#             else (self.config.output_attentions if output_attentions is None else output_attentions)
#         )
#         output_hidden_states = False

#         # ---- run base model under no_grad to keep base frozen & cheap ----
#         with torch.no_grad():
#             outputs: BaseModelOutputWithPast = self.model(
#                 input_ids=input_ids,
#                 attention_mask=attention_mask,
#                 position_ids=position_ids,
#                 past_key_values=past_key_values,
#                 inputs_embeds=inputs_embeds,
#                 use_cache=use_cache,
#                 output_attentions=output_attentions,
#                 output_hidden_states=False,
#                 cache_position=cache_position,
#                 **kwargs,
#             )

#             hidden_states = outputs.last_hidden_state          # (B,S,H)
#             logits        = self.lm_head(hidden_states)        # (B,S,V)

#             # Build token_probs_so_far if needed (for conf extractor)
#             if self.training and token_probs_so_far is None and labels is not None:
#                 logits_step = logits[:, :-1, :] if logits.size(1) == labels.size(1) + 1 else logits
#                 logp = torch.log_softmax(logits_step.float(), dim=-1)                # (B,S,V)
#                 tgt = labels.clamp(min=0)
#                 log_tok_p = logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)           # (B,S)
#                 log_tok_p = torch.where(labels.eq(-100), torch.zeros_like(log_tok_p), log_tok_p)
#                 token_probs_so_far = torch.clamp(log_tok_p.exp(), min=1e-8).to(logits_step.dtype).detach()

#         # ===============================
#         # TRAINING: sequence-level BCE
#         # ===============================
#         if self.training:
#             if correctness_labels is None:
#                 raise ValueError("`correctness_labels` (shape (B,) or (B,1), values in {-1,0,1}) is required for training.")

#             # Prepare inputs to _should_stop (expects reduced attn per layer: (B,H,k,k))
#             # Your backbone already returns reduced attentions; just pass them through.
#             mask = attention_mask.float() if attention_mask is not None else \
#                 torch.ones(hidden_states.size()[:2], device=hidden_states.device, dtype=torch.float)

#             stop_prob = self._should_stop(
#                 last_hidden = hidden_states,             # (B,S,H)
#                 attn_stack  = outputs.attentions,        # list[L] of (B,H,k,k)  (already reduced)
#                 token_probs = token_probs_so_far if token_probs_so_far is not None else
#                             torch.ones_like(mask, dtype=hidden_states.dtype),
#                 mask        = mask,                      # (B,S)
#                 input_ids   = input_ids,
#             )                                            # (B,1)

#             # BCE against sequence label (supports -1 skipping inside compute_stop_loss)
#             loss = self.compute_stop_loss(stop_prob, correctness_labels)

#             return CausalLMOutputWithPast(
#                 loss=loss,
#                 logits=logits,
#                 past_key_values=outputs.past_key_values,
#                 hidden_states=outputs.hidden_states,
#                 attentions=outputs.attentions,
#                 stop_prob=stop_prob.detach(),
#             )

#         # ===============================
#         # INFERENCE
#         # ===============================
#         stop_prob = None
#         if apply_budget:
#             if token_probs_so_far is None:
#                 raise ValueError("`token_probs_so_far` must be provided when `apply_budget=True` in inference.")

#             B, S, _ = hidden_states.shape
#             tok_p = token_probs_so_far
#             if tok_p.size(1) < S:
#                 pad = tok_p.new_full((B, S - tok_p.size(1)), 1.0)
#                 tok_p = torch.cat([tok_p, pad], dim=1)
#             elif tok_p.size(1) > S:
#                 tok_p = tok_p[:, :S]

#             mask = attention_mask[:, :S].float() if attention_mask is not None else \
#                 torch.ones(B, S, device=hidden_states.device, dtype=torch.float)

#             # Use full-sequence features & already-reduced attentions
#             stop_prob = self._should_stop(
#                 last_hidden = hidden_states,             # (B,S,H)
#                 attn_stack  = outputs.attentions,        # list[L] of (B,H,k,k) (already reduced)
#                 token_probs = tok_p,                     # (B,S)
#                 mask        = mask,                      # (B,S)
#                 input_ids   = input_ids[:, :S] if input_ids is not None else None,
#             )                                            # (B,1)
#             stop_prob = stop_prob.clamp_(min=1e-6, max=1-1e-6)

#         return CausalLMOutputWithPast(
#             logits=logits,
#             past_key_values=outputs.past_key_values,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#             stop_prob=stop_prob,   # (B,1) or None
#         )


#     # # old stop head forward
#     # @can_return_tuple
#     # @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
#     # @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     # @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
#     # def forward(
#     #     self,
#     #     input_ids: Optional[torch.LongTensor] = None,
#     #     attention_mask: Optional[torch.Tensor] = None,
#     #     position_ids: Optional[torch.LongTensor] = None,
#     #     past_key_values: Optional[Cache] = None,
#     #     inputs_embeds: Optional[torch.FloatTensor] = None,
#     #     labels: Optional[torch.LongTensor] = None,
#     #     use_cache: Optional[bool] = None,
#     #     output_attentions: Optional[bool] = None,
#     #     output_hidden_states: Optional[bool] = None,
#     #     cache_position: Optional[torch.LongTensor] = None,
#     #     logits_to_keep: Union[int, torch.Tensor] = 0,
#     #     # ---- NEW kwargs for stop-head ----
#     #     #  â†“â†“â†“ extra kwargs used at generation
#     #     token_probs_so_far: Optional[torch.Tensor] = None,   # (B,S) float
#     #     apply_budget: Optional[bool] = None,
#     #     **kwargs: Unpack[KwargsForCausalLM],
#     # ) -> CausalLMOutputWithPast:
#     #     r"""
#     #         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
#     #             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
#     #             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
#     #             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

#     #         logits_to_keep (`int` or `torch.Tensor`, *optional*):
#     #             If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
#     #             `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
#     #             token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
#     #             If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
#     #             This is useful when using packed tensor format (single dimension for batch and sequence length).

#     #     Returns:

#     #     Example:

#     #     ```python
#     #     >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

#     #     >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
#     #     >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

#     #     >>> prompt = "Hey, are you conscious? Can you talk to me?"
#     #     >>> inputs = tokenizer(prompt, return_tensors="pt")

#     #     >>> # Generate
#     #     >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
#     #     >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
#     #     "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
#     #     ```"""
#     #     # assert input_ids.shape[0] == 1, "current implementation only supports batch size = 1"
#     #     output_attentions = (
#     #         True if (self.training or apply_budget)
#     #         else (self.config.output_attentions if output_attentions is None else output_attentions)
#     #     )
#     #     output_hidden_states = False  # keep as True; you already set it this way


#     #     with torch.no_grad():
#     #         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
#     #         outputs: BaseModelOutputWithPast = self.model(
#     #             input_ids=input_ids,
#     #             attention_mask=attention_mask,
#     #             position_ids=position_ids,
#     #             past_key_values=past_key_values,
#     #             inputs_embeds=inputs_embeds,
#     #             use_cache=use_cache,
#     #             output_attentions=output_attentions,
#     #             output_hidden_states=False,
#     #             cache_position=cache_position,
#     #             **kwargs,
#     #         )


#     #         hidden_states = outputs.last_hidden_state          # (B,S,H)
#     #         logits        = self.lm_head(hidden_states)        # (B,S,V)

#     #         # ------------------------------------------------------------------
#     #         # build token_probs_so_far with numerically-stable scaling
#     #         # ------------------------------------------------------------------
#     #         if self.training and token_probs_so_far is None and labels is not None:
#     #             with torch.no_grad():
#     #                 # Align logits with labels
#     #                 logits_step = logits[:, :-1, :] if logits.size(1) == labels.size(1) + 1 else logits

#     #                 # log-softmax in full-precision â†’ avoids under-flow
#     #                 logp = torch.log_softmax(logits_step.float(), dim=-1)                # (B,S,V)

#     #                 # Gather log-prob of each gold token
#     #                 tgt   = labels.clamp(min=0)                                          # avoid -100 in gather
#     #                 log_tok_p = logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)           # (B,S)

#     #                 # Neutralise ignored positions (log(1)=0 keeps downstream math happy)
#     #                 log_tok_p = torch.where(labels.eq(-100), torch.zeros_like(log_tok_p), log_tok_p)

#     #                 # ----- optional: return to prob-space BUT clamp to Îµ so theyâ€™re never tiny -----
#     #                 eps = 1e-8
#     #                 tok_p = torch.clamp(log_tok_p.exp(), min=eps).to(logits_step.dtype)  # (B,S)

#     #             token_probs_so_far = tok_p.detach()        # keep gradients out


#     #     # --- Inside your Qwen3ForCausalLM.forward() method ---
#     #     if self.training:
#     #         assert labels is not None, "`labels` required for SFT of stop head"
#     #         assert token_probs_so_far is not None, "`token_probs_so_far` required"

#     #         # # 1) Ground-truth stop indicator per position (1 where token == </think>)
#     #         # print(labels)
#     #         target = (labels == self.stop_token_id).float()           # (B, S)
            
#     #         mask   = attention_mask.float()                           # (B, S)
#     #         eps    = 1e-6

            
#     #         stop_prob = self._should_stop(
#     #                             last_hidden = hidden_states,                     # (B,S,H)
#     #                             attn_stack  = outputs.attentions,                # list[L] (B,H,S,S)
#     #                             token_probs = token_probs_so_far,                # (B,S)
#     #                             mask        = mask,
#     #                             input_ids   = input_ids,
#     #                          ).squeeze(-1)                                        # (B,S)

#     #         # Clamp for stability before passing to the loss function
#     #         p_clamped = stop_prob.clamp(min=1e-6, max=1 - 1e-6)

#     #         # --- Call your new, stable loss function ---
#     #         loss = self.compute_stop_loss(p_clamped, target, mask)

#     #         return CausalLMOutputWithPast(
#     #             loss=loss,
#     #             logits=logits,
#     #             past_key_values=outputs.past_key_values,
#     #             hidden_states=outputs.hidden_states,
#     #             attentions=outputs.attentions,
#     #             stop_prob=stop_prob.detach(),
#     #         )

#     #     else:
#     #         stop_prob = None
#     #         if apply_budget:
#     #             # Require token_probs_so_far when budgeting
#     #             if token_probs_so_far is None:
#     #                 raise ValueError("`token_probs_so_far` must be provided when `apply_budget=True` in inference.")

#     #             B, S, H = hidden_states.shape

#     #             # --- Align shapes: token probs & mask up to S ---
#     #             tok_p = token_probs_so_far
#     #             if tok_p.size(1) < S:
#     #                 # pad with neutral 1.0 for missing positions (doesn't change multiplicative stats)
#     #                 pad = tok_p.new_full((B, S - tok_p.size(1)), 1.0)
#     #                 tok_p = torch.cat([tok_p, pad], dim=1)
#     #             elif tok_p.size(1) > S:
#     #                 tok_p = tok_p[:, :S]

#     #             mask = attention_mask[:, :S].float() if attention_mask is not None else \
#     #                 torch.ones(B, S, device=hidden_states.device, dtype=torch.float)

#     #             # If attentions weren't requested (shouldn't happen when apply_budget=True), skip compute
#     #             if outputs.attentions is not None:
#     #                 # With KV cache, q_len is already 1; still clip keys to :S
#     #                 attn_stack = [a[:, :, -1:, :S] for a in outputs.attentions]  # list[L] of (B, H, 1, S)

#     #                 # Compute stop prob for the **last** token only
#     #                 stop_prob = self._should_stop(
#     #                     last_hidden = hidden_states[:, -1:],         # (B,1,H)
#     #                     attn_stack  = attn_stack,                    # list[L] (B,H,1,S)
#     #                     token_probs = tok_p,                         # (B,S)
#     #                     mask        = mask,                          # (B,S)
#     #                     input_ids   = input_ids[:, :S] if input_ids is not None else None,
#     #                 )                                                # expect (B,1,1)

#     #                 # Numerical safety
#     #                 stop_prob = stop_prob.clamp_(min=1e-6, max=1-1e-6)

#     #     return CausalLMOutputWithPast(
#     #         logits=logits,
#     #         past_key_values=outputs.past_key_values,
#     #         hidden_states=outputs.hidden_states,
#     #         attentions=outputs.attentions,
#     #         stop_prob=stop_prob,   # (B,1,1) or None
#     #         )

# @add_start_docstrings(
#     """
#     The Qwen3 Model transformer with a sequence classification head on top (linear layer).

#     [`Qwen3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models
#     (e.g. GPT-2) do.

#     Since it does classification on the last token, it requires to know the position of the last token. If a
#     `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
#     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
#     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
#     each row of the batch).
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForSequenceClassification(Qwen3PreTrainedModel):
#     def __init__(self, config):
#         super().__init__(config)
#         self.num_labels = config.num_labels
#         self.model = Qwen3Model(config)
#         self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#     ) -> SequenceClassifierOutputWithPast:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
#             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
#             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
#         """

#         transformer_outputs: BaseModelOutputWithPast = self.model(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             use_cache=use_cache,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )
#         hidden_states = transformer_outputs.last_hidden_state
#         logits = self.score(hidden_states)

#         if input_ids is not None:
#             batch_size = input_ids.shape[0]
#         else:
#             batch_size = inputs_embeds.shape[0]

#         if self.config.pad_token_id is None and batch_size != 1:
#             raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
#         if self.config.pad_token_id is None:
#             last_non_pad_token = -1
#         elif input_ids is not None:
#             # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
#             non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
#             token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
#             last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
#         else:
#             last_non_pad_token = -1
#             logger.warning_once(
#                 f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
#                 "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
#             )

#         pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

#         loss = None
#         if labels is not None:
#             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

#         return SequenceClassifierOutputWithPast(
#             loss=loss,
#             logits=pooled_logits,
#             past_key_values=transformer_outputs.past_key_values,
#             hidden_states=transformer_outputs.hidden_states,
#             attentions=transformer_outputs.attentions,
#         )


# @add_start_docstrings(
#     """
#     The Qwen3 Model transformer with a token classification head on top (a linear layer on top of the hidden-states
#     output) e.g. for Named-Entity-Recognition (NER) tasks.
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForTokenClassification(Qwen3PreTrainedModel):
#     def __init__(self, config):
#         super().__init__(config)
#         self.num_labels = config.num_labels
#         self.model = Qwen3Model(config)
#         if getattr(config, "classifier_dropout", None) is not None:
#             classifier_dropout = config.classifier_dropout
#         elif getattr(config, "hidden_dropout", None) is not None:
#             classifier_dropout = config.hidden_dropout
#         else:
#             classifier_dropout = 0.1
#         self.dropout = nn.Dropout(classifier_dropout)
#         self.score = nn.Linear(config.hidden_size, config.num_labels)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.model.embed_tokens

#     def set_input_embeddings(self, value):
#         self.model.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     @add_code_sample_docstrings(
#         checkpoint=_CHECKPOINT_FOR_DOC,
#         output_type=TokenClassifierOutput,
#         config_class=_CONFIG_FOR_DOC,
#     )
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         labels: Optional[torch.LongTensor] = None,
#         use_cache: Optional[bool] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#     ) -> TokenClassifierOutput:
#         r"""
#         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
#             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
#             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
#         """

#         outputs: BaseModelOutputWithPast = self.model(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             use_cache=use_cache,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )
#         sequence_output = outputs.last_hidden_state
#         sequence_output = self.dropout(sequence_output)
#         logits = self.score(sequence_output)

#         loss = None
#         if labels is not None:
#             loss = self.loss_function(logits, labels, self.config)

#         return TokenClassifierOutput(
#             loss=loss,
#             logits=logits,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#         )


# @add_start_docstrings(
#     """
# The Qwen3 Model transformer with a span classification head on top for extractive question-answering tasks like
# SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).
#     """,
#     QWEN3_START_DOCSTRING,
# )
# class Qwen3ForQuestionAnswering(Qwen3PreTrainedModel):
#     base_model_prefix = "transformer"

#     def __init__(self, config):
#         super().__init__(config)
#         self.transformer = Qwen3Model(config)
#         self.qa_outputs = nn.Linear(config.hidden_size, 2)

#         # Initialize weights and apply final processing
#         self.post_init()

#     def get_input_embeddings(self):
#         return self.transformer.embed_tokens

#     def set_input_embeddings(self, value):
#         self.transformer.embed_tokens = value

#     @can_return_tuple
#     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
#     def forward(
#         self,
#         input_ids: Optional[torch.LongTensor] = None,
#         attention_mask: Optional[torch.FloatTensor] = None,
#         position_ids: Optional[torch.LongTensor] = None,
#         past_key_values: Optional[Cache] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         start_positions: Optional[torch.LongTensor] = None,
#         end_positions: Optional[torch.LongTensor] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         **kwargs,
#     ) -> QuestionAnsweringModelOutput:
#         r"""
#         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for position (index) of the start of the labelled span for computing the token classification loss.
#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
#             are not taken into account for computing the loss.
#         end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
#             Labels for position (index) of the end of the labelled span for computing the token classification loss.
#             Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
#             are not taken into account for computing the loss.
#         """

#         outputs: BaseModelOutputWithPast = self.transformer(
#             input_ids,
#             attention_mask=attention_mask,
#             position_ids=position_ids,
#             past_key_values=past_key_values,
#             inputs_embeds=inputs_embeds,
#             output_attentions=output_attentions,
#             output_hidden_states=output_hidden_states,
#         )

#         sequence_output = outputs.last_hidden_state

#         logits = self.qa_outputs(sequence_output)
#         start_logits, end_logits = logits.split(1, dim=-1)
#         start_logits = start_logits.squeeze(-1).contiguous()
#         end_logits = end_logits.squeeze(-1).contiguous()

#         loss = None
#         if start_positions is not None and end_positions is not None:
#             loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)

#         return QuestionAnsweringModelOutput(
#             loss=loss,
#             start_logits=start_logits,
#             end_logits=end_logits,
#             hidden_states=outputs.hidden_states,
#             attentions=outputs.attentions,
#         )


# __all__ = [
#     "Qwen3ForCausalLM",
#     "Qwen3ForQuestionAnswering",
#     "Qwen3Model",
#     "Qwen3PreTrainedModel",
#     "Qwen3ForSequenceClassification",
#     "Qwen3ForTokenClassification",
# ]
