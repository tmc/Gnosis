#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from functools import partial
from typing import Callable, Optional, Tuple, Union

import torch
from torch import nn

from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache
from ...generation import GenerationMixin
from ...modeling_attn_mask_utils import AttentionMaskConverter
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_outputs import (
    BaseModelOutputWithPast,
    QuestionAnsweringModelOutput,
    SequenceClassifierOutputWithPast,
    TokenClassifierOutput,
)
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack

try:
    from ...utils import LossKwargs  # preferred path in newer releases
except Exception:
    # fallback for older trees where it's only under loss/loss_utils.py
    from ...loss.loss_utils import LossKwargs


from ...utils import (
    # LossKwargs,
    add_code_sample_docstrings,
    add_start_docstrings,
    add_start_docstrings_to_model_forward,
    can_return_tuple,
    logging,
    replace_return_docstrings,
)


from ...utils.deprecation import deprecate_kwarg
from .configuration_qwen3 import Qwen3Config
from ...utils import ModelOutput
from dataclasses import dataclass
from torch.distributions import Gamma
import numpy as np
from transformers import BertConfig, BertModel
import torch, torch.nn.functional as F
from typing import List



#myedit, Visualization************************************************************************************************
# stop_viz.py (no cropping, small figs, no ticks)
import os, uuid, math
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from numbers import Integral
from typing import Optional, List, Sequence, Tuple
from matplotlib import colors  # <- already added above

# ---------- robust limits ----------
def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> tuple[float, float]:
    """Percentile-based vmin/vmax with safe fallback so we never get a flat colormap."""
    v = np.asarray(M)
    v = v[np.isfinite(v)]
    if v.size == 0:
        return 0.0, 1.0
    vmin = float(np.percentile(v, lo))
    vmax = float(np.percentile(v, hi))
    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
        vmin = float(v.min()) if v.size else 0.0
        vmax = float(v.max()) if v.size else 1.0
        if vmin == vmax:
            vmax = vmin + 1e-6
    return vmin, vmax

# ---------- small, enhanced token map (keep only ONE definition of this) ----------
def _plot_token_single_full(M, think_positions, path):
    S = M.shape[0]
    vmin, vmax = _robust_limits(M)
    norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)  # boost mid/low contrast

    plt.figure(figsize=(4, 4))  # small, fixed
    plt.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
    for t in (think_positions or []):
        if 0 <= t < S:
            plt.axvline(t, ls="--", lw=0.8, color="w")
            plt.axhline(t, ls="--", lw=0.8, color="w")
            plt.scatter([t], [t], s=10, c="red", marker="o")
    plt.xticks([]); plt.yticks([])  # no ticks to keep it light
    plt.title("Token attention (full, enhanced)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

@torch.no_grad()
def _unify_attentions(attn_stack):
    """
    attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
    Returns A_all: (B,L,S,S) after head-avg and square-izing.
    """
    if attn_stack is None or len(attn_stack) == 0:
        return None
    layers = []
    for A in attn_stack:
        A = A.float()  # (B,H,q,S)
        if A.dim() != 4:
            continue
        B, H, q, S = A.shape
        A = A.mean(dim=1)  # (B,q,S)
        if q == 1:
            sq = A.new_zeros(B, S, S)
            sq[:, -1, :] = A[:, 0, :]
            A_sq = sq
        elif q == S:
            A_sq = A
        else:
            if q < S:
                A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
            A_sq = A[:, :S, :S]
        layers.append(A_sq)  # (B,S,S)
    if not layers:
        return None
    return torch.stack(layers, dim=1)  # (B,L,S,S)

@torch.no_grad()
def _token_to_sentence_attention(A_tok: torch.Tensor, input_ids: torch.Tensor, sep_id: int):
    """
    A_tok: (B,L,S,S). Returns (B,L,Ns,Ns), seg, Ns.
    """
    B, L, S, _ = A_tok.shape
    seg = torch.cumsum((input_ids == sep_id).long(), dim=1)  # (B,S)
    Ns = int(seg.max().item()) + 1
    H = F.one_hot(seg, Ns).to(dtype=A_tok.dtype)             # (B,S,Ns)
    A_sum = torch.einsum("bqi, blqt, btj -> blij", H, A_tok, H)
    counts = H.sum(dim=1).clamp(min=1.0)                     # (B,Ns)
    denom = counts.unsqueeze(1).unsqueeze(3) * counts.unsqueeze(1).unsqueeze(2)
    return (A_sum / denom), seg, Ns

def _grid_dims(n: int) -> Tuple[int, int]:
    cols = int(np.ceil(np.sqrt(n)))
    rows = int(np.ceil(n / cols))
    return rows, cols

# ---------- outlier smoothing (confidence) ----------
@torch.no_grad()
def interp_outliers_inplace_token_probs(token_probs: torch.Tensor,
                                        mask: torch.Tensor,
                                        low: float = 1.0, high: float = 99.0) -> None:
    """
    In-place linear interpolation of outliers in token_probs per batch item.
    Outliers = values outside [low, high] percentiles over valid (mask>0) positions.
    """
    B, S = token_probs.shape
    for b in range(B):
        valid = (mask[b] > 0).cpu().numpy()
        if valid.sum() < 3:
            continue
        y = token_probs[b, valid].detach().float().cpu().numpy()
        x = np.arange(y.shape[0])

        lo, hi = np.percentile(y, [low, high])
        bad = (y < lo) | (y > hi)
        if not bad.any():
            continue

        x_good = x[~bad]
        y_good = y[~bad]
        if x_good.size >= 2:
            y[bad] = np.interp(x[bad], x_good, y_good)
        else:
            y = np.clip(y, lo, hi)

        token_probs[b, valid] = torch.as_tensor(y, device=token_probs.device, dtype=token_probs.dtype)

# ---------- marker finding (</think>) ----------
@torch.no_grad()
def _find_marker_positions(ids_row: torch.Tensor,
                           think_id: Optional[int] = None,
                           think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
    """
    Returns start indices where the marker occurs in input_ids.
    - If think_token_seq is provided, it searches for that subsequence.
    - Else falls back to single-token search using think_id.
    """
    ids = ids_row.tolist()
    positions: List[int] = []

    if think_token_seq and len(think_token_seq) > 0:
        pat = list(think_token_seq)
        m, n = len(pat), len(ids)
        if m <= n:
            for i in range(0, n - m + 1):
                if ids[i:i+m] == pat:
                    positions.append(i)
        return positions

    if think_id is not None:
        for i, tok in enumerate(ids):
            if tok == think_id:
                positions.append(i)
    return positions

# ---------- plotting helpers (small figs) ----------
def _plot_conf_full(probs, mask, think_positions, path):
    K = probs.shape[0]
    x = np.arange(K)
    plt.figure(figsize=(30, 5))
    plt.plot(x[mask > 0], probs[mask > 0], linewidth=1.0)
    for pos in think_positions:
        tmk = pos - 1  # probs[t] predicts token t+1
        if 0 <= tmk < K:
            plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
    plt.xticks([]); plt.yticks([])
    plt.title("Confidence (full)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

# ---------- NEW: uncertainty plots ----------
def _plot_uncertainty_full(unc, mask, think_positions, path, title="Uncertainty (full)"):
    """
    unc: numpy array shape (K,) with per-token uncertainty (e.g., -log p_gold or entropy).
    mask: numpy array shape (K,) in {0,1}
    """
    K = unc.shape[0]
    x = np.arange(K)
    plt.figure(figsize=(30, 5))
    plt.plot(x[mask > 0], unc[mask > 0], linewidth=1.0)
    for pos in think_positions:
        tmk = pos - 1  # aligns with next-token prediction index
        if 0 <= tmk < K:
            plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
    plt.xticks([]); plt.yticks([])
    plt.title(title, fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _plot_token_grid_full(A_layers, think_positions, path, max_layers=16):
    """
    Show exactly the last up-to-3 layers (3Ã—1). Each subplot uses robust normalization.
    """
    L, S, _ = A_layers.shape
    if L == 0:
        return

    idxs = list(range(max(0, L - 3), L))  # [L-3, L-2, L-1]
    panels = len(idxs)

    fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
    if panels == 1:
        axes = [axes]

    for ax, li in zip(axes, idxs):
        M = A_layers[li]
        vmin, vmax = _robust_limits(M)
        norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)
        ax.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
        for t in (think_positions or []):
            if 0 <= t < S:
                ax.axvline(t, ls="--", lw=0.8, color="w")
                ax.axhline(t, ls="--", lw=0.8, color="w")
                ax.scatter([t], [t], s=8, c="red", marker="o")
        ax.set_title(f"L{li}", fontsize=8)
        # keep ticks default off to stay light

    fig.tight_layout()
    fig.savefig(path, dpi=180)
    plt.close(fig)

def _plot_sentence_single(Ms, s_star, path):
    plt.figure(figsize=(4, 4))
    plt.imshow(Ms, aspect="auto", interpolation="nearest")
    if s_star is not None and 0 <= s_star < Ms.shape[0]:
        plt.axvline(s_star, ls="--", lw=0.8, color="w")
        plt.axhline(s_star, ls="--", lw=0.8, color="w")
    plt.xticks([]); plt.yticks([])
    plt.title("Sentence attention (full)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _plot_sentence_grid(M_layers, s_star, path, max_layers=16):
    L = M_layers.shape[0]
    L_show = min(L, max_layers)
    rows, cols = _grid_dims(L_show)
    vmax = float(M_layers[:L_show].max()) if L_show > 0 else 1.0
    vmax = max(vmax, 1e-12)

    fig, axes = plt.subplots(rows, cols, figsize=(2.0*cols, 2.0*rows))
    axes = np.atleast_2d(axes)
    idx = 0
    for r in range(rows):
        for c in range(cols):
            ax = axes[r, c]
            if idx < L_show:
                Ms = M_layers[idx]
                ax.imshow(Ms, aspect="auto", interpolation="nearest", vmin=0.0, vmax=vmax)
                if s_star is not None and 0 <= s_star < Ms.shape[0]:
                    ax.axvline(s_star, ls="--", lw=0.8, color="w")
                    ax.axhline(s_star, ls="--", lw=0.8, color="w")
                ax.set_title(f"L{idx}", fontsize=8)
                ax.set_xticks([]); ax.set_yticks([])
            else:
                ax.axis("off")
            idx += 1
    fig.tight_layout()
    fig.savefig(path, dpi=180)
    plt.close(fig)

# ---------- main (NO cropping) ----------
@torch.no_grad()
def plot_stop_viz(
    input_ids: torch.Tensor,          # (B,S)
    token_probs: torch.Tensor,        # (B,S)
    mask: torch.Tensor,               # (B,S)
    attn_stack,                       # list[L] of (B,H,S,S) or (B,H,1,S)
    save_root: str,
    sep_id: int = 198,
    think_id: Optional[int] = 151668,
    think_token_seq: Optional[Sequence[int]] = None,  # pass if </think> is multi-token
    batch_index: int = 0,
    max_layers_grid: int = 16,
    tag: Optional[str] = None,
    per_token_uncertainty: Optional[torch.Tensor] = None,  # <-- NEW (optional entropy/SCE)
    plot_uncertainty_delta: bool = False,                  # <-- NEW
):
    """
    Saves (FULL views, small figs, no ticks):
      - confidence/stop_conf_{tag}.png
      - uncertainty/stop_unc_{tag}.png                (NEW; surprisal or provided entropy)
      - uncertainty/stop_unc_delta_{tag}.png         (NEW; Î” uncertainty, optional)
      - token_attn/single/stop_tok_attn_{tag}.png
      - token_attn/grid/stop_tok_attn_grid_{tag}.png
      - sentence_attn/single/stop_sent_attn_{tag}.png
      - sentence_attn/grid/stop_sent_attn_grid_{tag}.png
    """
    if not isinstance(sep_id, Integral):
        sep_id = 198

    _ensure_dir(save_root)
    d_conf        = os.path.join(save_root, "confidence")
    d_unc         = os.path.join(save_root, "uncertainty")  # NEW
    d_tok_single  = os.path.join(save_root, "token_attn", "single")
    d_tok_grid    = os.path.join(save_root, "token_attn", "grid")
    d_sent_single = os.path.join(save_root, "sentence_attn", "single")
    d_sent_grid   = os.path.join(save_root, "sentence_attn", "grid")
    for d in [d_conf, d_unc, d_tok_single, d_tok_grid, d_sent_single, d_sent_grid]:
        _ensure_dir(d)

    tag = tag or uuid.uuid4().hex[:8]
    b = batch_index
    S = int(input_ids.size(1))
    K = S  # FULL length; discard any kept-token logic

    # find </think> occurrences
    think_positions: List[int] = _find_marker_positions(
        input_ids[b], think_id=think_id, think_token_seq=think_token_seq
    )

    # confidence (full) â€” smooth outliers then plot
    interp_outliers_inplace_token_probs(token_probs, mask)
    probs_np = token_probs[b, :K].detach().float().cpu().numpy()
    mask_np  = mask[b, :K].detach().float().cpu().numpy()
    _plot_conf_full(
        probs=probs_np,
        mask=mask_np,
        think_positions=think_positions,
        path=os.path.join(d_conf, f"stop_conf_{tag}.png"),
    )

    # ---------- NEW: uncertainty (full sequence) ----------
    # If an uncertainty tensor is provided (e.g., entropy), plot that.
    # Otherwise, use surprisal: u = -log p_gold.
    if per_token_uncertainty is not None:
        U = per_token_uncertainty[b, :K].detach().float().cpu().numpy()
        unc_title = "Uncertainty (provided)"
    else:
        eps = 1e-8
        U = np.log(np.clip(probs_np, eps, 1.0))
        unc_title = "Uncertainty = -log p_gold"

    _plot_uncertainty_full(
        unc=U,
        mask=mask_np,
        think_positions=think_positions,
        path=os.path.join(d_unc, f"stop_unc_{tag}.png"),
        title=unc_title
    )

    if plot_uncertainty_delta:
        # simple first difference to visualize "change" in uncertainty
        dU = np.diff(U, prepend=U[:1])
        _plot_uncertainty_full(
            unc=dU,
            mask=mask_np,
            think_positions=think_positions,
            path=os.path.join(d_unc, f"stop_unc_delta_{tag}.png"),
            title="Î” Uncertainty (first difference)"
        )

    # token attention (full)
    A_all = _unify_attentions(attn_stack)  # (B,L,S,S) or None
    if A_all is not None and A_all.size(1) > 0:
        A_all_b = torch.nan_to_num(A_all[b].float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0)
        A_layers_full = A_all_b.cpu().numpy()     # (L,S,S)
        A_last_full   = A_layers_full[-1]         # (S,S)

        _plot_token_single_full(
            M=A_last_full,
            think_positions=think_positions,
            path=os.path.join(d_tok_single, f"stop_tok_attn_{tag}.png"),
        )

        _plot_token_grid_full(
            A_layers=A_layers_full,
            think_positions=think_positions,
            path=os.path.join(d_tok_grid, f"stop_tok_attn_grid_{tag}.png"),
            max_layers=max_layers_grid,
        )

        # sentence attention (full)
        A_sent, seg, Ns = _token_to_sentence_attention(A_all[b:b+1], input_ids[b:b+1], sep_id)
        A_sent_np = A_sent[0].float().clamp_min(0).cpu().numpy()

        s_mark = None
        if think_positions:
            last_pos = think_positions[-1]
            s_mark = int(seg[0, last_pos].item())

        _plot_sentence_single(
            Ms=A_sent_np[-1],
            s_star=s_mark,
            path=os.path.join(d_sent_single, f"stop_sent_attn_{tag}.png"),
        )
        _plot_sentence_grid(
            M_layers=A_sent_np,
            s_star=s_mark,
            path=os.path.join(d_sent_grid, f"stop_sent_attn_grid_{tag}.png"),
            max_layers=max_layers_grid,
        )

#**********************************************************************************************************************
#Plot attn maps
import os, uuid
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import colors
from typing import Optional, Sequence, List, Tuple


# =========================
# utils
# =========================
def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def _unique_path(path: str) -> str:
    """Return path if free; otherwise append _001, _002, ... before extension."""
    if not os.path.exists(path):
        return path
    base, ext = os.path.splitext(path)
    i = 1
    while True:
        cand = f"{base}_{i:03d}{ext}"
        if not os.path.exists(cand):
            return cand
        i += 1

@torch.no_grad()
def _unify_attentions(attn_stack):
    """
    attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
    â†’ returns (B,L,S,S) with heads averaged and 'q' expanded to S (last query row if q=1).
    """
    if attn_stack is None or len(attn_stack) == 0:
        return None
    layers = []
    for A in attn_stack:
        if A is None or A.dim() != 4:
            continue
        A = A.float()                   # (B,H,q,S)
        B, H, q, S = A.shape
        A = A.mean(dim=1)               # (B,q,S)
        if q == 1:
            sq = A.new_zeros(B, S, S)
            sq[:, -1, :] = A[:, 0, :]
            A_sq = sq
        elif q == S:
            A_sq = A
        else:
            if q < S:
                A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
            A_sq = A[:, :S, :S]
        layers.append(A_sq)              # (B,S,S)
    if not layers:
        return None
    return torch.stack(layers, dim=1)   # (B,L,S,S)

def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> Tuple[float, float]:
    v = np.asarray(M)
    v = v[np.isfinite(v)]
    if v.size == 0:
        return 0.0, 1.0
    vmin = float(np.percentile(v, lo))
    vmax = float(np.percentile(v, hi))
    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
        vmin = float(v.min()) if v.size else 0.0
        vmax = float(v.max()) if v.size else 1.0
    if vmin == vmax:
        vmax = vmin + 1e-6
    return vmin, vmax

@torch.no_grad()
def _find_marker_positions(ids_row: torch.Tensor,
                           think_id: Optional[int] = None,
                           think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
    """
    Return start positions where the marker occurs in input_ids row.
    - If think_token_seq is provided, match that subsequence.
    - Else use single-token think_id.
    """
    ids = ids_row.tolist()
    pos: List[int] = []
    if think_token_seq and len(think_token_seq) > 0:
        pat = list(think_token_seq)
        m, n = len(pat), len(ids)
        for i in range(0, n - m + 1):
            if ids[i:i+m] == pat:
                pos.append(i)
        return pos
    if think_id is not None:
        for i, t in enumerate(ids):
            if t == think_id:
                pos.append(i)
    return pos

def _draw_think_marks(ax, positions: Sequence[int], S: int):
    for t in positions or []:
        if 0 <= t < S:
            ax.axvline(t, ls="--", lw=0.8, color="w")
            ax.axhline(t, ls="--", lw=0.8, color="w")
            ax.scatter([t], [t], s=8, c="red", marker="o")

def _project_marks_to_size(positions: Sequence[int], S_src: int, S_tgt: int) -> List[int]:
    if S_src <= 0 or S_tgt <= 0:
        return []
    out = []
    for p in positions or []:
        q = int(np.floor(p * S_tgt / max(1, S_src)))
        if 0 <= q < S_tgt:
            out.append(q)
    return sorted(set(out))

def _save_grid(fig, path, dpi=180) -> str:
    fig.tight_layout()
    path = _unique_path(path)
    fig.savefig(path, dpi=dpi)
    plt.close(fig)
    print(f"[attn_reductions] saved: {path}")
    return path

def _save_line(fig, path, dpi=200) -> str:
    fig.tight_layout()
    path = _unique_path(path)
    fig.savefig(path, dpi=dpi)
    plt.close(fig)
    print(f"[attn_reductions] saved: {path}")
    return path


# =========================
# reductions (CPU-safe)
# =========================
def _strided_pool_maps(A_last: torch.Tensor, sizes=(8,16,32)) -> List[torch.Tensor]:
    # A_last: (B,S,S) on CPU
    assert A_last.device.type == "cpu"
    X = A_last.unsqueeze(1)  # (B,1,S,S)
    out = []
    B, S, _ = A_last.shape
    for k in sizes:
        stride = max(1, S // k)
        out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
    return out

def _gauss_blur_pool_maps(A_last: torch.Tensor, sizes=(8,16,32), sigma=1.0) -> List[torch.Tensor]:
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    X = A_last.unsqueeze(1)  # (B,1,S,S)
    rad = int(3*sigma + 0.5)
    t = torch.arange(-rad, rad+1, dtype=A_last.dtype)
    g = torch.exp(-(t**2)/(2*sigma**2)); g = g / g.sum()
    k2 = (g[:,None] @ g[None,:]).unsqueeze(0).unsqueeze(0)  # (1,1,K,K)
    X = F.conv2d(X, k2, padding=rad)
    out = []
    for k in sizes:
        stride = max(1, S // k)
        out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
    return out

def _distance_binned_profile(A_last: torch.Tensor, bins=64) -> torch.Tensor:
    # (B,S,S) â†’ (B,bins), mean attention per |i-j| bin (CPU)
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    dists = torch.arange(S, dtype=torch.long)
    D = (dists[None,:] - dists[:,None]).abs().unsqueeze(0)  # (1,S,S)
    A_use = A_last
    edges = torch.linspace(0, S-1, bins+1)
    idx = torch.bucketize(D, edges) - 1                     # (1,S,S) in [0,bins-1]
    profiles = []
    for b in range(bins):
        m = (idx == b)
        num = (A_use * m).sum(dim=(1,2))
        den = m.sum(dim=(1,2)).clamp_min(1)
        profiles.append(num / den)
    return torch.stack(profiles, dim=-1)  # (B,bins)

def _topk_sparse_pool(A_last: torch.Tensor, ratios=(0.01, 0.05, 0.10), size=32) -> List[torch.Tensor]:
    # CPU top-k per row â†’ sparse â†’ pooled
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    for r in ratios:
        k = max(1, int(S * r))
        vals, idx = torch.topk(A_last, k, dim=-1)           # (B,S,k) long indices
        M = torch.zeros_like(A_last)
        M.scatter_(-1, idx.long(), 1.0)                     # 1 at top-k keys
        Sparse = A_last * M
        stride = max(1, S // size)
        out.append(F.avg_pool2d(Sparse.unsqueeze(1), stride, stride, ceil_mode=True).squeeze(1))
    return out

def _kmeans1d_assign(S: int, K: int, iters=8) -> torch.Tensor:
    # CPU 1D k-means on positions
    x = torch.linspace(0, 1, S, dtype=torch.float32).unsqueeze(-1)   # (S,1)
    centers = torch.linspace(0, 1, K, dtype=torch.float32).unsqueeze(-1)
    for _ in range(iters):
        d = (x - centers.T)**2                                       # (S,K)
        assign = d.argmin(dim=1)                                     # (S,) long
        for k in range(K):
            pts = x[assign == k]
            centers[k] = pts.mean(dim=0) if pts.numel() else centers[k]
    return assign.long()

def _cluster_pool(A_last: torch.Tensor, Ks=(16,32,64)) -> List[torch.Tensor]:
    # CPU positional clusters â†’ clusterÃ—cluster map
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    for K in Ks:
        K = min(K, S)
        assign = _kmeans1d_assign(S, K)                   # (S,) long
        H = F.one_hot(assign, K).to(A_last.dtype)         # (S,K)
        Ht = H.t().unsqueeze(0).expand(B, -1, -1)         # (B,K,S)
        Hb = H.unsqueeze(0).expand(B, -1, -1)             # (B,S,K)
        Cluster = torch.bmm(Ht, torch.bmm(A_last, Hb))    # (B,K,K)
        cnt = H.sum(dim=0).clamp_min(1.0)                 # (K,)
        Cluster = Cluster / (cnt.unsqueeze(0).unsqueeze(-1) * cnt.unsqueeze(0).unsqueeze(1))
        out.append(Cluster)
    return out

def _landmark_pool(A_last: torch.Tensor, Ms=(16,32,64)) -> List[torch.Tensor]:
    # CPU landmarks by out-degree
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    outdeg = A_last.sum(-1)                            # (B,S)
    for M in Ms:
        M = min(M, S)
        idx = torch.topk(outdeg, M, dim=-1).indices.long()   # (B,M)
        batch = torch.arange(B, dtype=torch.long)[:, None]   # (B,1)
        L = A_last[batch, idx][:, :, idx]                    # (B,M,M)
        sort_idx = idx.sort(dim=-1).indices
        L = L[batch, sort_idx][:, :, sort_idx]
        out.append(L)
    return out

def _pyramid_maps(A_last: torch.Tensor, scales=(8,16,32)) -> List[torch.Tensor]:
    # same as strided pool, just named as pyramid (multi-scale)
    return _strided_pool_maps(A_last, sizes=scales)


# =========================
# Internal/External (span-based) helpers
# =========================
def _find_internal_span(input_ids_row: torch.Tensor,
                        close_positions: List[int],
                        think_open_seq: Optional[Sequence[int]] = None,
                        internal_window: int = 128) -> Optional[Tuple[int, int]]:
    """
    Return (start, end) token indices for the internal region.
    - Prefer the last '<think>' ... '</think>' span if think_open_seq is provided.
    - Else fallback to window: [max(0, t_close - internal_window), t_close)
    Excludes the close token itself.
    """
    if not close_positions:
        return None
    t_close = close_positions[-1]  # last close
    if think_open_seq and len(think_open_seq) > 0:
        ids = input_ids_row.tolist()
        pat = list(think_open_seq)
        last_open = -1
        for i in range(0, t_close - len(pat) + 1):
            if ids[i:i+len(pat)] == pat:
                last_open = i
        if last_open >= 0 and last_open < t_close:
            return (last_open, t_close)
    s = max(0, t_close - int(internal_window))
    return (s, t_close)

def _int_ext_fractions(A: torch.Tensor, start: int, end: int) -> Tuple[float, float, float, float]:
    """
    A: (S,S) CPU attention (nonnegative). Internal = [start, end) ; External = complement.
    Returns fractions (II, IE, EI, EE) that sum to 1.
    """
    S = A.size(0)
    I = torch.zeros(S, dtype=torch.float32)
    I[start:end] = 1.0
    E = 1.0 - I
    M_II = torch.ger(I, I)
    M_IE = torch.ger(I, E)
    M_EI = torch.ger(E, I)
    M_EE = torch.ger(E, E)
    tot = A.sum().item() + 1e-12
    II = float((A * M_II).sum().item() / tot)
    IE = float((A * M_IE).sum().item() / tot)
    EI = float((A * M_EI).sum().item() / tot)
    EE = float((A * M_EE).sum().item() / tot)
    return II, IE, EI, EE

def _plot_int_ext_layers(A_layers: torch.Tensor,  # (L,S,S) CPU
                         start: int, end: int,
                         path: str):
    L, S, _ = A_layers.shape
    II_list, IE_list, EI_list = [], [], []
    for li in range(L):
        II, IE, EI, EE = _int_ext_fractions(A_layers[li], start, end)
        II_list.append(II); IE_list.append(IE); EI_list.append(EI)
    x = np.arange(L)
    fig = plt.figure(figsize=(6.5, 2.0))
    plt.plot(x, II_list, label="Iâ†’I", linewidth=1.2)
    plt.plot(x, IE_list, label="Iâ†’E", linewidth=1.0)
    plt.plot(x, EI_list, label="Eâ†’I", linewidth=1.0)
    plt.legend(loc="upper right", fontsize=7, framealpha=0.25)
    plt.title("Internal vs External attention (per layer)", fontsize=9)
    _save_line(fig, path, dpi=200)

def _plot_int_ext_last_bar(A_last: torch.Tensor,  # (S,S) CPU
                           start: int, end: int,
                           path: str):
    II, IE, EI, EE = _int_ext_fractions(A_last, start, end)
    vals = [II, IE, EI, EE]
    labels = ["Iâ†’I", "Iâ†’E", "Eâ†’I", "Eâ†’E"]
    fig = plt.figure(figsize=(4.5, 2.2))
    xpos = np.arange(len(vals))
    plt.bar(xpos, vals, width=0.6)
    for i, v in enumerate(vals):
        plt.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
    plt.title("Internal/External fractions (last layer)", fontsize=9)
    _save_line(fig, path, dpi=200)


# =========================
# NEW: Grouped Internal/External helpers
# =========================
def _group_intra_external(A_layers: torch.Tensor, K: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    A_layers: (L,S,S) CPU, nonnegative
    K: number of groups
    Returns:
      intra[L]   : sum diag(H^T A H) / sum(A)
      external[L]: 1 - intra[L]
      last_B(K,K): group block attention for last layer, normalised by total mass
    """
    L, S, _ = A_layers.shape
    K = max(1, min(int(K), S))

    # fixed positional k-means assignment (deterministic, CPU)
    assign = _kmeans1d_assign(S, K)                   # (S,)
    H = F.one_hot(assign, K).to(A_layers.dtype)       # (S,K)
    Ht = H.t()                                        # (K,S)

    intra = np.zeros(L, dtype=np.float32)
    external = np.zeros(L, dtype=np.float32)
    last_B = None

    for li in range(L):
        A = A_layers[li]                              # (S,S)
        B = Ht @ (A @ H)                              # (K,K)
        tot = float(A.sum().item() + 1e-12)
        intra_li = float(torch.trace(B).item()) / tot
        intra[li] = intra_li
        external[li] = max(0.0, 1.0 - intra_li)
        if li == L - 1:
            last_B = (B / tot).cpu().numpy()

    return intra, external, last_B

def _plot_group_intra_external_lines(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
    """
    3Ã—1 panels: for each K, plot intra vs external vs layers.
    """
    fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
    if len(Ks) == 1:
        axes = [axes]
    for ax, K in zip(axes, Ks):
        intra, ext, _ = _group_intra_external(A_layers, K)
        x = np.arange(len(intra))
        ax.plot(x, intra, label="intra", linewidth=1.2)
        ax.plot(x, ext,   label="external", linewidth=1.0)
        ax.set_title(f"group K={K}", fontsize=9)
        ax.legend(loc="upper right", fontsize=7, framealpha=0.25)
    _save_grid(fig, path, dpi=200)

def _plot_group_intra_external_bars(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
    """
    3Ã—1 panels: for each K, bar of last layer intra vs external.
    """
    L, S, _ = A_layers.shape
    fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
    if len(Ks) == 1:
        axes = [axes]
    for ax, K in zip(axes, Ks):
        intra, ext, _ = _group_intra_external(A_layers, K)
        vals = [float(intra[-1]), float(ext[-1])]
        xpos = np.arange(2)
        ax.bar(xpos, vals, width=0.6)
        ax.set_title(f"last layer (K={K})", fontsize=9)
        for i, v in enumerate(vals):
            ax.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
    _save_grid(fig, path, dpi=200)


# =========================
# plotting helpers
# =========================
def _imshow_small(ax, M: np.ndarray, title: str, think_pos: Sequence[int]):
    vmin, vmax = _robust_limits(M)
    ax.imshow(M, interpolation="nearest", aspect="auto",
              norm=colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax))
    S = M.shape[0]
    _draw_think_marks(ax, think_pos, S)
    ax.set_title(title, fontsize=8)

def _imshow_small_project(ax, M: np.ndarray, title: str,
                          think_pos_tokens: Sequence[int], S_src_tokens: int):
    # Project token indices into this mapâ€™s axis length
    S_tgt = M.shape[0]
    marks = _project_marks_to_size(think_pos_tokens, S_src_tokens, S_tgt)
    _imshow_small(ax, M, title, marks)

def _plot_line_profile(vec: np.ndarray, path: str, title="Distance-binned attention"):
    fig = plt.figure(figsize=(5, 1.6))
    x = np.arange(vec.shape[-1])
    plt.plot(x, vec, linewidth=1.0)
    plt.title(title, fontsize=9)
    _save_line(fig, path, dpi=200)


# =========================
# main API
# =========================
@torch.no_grad()
def plot_all_token_attn_reductions(
    input_ids: torch.Tensor,     # (B,S)
    attn_stack,                  # list[L] of (B,H,S,S) or (B,H,1,S)
    save_root: str,
    think_id: Optional[int] = 151668,
    think_token_seq: Optional[Sequence[int]] = None,
    think_open_seq: Optional[Sequence[int]] = None,   # optional "<think>" opener
    internal_window: int = 128,                        # fallback window
    batch_index: int = 0,
    tag: Optional[str] = None,
):
    """
    Writes PNGs under {save_root}/attn_reductions/<method>/...  (small figs, 3Ã—1 where applicable).
    Shows </think> position(s) in all map-based plots (projected where needed). Unique filenames.
    Also plots Internal vs External (span-based) and Grouped Intra/External (Kâˆˆ{8,16,32}).
    """
    _ensure_dir(save_root)
    base = os.path.join(save_root, "attn_reductions")
    subdirs = {
        "token_full_single": os.path.join(base, "token_full", "single"),
        "token_full_last3":  os.path.join(base, "token_full", "last3"),
        "strided_pool":      os.path.join(base, "strided_pool"),
        "gauss_pool":        os.path.join(base, "gauss_pool"),
        "dist_binned":       os.path.join(base, "dist_binned"),
        "topk_pool":         os.path.join(base, "topk_pool"),
        "cluster_pool":      os.path.join(base, "cluster_pool"),
        "landmarks":         os.path.join(base, "landmarks"),
        "pyramid":           os.path.join(base, "pyramid"),
        "int_ext_ratio":     os.path.join(base, "int_ext_ratio"),
        "group_int_ext":     os.path.join(base, "group_int_ext"),
    }
    for d in subdirs.values():
        _ensure_dir(d)

    tag = tag or uuid.uuid4().hex[:8]
    b = batch_index

    # ---- unify attentions and select last layer ----
    A_all = _unify_attentions(attn_stack)  # (B,L,S,S)
    if A_all is None or A_all.size(1) == 0:
        print("[attn_reductions] no attention to plot")
        return

    # Move to CPU once; sanitize; keep shapes
    A_all_cpu = torch.nan_to_num(A_all.detach().float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0).cpu()
    A_last = A_all_cpu[b, -1].unsqueeze(0)  # (1,S,S) CPU
    B_, S, _ = A_last.shape

    # find all </think> positions (token indices)
    think_pos_tokens = _find_marker_positions(input_ids[b], think_id=think_id, think_token_seq=think_token_seq)

    print(f"[attn_reductions] S={S}, Layers={int(A_all_cpu.shape[1])}, think_positions={think_pos_tokens}")

    # === Normal token attention (comparison) ===
    try:
        print("[attn_reductions] plotting: token_full_single")
        M = A_last[0].numpy()
        fig, ax = plt.subplots(1, 1, figsize=(4, 4))
        _imshow_small(ax, M, "Token attention (last layer, full)", think_pos_tokens)
        _save_grid(fig, os.path.join(subdirs["token_full_single"], f"token_full_single_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:token_full_single] skipped: {e}")

    try:
        print("[attn_reductions] plotting: token_full_last3")
        L = int(A_all_cpu.shape[1])
        idxs = list(range(max(0, L - 3), L))
        panels = len(idxs)
        fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
        if panels == 1:
            axes = [axes]
        for ax, li in zip(axes, idxs):
            M = A_all_cpu[b, li].numpy()
            _imshow_small(ax, M, f"Layer {li}", think_pos_tokens)
        _save_grid(fig, os.path.join(subdirs["token_full_last3"], f"token_full_last3_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:token_full_last3] skipped: {e}")

    # === Reductions (3Ã—1 each where applicable) ===
    try:
        print("[attn_reductions] plotting: strided_pool (8,16,32)")
        maps = _strided_pool_maps(A_last, sizes=(8,16,32))
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"avg k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["strided_pool"], f"strided_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:strided_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: gauss_pool (8,16,32)")
        maps = _gauss_blur_pool_maps(A_last, sizes=(8,16,32), sigma=1.0)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"gauss k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["gauss_pool"], f"gauss_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:gauss_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: dist_binned (bins=64)")
        prof = _distance_binned_profile(A_last, bins=64)[0].numpy()
        fig = plt.figure(figsize=(5, 1.6))
        x = np.arange(prof.shape[-1])
        plt.plot(x, prof, linewidth=1.0)
        plt.title("Distance-binned attention", fontsize=9)
        _save_line(fig, os.path.join(subdirs["dist_binned"], f"dist_binned_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:dist_binned] skipped: {e}")

    try:
        print("[attn_reductions] plotting: topk_pool (1%,5%,10%)")
        maps = _topk_sparse_pool(A_last, ratios=(0.01, 0.05, 0.10), size=32)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, label in zip(axes, maps, ("top-1%", "top-5%", "top-10%")):
            M = m[0].numpy()
            _imshow_small_project(ax, M, label, think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["topk_pool"], f"topk_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:topk_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: cluster_pool (K=16,32,64)")
        Ks = (16,32,64)
        maps = _cluster_pool(A_last, Ks=Ks)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, K in zip(axes, maps, Ks):
            assign = _kmeans1d_assign(S, K)
            marks = [int(assign[p].item()) for p in think_pos_tokens if 0 <= p < S]
            _imshow_small(ax, m[0].numpy(), f"cluster K={K}", marks)
        _save_grid(fig, os.path.join(subdirs["cluster_pool"], f"cluster_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:cluster_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: landmarks (M=16,32,64)")
        Ms_ = (16,32,64)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        outdeg = A_last.sum(-1)[0]  # (S,)
        for ax, Msize in zip(axes, Ms_):
            Msize = min(Msize, S)
            idx = torch.topk(outdeg, Msize, dim=-1).indices.long()
            idx_sorted, order = idx.sort()
            pos_map = {int(tok.item()): int(i) for i, tok in enumerate(idx_sorted)}
            marks = [pos_map[p] for p in think_pos_tokens if p in pos_map]
            Lm = A_last[0][idx][:, idx]
            Lm = Lm[order][:, order]
            _imshow_small(ax, Lm.numpy(), f"landmarks M={Msize}", marks)
        _save_grid(fig, os.path.join(subdirs["landmarks"], f"landmarks_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:landmarks] skipped: {e}")

    try:
        print("[attn_reductions] plotting: pyramid (8,16,32)")
        maps = _pyramid_maps(A_last, scales=(8,16,32))
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"pyr k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["pyramid"], f"pyramid_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:pyramid] skipped: {e}")

    # === Span-based Internal vs External ===
    try:
        print("[attn_reductions] computing: internal/external ratios (span)")
        close_positions = think_pos_tokens
        span = _find_internal_span(input_ids[b], close_positions,
                                   think_open_seq=think_open_seq,
                                   internal_window=internal_window)
        if span is None:
            print("[attn_reductions:int_ext_ratio] skipped (no </think> found)")
        else:
            s_int, e_int = span
            print(f"[attn_reductions:int_ext_ratio] internal span: [{s_int}, {e_int})")
            print("[attn_reductions] plotting: int_ext_ratio per-layer line")
            _plot_int_ext_layers(
                A_layers=A_all_cpu[b],
                start=s_int, end=e_int,
                path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_layers_{tag}.png")
            )
            print("[attn_reductions] plotting: int_ext_ratio last-layer bar")
            _plot_int_ext_last_bar(
                A_last=A_last[0],
                start=s_int, end=e_int,
                path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_last_{tag}.png")
            )
    except Exception as e:
        print(f"[attn_reductions:int_ext_ratio] skipped: {e}")

    # === Grouped Internal vs External ===
    try:
        print("[attn_reductions] plotting: group_int_ext lines (K=8,16,32)")
        _plot_group_intra_external_lines(
            A_layers=A_all_cpu[b],
            path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_lines_{tag}.png"),
            Ks=(8,16,32),
        )
    except Exception as e:
        print(f"[attn_reductions:group_int_ext_lines] skipped: {e}")

    try:
        print("[attn_reductions] plotting: group_int_ext bars (K=8,16,32)")
        _plot_group_intra_external_bars(
            A_layers=A_all_cpu[b],
            path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_bars_{tag}.png"),
            Ks=(8,16,32),
        )
    except Exception as e:
        print(f"[attn_reductions:group_int_ext_bars] skipped: {e}")


#**********************************************************************************************************************


logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = "Qwen/Qwen3-8B"
_CONFIG_FOR_DOC = "Qwen3Config"


def assert_finite(**named_tensors):
    """Raise if any tensor contains NaN/Inf."""
    for k, v in named_tensors.items():
        if not torch.all(torch.isfinite(v)):
            raise FloatingPointError(f"{k} has NaN/Inf")


#myedit
@dataclass
class CausalLMOutputWithPast(ModelOutput):
    """
    Outputs for causal language modeling with an optional stop-probability head.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Training loss (e.g., LM loss or stop-head loss).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
            Pre-softmax scores from the LM head.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):
            A nested tuple with length `config.num_hidden_layers`. Each inner tuple contains
            key/value tensors of shape `(batch_size, num_heads, seq_len, head_dim)`.
            Can be fed to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of hidden states (output of embeddings + each layer), each of shape
            `(batch_size, sequence_length, hidden_size)`.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of attention maps for each layer, each of shape
            `(batch_size, num_heads, sequence_length, sequence_length)`.
        stop_prob (`torch.FloatTensor` of shape `(batch_size, sequence_length)` in train
            or `(batch_size, 1, 1)` at inference, *optional*):
            Probability from the stop head indicating that thinking should end at each position.
    """
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, ...], ...]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
    stop_prob: Optional[torch.FloatTensor] = None



class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling

    if attention_mask is not None:
        # Accept 2D [B,S] or 4D [B,1,Q,K]
        if attention_mask.dim() == 2:
            # 0/1 or bool -> additive mask with -inf for masked keys
            m = (attention_mask != 0).to(attn_weights.dtype)       # [B,S]
            add = (1.0 - m) * torch.finfo(attn_weights.dtype).min  # [B,S]
            attention_mask = add[:, None, None, :]                 # [B,1,1,S]
    if attention_mask is not None:
        # print()
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window
        if not (
            self.config.use_sliding_window
            and getattr(self.config, "sliding_window", None) is not None
            and self.layer_idx >= self.config.max_window_layers
        ):
            self.sliding_window = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: Tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_value: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_value is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        #myedit
        # attention_interface: Callable = eager_attention_forward
        # if self.config._attn_implementation != "eager":
        #     if self.config._attn_implementation == "sdpa" and kwargs.get("output_attentions", False):
        #         logger.warning_once(
        #             "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
        #             'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
        #         )
        #     else:
        #         attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]
        # --- choose kernel -------------------------------------------------------
        want_weights = kwargs.get("output_attentions", False)

        if want_weights:                       # <â”€â”€ we need A = softmax(qkáµ€/âˆšd)
            attention_interface = eager_attention_forward   # eager can return it
        else:                                   # keep the fast kernel
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]


        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3DecoderLayer(nn.Module):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)
        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        if (
            config.sliding_window and config._attn_implementation != "flash_attention_2"
        ):  # diff with Llama is this warning
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        # #myedit
        # if output_attentions and self_attn_weights is not None:
        #     # (B , n_heads , S , S) â†’ (B , 1 , S , S)
        #     self_attn_weights = self_attn_weights.mean(dim=1, keepdim=True)

        if output_attentions:
            outputs += (self_attn_weights,)

        return outputs


class Qwen3RotaryEmbedding(nn.Module):
    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and config.rope_scaling is not None:
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


QWEN3_START_DOCSTRING = r"""
    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
    etc.)

    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
    and behavior.

    Parameters:
        config ([`Qwen3Config`]):
            Model configuration class with all the parameters of the model. Initializing with a config file does not
            load the weights associated with the model, only the configuration. Check out the
            [`~PreTrainedModel.from_pretrained`] method to load the model weights.
"""


@add_start_docstrings(
    "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
    QWEN3_START_DOCSTRING,
)
class Qwen3PreTrainedModel(PreTrainedModel):
    config_class = Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn_2 = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _supports_cache_class = True
    _supports_quantized_cache = True
    _supports_static_cache = True
    _supports_attention_backend = True

    def _init_weights(self, module):
        std = self.config.initializer_range
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=std)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()



#myedit
# ---- utils ----
def lower_tri_mask(N, device):
    return torch.tril(torch.ones(N, N, device=device, dtype=torch.bool))

def conv1d_fp32(x, w, b=None, dilation=1, causal_pad_left=0, groups=1, stride=1):
    x32 = F.pad(x, (causal_pad_left, 0)).float() if causal_pad_left > 0 else x.float()
    return F.conv1d(x32, w.float(), None if b is None else b.float(),
                    stride=stride, padding=0, dilation=dilation, groups=groups)

def conv2d_fp32(x, conv: nn.Conv2d):
    return F.conv2d(
        x.float(),
        conv.weight.float(),
        None if conv.bias is None else conv.bias.float(),
        stride=conv.stride, padding=conv.padding, dilation=conv.dilation, groups=conv.groups
    )

# ---- stable causal TCN (1D) ----
class _CausalConv1d(nn.Module):
    def __init__(self, c_in, c_out, kernel_size=3, dilation=1, bias=True):
        super().__init__()
        self.ks, self.dil = kernel_size, dilation
        self.weight = nn.Parameter(torch.empty(c_out, c_in, kernel_size))
        self.bias   = nn.Parameter(torch.zeros(c_out)) if bias else None
        nn.init.kaiming_normal_(self.weight, nonlinearity="linear")

    def forward(self, x):  # (B,C,N)
        pad_left = (self.ks - 1) * self.dil
        return conv1d_fp32(x, self.weight, self.bias, dilation=self.dil, causal_pad_left=pad_left)  # fp32

class _Pointwise1d(nn.Module):
    def __init__(self, c_in, c_out, bias=True):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(c_out, c_in, 1))
        self.bias   = nn.Parameter(torch.zeros(c_out)) if bias else None
        nn.init.kaiming_normal_(self.weight, nonlinearity="linear")

    def forward(self, x):  # (B,C,N) fp32
        return conv1d_fp32(x, self.weight, self.bias)

class CausalTCN1D(nn.Module):
    """causal, fp32 math inside, returns to input dtype; residual + RMSNorm to avoid NaNs in bf16."""
    def __init__(self, c_in, d, layers=3, kernel_size=3, dropout=0.0):
        super().__init__()
        self.blocks = nn.ModuleList()
        cin = c_in
        for i in range(layers):
            dil = 2 ** i
            self.blocks.append(nn.ModuleDict({
                "causal": _CausalConv1d(cin, d, kernel_size=kernel_size, dilation=dil, bias=True),
                "act1":  nn.GELU(),
                "pw":    _Pointwise1d(d, d, bias=True),
                "act2":  nn.GELU(),
                "drop":  nn.Dropout(dropout),
                "rms":   nn.RMSNorm(d, eps=1e-5),
                "res":   _Pointwise1d(cin, d, bias=False) if cin != d else nn.Identity(),
            }))
            cin = d
        self.res_scale = 0.5
        self.main_scale = 0.5

    def forward(self, x):  # (B*, C, N)
        in_dtype = x.dtype
        h = x
        for blk in self.blocks:
            y = blk["causal"](h)         # fp32
            y = blk["act1"](y)           # fp32
            y = blk["pw"](y)             # fp32
            y = blk["act2"](y)           # fp32
            y = blk["drop"](y)           # fp32
            y = blk["rms"](y.transpose(1, 2)).transpose(1, 2)  # fp32, norm over channels
            res = h if isinstance(blk["res"], nn.Identity) else blk["res"](h)  # fp32
            h = self.main_scale * y + self.res_scale * res                      # fp32
            h = torch.clamp(torch.nan_to_num(h, nan=0.0), -1e3, 1e3)            # safety
        return h.to(in_dtype)

# ---- depthwise 2D (fp32 convs inside) ----
class Depthwise2D(nn.Module):
    def __init__(self, c_in, d, blocks=2, ks=3):
        super().__init__()
        blks = []
        cin = c_in
        for _ in range(blocks):
            blks += [nn.Conv2d(cin, cin, ks, padding=ks//2, groups=cin), nn.GELU(),
                     nn.Conv2d(cin, d, 1), nn.GELU()]
            blks += [nn.Conv2d(d, d, (1,7), padding=(0,3)), nn.GELU(),
                     nn.Conv2d(d, d, (7,1), padding=(3,0)), nn.GELU()]
            cin = d
        self.net = nn.Sequential(*blks)
        self.out_dim = d

    def forward(self, x):  # (B,C,N,N)
        dt = x.dtype
        y = x
        for m in self.net:
            if isinstance(m, nn.Conv2d):
                y = conv2d_fp32(y, m)    # fp32 conv with fp32 weights/bias
            else:
                y = m(y.float())         # activations in fp32
        return y.to(dt)

# ---- super simple causal encoder ----
class SimpleCausalBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_ff=2048, dropout=0.0):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)
        self.ff   = nn.Sequential(nn.Linear(d_model, dim_ff), nn.GELU(),
                                  nn.Linear(dim_ff, d_model), nn.Dropout(dropout))
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):  # (B,N,D)
        x = x.to(next(self.attn.parameters()).dtype)
        N = x.size(1)
        mask = torch.triu(torch.ones((N, N), device=x.device, dtype=torch.bool), 1)
        y, _ = self.attn(x, x, x, attn_mask=mask, need_weights=False)
        x = self.norm1(x + y)
        x = self.norm2(x + self.ff(x))
        return x

class SimpleCausalEncoder(nn.Module):
    def __init__(self, d_model, nhead=8, num_layers=2, dim_ff=1024, dropout=0.0):
        super().__init__()
        self.layers = nn.ModuleList([SimpleCausalBlock(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)])
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# ---- the embedder ----
class AttnFeatureEmbedder(nn.Module):
    def __init__(self, c_in, d_model=256, d_emb=256, bands=(0,1,2,4,8), tcn_layers=3, use_2d=True):
        super().__init__()
        self.use_2d = use_2d
        self.bands  = bands

        self.tcn_row = CausalTCN1D(c_in, d_model, layers=tcn_layers)
        self.tcn_col = CausalTCN1D(c_in, d_model, layers=tcn_layers)

        if use_2d:
            self.cnn2d = Depthwise2D(c_in, d_model // 2, blocks=2)
            d2, n_band = self.cnn2d.out_dim, len(bands)
        else:
            d2, n_band = 0, 0

        self.q_row = nn.Parameter(torch.randn(d_model))
        self.q_col = nn.Parameter(torch.randn(d_model))

        n_stats = 7
        fuse_in = (2 * d_model) + d2 + n_stats + n_band
        self.fuse = nn.Sequential(nn.LayerNorm(fuse_in), nn.Linear(fuse_in, d_emb))
        self.causal_enc = SimpleCausalEncoder(d_model=d_emb, nhead=8, num_layers=3)

    @staticmethod
    def _stats(A):  # A: (B,N,N), nonneg
        Af = A.float()
        Af = Af / (Af.sum(-1, keepdim=True) + 1e-8)
        eps = 1e-6
        B, N, _ = Af.shape
        H    = - (Af.clamp_min(eps) * Af.clamp_min(eps).log()).sum(-1)
        topk = Af.topk(k=min(4, N), dim=-1).values.sum(-1)
        outd = Af.sum(-1); ind = Af.sum(-2)
        asym = (Af - Af.transpose(-1,-2)).abs().sum(-1)
        j = torch.arange(N, device=A.device, dtype=Af.dtype)
        dist = (j[None,None,:] - j[None,:,None]).abs()
        mu  = (Af * dist).sum(-1)
        var = (Af * (dist - mu.unsqueeze(-1))**2).sum(-1)
        return [H, topk, outd, ind, asym, mu, var]

    def forward(self, X):  # X: (B,C,N,N)
        B, C, N, _ = X.shape
        device = X.device

        # causal mask and masked attention map
        m  = lower_tri_mask(N, device)
        Xc = X.masked_fill(~m[None, None], 0.0)

        # rows: (B,N_rows,C,N_cols) -> (B*N_rows,C,N_cols)
        rows = Xc.permute(0, 2, 1, 3).reshape(B * N, C, N)
        row_feats = self.tcn_row(rows).transpose(1, 2).reshape(B, N, N, -1)
        q_row = self.q_row.to(row_feats.dtype)[None, None, None, :]
        logits = (row_feats * q_row).sum(-1)  # (B,N,N)

        # cols: swap N,N then same
        cols = Xc.transpose(-1, -2).permute(0, 2, 1, 3).reshape(B * N, C, N)
        col_feats = self.tcn_col(cols).transpose(1, 2).reshape(B, N, N, -1)
        q_col = self.q_col.to(col_feats.dtype)[None, None, None, :]
        logits_c = (col_feats * q_col).sum(-1)

        # stable softmax with additive -inf (fp32)
        mask_inf = torch.triu(torch.full((N, N), float('-inf'), device=device, dtype=torch.float32), 1)
        w  = torch.softmax(logits.float()   + mask_inf, dim=-1).to(row_feats.dtype)
        wc = torch.softmax(logits_c.float() + mask_inf, dim=-1).to(col_feats.dtype)

        r = (row_feats * w[...,  None]).sum(2)  # (B,N,d_model)
        c = (col_feats * wc[..., None]).sum(2)  # (B,N,d_model)

        parts = [r, c]
        band_feats = []

        if self.use_2d:
            U = self.cnn2d(Xc)                               # (B,d2,N,N) safe fp32 inside
            u_row = U.mean(-1)                               # (B,d2,N)
            u_col = U.mean(-2)                               # (B,d2,N)
            Utok  = (u_row.transpose(1, 2) + u_col.transpose(1, 2)) * 0.5  # (B,N,d2)
            parts.append(Utok)

            for k in self.bands:
                L = max(N - k, 0)
                diag = torch.diagonal(U, offset=-k, dim1=-2, dim2=-1).contiguous()  # (B,d2,L)
                if N - L > 0:
                    diag = F.pad(diag, (0, N - L))                                   # (B,d2,N)
                band_feats.append(diag.mean(1))                                      # (B,N)

        # graph/stat feats
        A = (Xc.sum(1) + 1e-8)                # (B,N,N)
        S = [s.to(Xc.dtype) for s in self._stats(A)]  # 7*(B,N)

        # fuse
        feats   = torch.cat(parts, dim=-1)                   # (B,N, 2*d_model (+ d2))
        scalars = torch.stack(S, dim=-1)                     # (B,N,7)
        if band_feats:
            band_stack = torch.stack(band_feats, dim=-1)     # (B,N,|bands|)
            alltok = torch.cat([feats, scalars, band_stack], dim=-1)
        else:
            alltok = torch.cat([feats, scalars], dim=-1)

        z = self.fuse(alltok.to(next(self.fuse.parameters()).dtype))  # (B,N,d_emb)
        z = self.causal_enc(z.to(next(self.causal_enc.parameters()).dtype))
        return z


class ConfidenceCurveEncoder(nn.Module):
    """
    Turns scalar token-prob curves (B, S) into causal features (B, S, D).
    Receptive field grows exponentially with layer depth.
    """
    def __init__(
        self,
        out_dim: int,
        hidden: int = 64,
        n_layers: int = 4,
        kernel_size: int = 5,          # 3 works well; k must be odd and >= 2
    ):
        super().__init__()

        layers = []
        in_ch = 1
        dilation = 1
        for _ in range(n_layers):
            # causal padding: pad only on the **left**
            pad = (kernel_size - 1) * dilation
            layers.append(nn.ConstantPad1d((pad, 0), 0.0))   # (left,right)
            layers.append(
                nn.Conv1d(
                    in_channels=in_ch,
                    out_channels=hidden,
                    kernel_size=kernel_size,
                    dilation=dilation,
                    bias=False,
                )
            )
            layers.append(nn.GELU())
            in_ch = hidden
            dilation *= 2           # double dilation â†’ exponential RF

        # final 1Ã—1 projection to out_dim
        layers.append(nn.Conv1d(in_ch, out_dim, kernel_size=1, bias=False))
        self.net = nn.Sequential(*layers)

        # register output dtype for convenient casting later
        self.out_dtype = torch.get_default_dtype()

    def forward(self, probs: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        probs : (B, S)  â€“ token probabilities (any float dtype)
        mask  : (B, S)  â€“ 1 for real tokens, 0 for padding
        returns (B, S, out_dim)
        """
        x = probs.unsqueeze(1)          # (B, 1, S)
        x = self.net(x)                # (B, out_dim, S)
        x = x.transpose(1, 2)          # (B, S, out_dim)
        return x * mask.unsqueeze(-1)  # keep padding at zero


# def gauss_blur_adaptive_pool(
#     A_tok: torch.Tensor,        # (B, C, S, S)
#     input_ids,                  # (B, S)  list/np/tensor all OK
#     sep_id: int                 # scalar separator token id
# ) -> torch.Tensor:              # (B, C, Ns, Ns)
#     """
#     Aggregates token-level attention into sentence-level attention
#     **without** collapsing the C (=LÃ—H) channels.
#     """
#     device, dtype = A_tok.device, A_tok.dtype

#     # 1) ensure input_ids is a tensor on the same device
#     input_ids = torch.as_tensor(input_ids, device=device, dtype=torch.long)

#     # 2) sentence index per token
#     seg      = torch.cumsum((input_ids == sep_id).long(), dim=1)   # (B,S)
#     Ns       = int(seg.max().item()) + 1

#     # 3) one-hot assignment  H : (B, S, Ns)
#     H = F.one_hot(seg, Ns).to(dtype)                               # match A_tok dtype

#     # 4) aggregate:   Háµ€ Â· A_tok Â· H   keeping channels -------------
#     #    einsum handles the double sum over q (rows) and t (cols)
#     #    indices:        b = batch, c = channel, q/t = token, i/j = sentence
#     A_sent_sum = torch.einsum('bqi, bcqt, btj -> bcij', H, A_tok, H)  # (B,C,Ns,Ns)

#     # 5) divide by |S_i||S_j|  to get the **mean**
#     counts = H.sum(dim=1).clamp(min=1.0)            # (B,Ns)
#     denom  = counts.unsqueeze(1).unsqueeze(3) * counts.unsqueeze(1).unsqueeze(2)
#     #             (B,1,Ns,1)             * (B,1,1,Ns)  â†’ broadcast (B,1,Ns,Ns)
#     A_sent_mean = A_sent_sum / denom                # (B,C,Ns,Ns)

#     return A_sent_mean


def gauss_blur_adaptive_pool(
    A_tok: torch.Tensor,   # (B, C, S, S)
    size: int,             # target k (e.g., 16/32/64/â€¦)
    sigma: float = 1.0
) -> torch.Tensor:
    """
    Depthwise Gaussian blur (per-channel) + adaptive avg pool to (size,size).
    Preserves channels. Works on CPU/GPU and remains differentiable.
    Returns: (B, C, size, size)
    """
    assert A_tok.dim() == 4, "A_tok must be (B,C,S,S)"
    B, C, S, T = A_tok.shape
    assert S == T, "attention maps must be square"

    # sanitize size
    k = int(size)
    if k <= 0:
        raise ValueError(f"`size` must be > 0, got {size}")

    device, dtype = A_tok.device, A_tok.dtype

    # build separable Gaussian kernel
    if sigma <= 0:
        k2 = torch.ones((1, 1, 1, 1), device=device, dtype=dtype)
        pad = 0
    else:
        rad = int(3 * sigma + 0.5)
        t = torch.arange(-rad, rad + 1, device=device, dtype=dtype)
        g = torch.exp(-(t * t) / (2 * sigma * sigma))
        g = g / (g.sum() + 1e-12)
        k2 = (g[:, None] @ g[None, :]).unsqueeze(0).unsqueeze(0)  # (1,1,K,K)
        pad = k2.size(-1) // 2

    # depthwise blur (no channel mixing)
    weight = k2.expand(C, 1, k2.size(-2), k2.size(-1)).contiguous()
    X = F.conv2d(A_tok, weight=weight, bias=None, stride=1, padding=pad, groups=C)

    # exact kÃ—k
    return F.adaptive_avg_pool2d(X, (k, k))  # (B,C,k,k)



QWEN3_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
            it.

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            [What are input IDs?](../glossary#input-ids)
        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            [What are attention masks?](../glossary#attention-mask)

            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
            [`PreTrainedTokenizer.__call__`] for details.

            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see
            `past_key_values`).

            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]
            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more
            information on the default strategy.

            - 1 indicates the head is **not masked**,
            - 0 indicates the head is **masked**.
        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
            config.n_positions - 1]`.

            [What are position IDs?](../glossary#position-ids)
        past_key_values (`Cache`, *optional*):
            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`
            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.

            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).

            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't
            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`
            of shape `(batch_size, sequence_length)`.
        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
            model's internal embedding lookup matrix.
        use_cache (`bool`, *optional*):
            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see
            `past_key_values`).
        output_attentions (`bool`, *optional*):
            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
            tensors for more detail.
        output_hidden_states (`bool`, *optional*):
            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
            more detail.
        return_dict (`bool`, *optional*):
            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):
            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,
            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer
            the complete sequence length.
"""


@add_start_docstrings(
    "The bare Qwen3 Model outputting raw hidden-states without any specific head on top.",
    QWEN3_START_DOCSTRING,
)
class Qwen3Model(Qwen3PreTrainedModel):
    """
    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Qwen3DecoderLayer`]

    Args:
        config: Qwen3Config
    """

    def __init__(self, config: Qwen3Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value

    @can_return_tuple
    @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],
    ) -> BaseModelOutputWithPast:
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        use_cache = use_cache if use_cache is not None else self.config.use_cache

        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if self.gradient_checkpointing and self.training and use_cache:
            logger.warning_once(
                "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`."
            )
            use_cache = False

        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache
        if not isinstance(past_key_values, (type(None), Cache)):
            raise ValueError("The `past_key_values` should be either a `Cache` object or `None`.")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache()

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        causal_mask = self._update_causal_mask(
            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions
        )

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        # decoder layers
        all_hidden_states = () if output_hidden_states else None
        all_self_attns = () if output_attentions else None


        # â”€â”€â”€ main layer loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for idx, decoder_layer in enumerate(self.layers):
            need_attn = output_attentions and (idx % 10 == 9)

            if output_hidden_states:
                all_hidden_states += (hidden_states,)
            # print(causal_mask.shape)
            if self.gradient_checkpointing and self.training:
                layer_outputs = self._gradient_checkpointing_func(
                    partial(decoder_layer.__call__, **flash_attn_kwargs),
                    hidden_states,
                    causal_mask,
                    position_ids,
                    past_key_values,
                    # output_attentions,
                    need_attn,
                    use_cache,
                    cache_position,
                    position_embeddings,
                )
            else:
                layer_outputs = decoder_layer(
                    hidden_states,
                    attention_mask=causal_mask,
                    position_ids=position_ids,
                    past_key_value=past_key_values,
                    output_attentions=output_attentions,
                    use_cache=use_cache,
                    cache_position=cache_position,
                    position_embeddings=position_embeddings,
                    **flash_attn_kwargs,
                )

            hidden_states = layer_outputs[0]


            if need_attn:
                all_self_attns += (layer_outputs[1],)
            



        hidden_states = self.norm(hidden_states)

        # add hidden states from the last decoder layer
        if output_hidden_states:
            all_hidden_states += (hidden_states,)

        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
            hidden_states=all_hidden_states,
            attentions=all_self_attns,
        )

    def _update_causal_mask(
        self,
        attention_mask: torch.Tensor,
        input_tensor: torch.Tensor,
        cache_position: torch.Tensor,
        past_key_values: Cache,
        output_attentions: bool = False,
    ):
        if self.config._attn_implementation == "flash_attention_2":
            if attention_mask is not None and past_key_values is not None:
                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]
                if is_padding_right:
                    raise ValueError(
                        "You are attempting to perform batched generation with padding_side='right'"
                        " this may lead to unexpected behaviour for Flash Attention version of Qwen3. Make sure to "
                        " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
                    )
            if attention_mask is not None and 0.0 in attention_mask:
                return attention_mask
            return None

        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in
        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail
        # to infer the attention mask.
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        using_static_cache = isinstance(past_key_values, StaticCache)
        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)

        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward
        if (
            self.config._attn_implementation == "sdpa"
            and not (using_static_cache or using_sliding_window_cache)
            and not output_attentions
        ):
            if AttentionMaskConverter._ignore_causal_mask_sdpa(
                attention_mask,
                inputs_embeds=input_tensor,
                past_key_values_length=past_seen_tokens,
                sliding_window=self.config.sliding_window,
                is_training=self.training,
            ):
                return None

        dtype, device = input_tensor.dtype, input_tensor.device
        min_dtype = torch.finfo(dtype).min
        sequence_length = input_tensor.shape[1]
        # SlidingWindowCache or StaticCache
        if using_sliding_window_cache or using_static_cache:
            target_length = past_key_values.get_max_cache_shape()
        # DynamicCache or no cache
        else:
            target_length = (
                attention_mask.shape[-1]
                if isinstance(attention_mask, torch.Tensor)
                else past_seen_tokens + sequence_length + 1
            )

        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).
        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(
            attention_mask,
            sequence_length=sequence_length,
            target_length=target_length,
            dtype=dtype,
            device=device,
            cache_position=cache_position,
            batch_size=input_tensor.shape[0],
            config=self.config,
            past_key_values=past_key_values,
        )

        if (
            self.config._attn_implementation == "sdpa"
            and attention_mask is not None
            and attention_mask.device.type in ["cuda", "xpu"]
            and not output_attentions
        ):
            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when
            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.
            # Details: https://github.com/pytorch/pytorch/issues/110213
            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)

        return causal_mask

    @staticmethod
    def _prepare_4d_causal_attention_mask_with_cache_position(
        attention_mask: torch.Tensor,
        sequence_length: int,
        target_length: int,
        dtype: torch.dtype,
        device: torch.device,
        cache_position: torch.Tensor,
        batch_size: int,
        config: Qwen3Config,
        past_key_values: Cache,
    ):
        """
        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

        Args:
            attention_mask (`torch.Tensor`):
                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.
            sequence_length (`int`):
                The sequence length being processed.
            target_length (`int`):
                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.
            dtype (`torch.dtype`):
                The dtype to use for the 4D attention mask.
            device (`torch.device`):
                The device to place the 4D attention mask on.
            cache_position (`torch.Tensor`):
                Indices depicting the position of the input sequence tokens in the sequence.
            batch_size (`torch.Tensor`):
                Batch size.
            config (`Qwen3Config`):
                The model's configuration class
            past_key_values (`Cache`):
                The cache class that is being used currently to generate
        """
        if attention_mask is not None and attention_mask.dim() == 4:
            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.
            causal_mask = attention_mask
        else:
            min_dtype = torch.finfo(dtype).min
            causal_mask = torch.full(
                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device
            )
            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)
            if config.sliding_window is not None:
                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also
                # the check is needed to verify is current checkpoint was trained with sliding window or not
                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:
                    sliding_attend_mask = torch.arange(target_length, device=device) <= (
                        cache_position.reshape(-1, 1) - config.sliding_window
                    )
                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)
            causal_mask *= diagonal_attend_mask
            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)
            if attention_mask is not None:
                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit
                if attention_mask.shape[-1] > target_length:
                    attention_mask = attention_mask[:, :target_length]
                mask_length = attention_mask.shape[-1]
                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(
                    causal_mask.device
                )
                padding_mask = padding_mask == 0
                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(
                    padding_mask, min_dtype
                )
        return causal_mask


class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...


class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        #myedit
        # D_ATT  = getattr(config, "stop_attn_dim", 128)
        D_CONF = getattr(config, "stop_conf_dim", 64)
        D_HID = getattr(config, "stop_hid_dim", 128)          # new hyper-param

        D_ATT = getattr(config, "stop_axial_dim", 128)
        # C_IN    = config.num_hidden_layers * config.num_attention_heads
        # COLLECT_EVERY = 8                       # we keep layers 3,7,11,â€¦
        # LAYERS_COLLECTED = (config.num_hidden_layers + COLLECT_EVERY - 1) // COLLECT_EVERY
        C_IN = 3 

        SEP_ID  = getattr(config, "sep_token_id", 198)     # add to config

        self.stop_threshold = getattr(config, "stop_threshold", 0.5)
        self.stop_token_id  = 151668   # "</think>"
        self.newline_token_id = 198 # "\n"

        self.axial_sent_encoder  = AttnFeatureEmbedder(c_in = C_IN, d_emb=D_ATT)

        self.hid_encoder = nn.Sequential(
            nn.LayerNorm(config.hidden_size),
            nn.Linear(config.hidden_size, D_HID),
            nn.GELU()
        )

        self.conf_encoder  = ConfidenceCurveEncoder(out_dim=D_CONF)

        concat_dim = D_HID + D_CONF + D_ATT
        self.stop_head = nn.Sequential(
            nn.LayerNorm(concat_dim),
            nn.Linear(concat_dim, concat_dim),
            nn.GELU(),
            nn.Linear(concat_dim, 1)          # scalar logit
        )

        # Initialize weights and apply final processing
        self.post_init()

        # myedit, FREEZE everything except the three new modules
        for n, p in self.named_parameters():
            if not (n.startswith("hid_encoder") or n.startswith("axial_tok_encoder") or n.startswith("axial_sent_encoder") or n.startswith("conf_encoder") or n.startswith("stop_head")):
                p.requires_grad_(False)


    #  myedit
    def _should_stop(
        self,
        last_hidden: torch.Tensor,          # (B,S,H)
        attn_stack:  Optional[List[torch.Tensor]],   # L Ã— (B,H,S,S)  or None
        token_probs: torch.Tensor,          # (B,S)
        mask:        torch.Tensor,          # (B,S)
        input_ids:   Optional[torch.Tensor] # (B,S)  needed for sentence feats
    ) -> torch.Tensor:                      # returns (B,S,1) stop-probabilities
        """
        Combines multiple feature streams (hidden, token-axial, sentence-axial,
        confidence) and feeds them to the stop head.
        """
        B, S, _ = last_hidden.shape
        eps = 1e-6
        # print(attn_stack)
        # â”€â”€ dtype helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        hid_dtype  = next(self.hid_encoder.parameters()).dtype
        conf_dtype = next(self.conf_encoder.parameters()).dtype
        out_dtype  = next(self.stop_head.parameters()).dtype
        ax_dtype   = next(self.axial_sent_encoder.parameters()).dtype

        # â”€â”€ 1) Hidden-state encoder  (B,S,D_HID) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        z_hid = self.hid_encoder(last_hidden.to(hid_dtype))          # (B,S,D_HID)

        A_tok = torch.stack(attn_stack, dim=1).to(ax_dtype)          # contiguous
        B_, L, H_, S_, _ = A_tok.shape                               # unpack

        A_tok = A_tok.mean(dim=2).to(A_tok.dtype)
        A_flat = A_tok.reshape(B_, L, S_, S_)                   # (B,C,S,S)
            
        # â”€â”€ 3) Sentence-level axial attention  z_ax_sent : (B,S,D_AXIAL) â”€â”€â”€â”€â”€â”€â”€â”€
        if attn_stack is None or input_ids is None:
            z_ax_sent = torch.zeros_like(z_ax_tok)
        else:
            SEP_ID = 198                 
            # 3a)  (B,C,S,S)  â†’ (B,C,Ns,Ns)     *channels preserved*

            # A_sent = token_to_sentence_attention(A_flat, input_ids, SEP_ID)  # (B,C,Ns,Ns)
            grid_size = int(getattr(self, "stop_grid_size", 32))   # <-- set this in your __init__/config
            grid_size = max(1, grid_size)
            A_sent = gauss_blur_adaptive_pool(A_flat, size=grid_size, sigma=1.0)  # (B,L,k,k)

            # 3b)  axial encoder over sentence map
            z_sent = self.axial_sent_encoder(A_sent.float())                       # (B,Ns,D_ax)

            # 6) broadcast back to tokens via linear bucket map i â†¦ floor(i*k/S)
            device = last_hidden.device
            idx = torch.floor(torch.arange(S, device=device, dtype=torch.float32) * (float(grid_size) / max(1.0, float(S)))).long()
            idx = idx.clamp_max(grid_size - 1)
            z_ax_sent = z_sent[:, idx, :]        


        # â”€â”€ 4) Confidence-curve encoder  (B,S,D_CONF) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        z_conf = self.conf_encoder(token_probs.to(conf_dtype),
                                mask.to(conf_dtype))                      # (B,S,D_CONF)

        # â”€â”€ 5) Concatenate all features and feed stop head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        cat = torch.cat([ z_hid.to(out_dtype),
                        # z_ax_tok.to(out_dtype),
                        z_ax_sent.to(out_dtype),
                        z_conf.to(out_dtype) ], dim=-1)                    # (B,S,C)
        logits = self.stop_head(cat.view(B * S, -1))                        # (BÂ·S,1)


        # plot_stop_viz(
        #     input_ids=input_ids,
        #     token_probs=token_probs,
        #     mask=mask,
        #     attn_stack=attn_stack,
        #     save_root="/home/amirhosein/codes/BudgetGuidance/visualization",
        #     sep_id=198,
        #     think_id=151668,
        #     batch_index=0,
        # )

        # plot_all_token_attn_reductions(
        #     input_ids=input_ids,
        #     attn_stack=attn_stack,
        #     save_root="/home/amirhosein/codes/BudgetGuidance/visualization_attn",
        #     think_id=getattr(self, "stop_token_id", 151668),     # or think_token_seq=...
        #     think_token_seq=None,
        #     batch_index=0,
        #     tag="train"  # or any string you like
        # )


        return torch.sigmoid(logits).view(B, S, 1)

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model


    # def compute_stop_loss(self, p, target, mask, eps=1e-6):
    #     """
    #     p      : (B,S) stop probabilities in (0,1)
    #     target : (B,S) binary 1 at ground-truth </think> position
    #     mask   : (B,S) 1 for real tokens, 0 for pads
    #     """
    #     p, target, mask = p.float(), target.float(), mask.float()
    #     # p = p.clamp_(eps, 1.0 - eps)               # 1ï¸âƒ£ strictly inside (0,1)

    #     log_surv = torch.cumsum(torch.log1p(-p) * mask, dim=1) - torch.log1p(-p) * mask
    #     logq = torch.log(p) + log_surv             # log q_t
    #     logq = logq - logq.max(dim=1, keepdim=True).values     # 2ï¸âƒ£ subtract max
    #     q = (logq.exp() * mask)                    # still finite
    #     q = q / (q.sum(dim=1, keepdim=True) + eps)            # 3ï¸âƒ£ normalise

    #     B, S = p.shape
    #     t = torch.arange(S, device=p.device).unsqueeze(0)
    #     t_star = target.argmax(dim=1, keepdim=True)           # index of </think>
    #     dist = (t - t_star).abs() * mask
    #     loss_vec = (q * dist).sum(dim=1)

    #     valid = target.sum(dim=1) > 0
    #     return loss_vec[valid].mean()

    # symetric but more stable
    def compute_stop_loss(self, p, target, mask, eps=1e-6):
        """
        p      : (B,S) stop probabilities in (0,1)
        target : (B,S) binary 1 at ground-truth </think> position
        mask   : (B,S) 1 for real tokens, 0 for pads
        """
        # ---- sanitize & fp32 ----
        p      = torch.nan_to_num(p.float(),    nan=0.5)
        target = torch.nan_to_num(target.float(), nan=0.0)
        mask   = torch.nan_to_num(mask.float(),   nan=0.0)

        # Keep strictly inside (0,1) to avoid log(0) and log1p(-1)
        p = p.clamp(eps, 1.0 - eps)

        # ---- hazard -> event pmf q via MASKED log-softmax (no manual divide) ----
        # log q_t = log p_t + sum_{i<t} log(1 - p_i)
        log1m_p  = torch.log1p(-p) * mask                          # (B,S)
        log_surv = torch.cumsum(log1m_p, dim=1) - log1m_p          # exclusive cumsum
        logp     = torch.where(mask > 0, torch.log(p), torch.zeros_like(p))
        logq_raw = logp + log_surv                                  # (B,S)

        neg_inf = torch.finfo(logq_raw.dtype).min
        logq_masked = torch.where(mask > 0, logq_raw, torch.full_like(logq_raw, neg_inf))
        logq = torch.log_softmax(logq_masked, dim=1)                # (B,S), normalized over valid tokens
        q    = torch.exp(logq)                                      # (B,S)

        # ---- your distance objective ----
        B, S = p.shape
        t = torch.arange(S, device=p.device).unsqueeze(0)
        t_star = target.argmax(dim=1, keepdim=True)                 # (B,1)
        dist = (t - t_star).abs() * mask
        loss_vec = (q * dist).sum(dim=1)

        valid = target.sum(dim=1) > 0
        loss = loss_vec[valid].mean() if valid.any() else loss_vec.mean()

        # final safety: replace non-finite (keeps scaler alive on edge cases)
        if not torch.isfinite(loss):
            loss = torch.zeros((), device=p.device, dtype=torch.float32)
        return loss


    @can_return_tuple
    @deprecate_kwarg("num_logits_to_keep", version="4.50", new_name="logits_to_keep")
    @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        # ---- NEW kwargs for stop-head ----
        #  â†“â†“â†“ extra kwargs used at generation
        token_probs_so_far: Optional[torch.Tensor] = None,   # (B,S) float
        apply_budget: Optional[bool] = None,
        **kwargs: Unpack[KwargsForCausalLM],
    ) -> CausalLMOutputWithPast:
        r"""
            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

            logits_to_keep (`int` or `torch.Tensor`, *optional*):
                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all
                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that
                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.
                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.
                This is useful when using packed tensor format (single dimension for batch and sequence length).

        Returns:

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        # assert input_ids.shape[0] == 1, "current implementation only supports batch size = 1"
        output_attentions = (
            True if (self.training or apply_budget)
            else (self.config.output_attentions if output_attentions is None else output_attentions)
        )
        output_hidden_states = False  # keep as True; you already set it this way


        with torch.no_grad():
            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
            outputs: BaseModelOutputWithPast = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=False,
                cache_position=cache_position,
                **kwargs,
            )


            hidden_states = outputs.last_hidden_state          # (B,S,H)
            logits        = self.lm_head(hidden_states)        # (B,S,V)

            # ------------------------------------------------------------------
            # build token_probs_so_far with numerically-stable scaling
            # ------------------------------------------------------------------
            if self.training and token_probs_so_far is None and labels is not None:
                with torch.no_grad():
                    # Align logits with labels
                    logits_step = logits[:, :-1, :] if logits.size(1) == labels.size(1) + 1 else logits

                    # log-softmax in full-precision â†’ avoids under-flow
                    logp = torch.log_softmax(logits_step.float(), dim=-1)                # (B,S,V)

                    # Gather log-prob of each gold token
                    tgt   = labels.clamp(min=0)                                          # avoid -100 in gather
                    log_tok_p = logp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)           # (B,S)

                    # Neutralise ignored positions (log(1)=0 keeps downstream math happy)
                    log_tok_p = torch.where(labels.eq(-100), torch.zeros_like(log_tok_p), log_tok_p)

                    # ----- optional: return to prob-space BUT clamp to Îµ so theyâ€™re never tiny -----
                    eps = 1e-8
                    tok_p = torch.clamp(log_tok_p.exp(), min=eps).to(logits_step.dtype)  # (B,S)

                token_probs_so_far = tok_p.detach()        # keep gradients out


        # --- Inside your Qwen3ForCausalLM.forward() method ---
        if self.training:
            assert labels is not None, "`labels` required for SFT of stop head"
            assert token_probs_so_far is not None, "`token_probs_so_far` required"

            # # 1) Ground-truth stop indicator per position (1 where token == </think>)
            # print(labels)
            target = (labels == self.stop_token_id).float()           # (B, S)
            
            mask   = attention_mask.float()                           # (B, S)
            eps    = 1e-6

            
            stop_prob = self._should_stop(
                                last_hidden = hidden_states,                     # (B,S,H)
                                attn_stack  = outputs.attentions,                # list[L] (B,H,S,S)
                                token_probs = token_probs_so_far,                # (B,S)
                                mask        = mask,
                                input_ids   = input_ids,
                             ).squeeze(-1)                                        # (B,S)

            # Clamp for stability before passing to the loss function
            p_clamped = stop_prob.clamp(min=1e-6, max=1 - 1e-6)

            # --- Call your new, stable loss function ---
            loss = self.compute_stop_loss(p_clamped, target, mask)

            return CausalLMOutputWithPast(
                loss=loss,
                logits=logits,
                past_key_values=outputs.past_key_values,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
                stop_prob=stop_prob.detach(),
            )

        else:
            stop_prob = None
            if apply_budget:
                # Require token_probs_so_far when budgeting
                if token_probs_so_far is None:
                    raise ValueError("`token_probs_so_far` must be provided when `apply_budget=True` in inference.")

                B, S, H = hidden_states.shape

                # --- Align shapes: token probs & mask up to S ---
                tok_p = token_probs_so_far
                if tok_p.size(1) < S:
                    # pad with neutral 1.0 for missing positions (doesn't change multiplicative stats)
                    pad = tok_p.new_full((B, S - tok_p.size(1)), 1.0)
                    tok_p = torch.cat([tok_p, pad], dim=1)
                elif tok_p.size(1) > S:
                    tok_p = tok_p[:, :S]

                mask = attention_mask[:, :S].float() if attention_mask is not None else \
                    torch.ones(B, S, device=hidden_states.device, dtype=torch.float)

                # If attentions weren't requested (shouldn't happen when apply_budget=True), skip compute
                if outputs.attentions is not None:
                    # With KV cache, q_len is already 1; still clip keys to :S
                    attn_stack = [a[:, :, -1:, :S] for a in outputs.attentions]  # list[L] of (B, H, 1, S)

                    # Compute stop prob for the **last** token only
                    stop_prob = self._should_stop(
                        last_hidden = hidden_states[:, -1:],         # (B,1,H)
                        attn_stack  = attn_stack,                    # list[L] (B,H,1,S)
                        token_probs = tok_p,                         # (B,S)
                        mask        = mask,                          # (B,S)
                        input_ids   = input_ids[:, :S] if input_ids is not None else None,
                    )                                                # expect (B,1,1)

                    # Numerical safety
                    stop_prob = stop_prob.clamp_(min=1e-6, max=1-1e-6)

        return CausalLMOutputWithPast(
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
            stop_prob=stop_prob,   # (B,1,1) or None
            )

@add_start_docstrings(
    """
    The Qwen3 Model transformer with a sequence classification head on top (linear layer).

    [`Qwen3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models
    (e.g. GPT-2) do.

    Since it does classification on the last token, it requires to know the position of the last token. If a
    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
    each row of the batch).
    """,
    QWEN3_START_DOCSTRING,
)
class Qwen3ForSequenceClassification(Qwen3PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen3Model(config)
        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> SequenceClassifierOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        transformer_outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        hidden_states = transformer_outputs.last_hidden_state
        logits = self.score(hidden_states)

        if input_ids is not None:
            batch_size = input_ids.shape[0]
        else:
            batch_size = inputs_embeds.shape[0]

        if self.config.pad_token_id is None and batch_size != 1:
            raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
        if self.config.pad_token_id is None:
            last_non_pad_token = -1
        elif input_ids is not None:
            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id
            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)
            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)
            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)
        else:
            last_non_pad_token = -1
            logger.warning_once(
                f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
                "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
            )

        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)

        return SequenceClassifierOutputWithPast(
            loss=loss,
            logits=pooled_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
        )


@add_start_docstrings(
    """
    The Qwen3 Model transformer with a token classification head on top (a linear layer on top of the hidden-states
    output) e.g. for Named-Entity-Recognition (NER) tasks.
    """,
    QWEN3_START_DOCSTRING,
)
class Qwen3ForTokenClassification(Qwen3PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.model = Qwen3Model(config)
        if getattr(config, "classifier_dropout", None) is not None:
            classifier_dropout = config.classifier_dropout
        elif getattr(config, "hidden_dropout", None) is not None:
            classifier_dropout = config.hidden_dropout
        else:
            classifier_dropout = 0.1
        self.dropout = nn.Dropout(classifier_dropout)
        self.score = nn.Linear(config.hidden_size, config.num_labels)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    @can_return_tuple
    @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        checkpoint=_CHECKPOINT_FOR_DOC,
        output_type=TokenClassifierOutput,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
    ) -> TokenClassifierOutput:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
        """

        outputs: BaseModelOutputWithPast = self.model(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )
        sequence_output = outputs.last_hidden_state
        sequence_output = self.dropout(sequence_output)
        logits = self.score(sequence_output)

        loss = None
        if labels is not None:
            loss = self.loss_function(logits, labels, self.config)

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


@add_start_docstrings(
    """
The Qwen3 Model transformer with a span classification head on top for extractive question-answering tasks like
SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).
    """,
    QWEN3_START_DOCSTRING,
)
class Qwen3ForQuestionAnswering(Qwen3PreTrainedModel):
    base_model_prefix = "transformer"

    def __init__(self, config):
        super().__init__(config)
        self.transformer = Qwen3Model(config)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)

        # Initialize weights and apply final processing
        self.post_init()

    def get_input_embeddings(self):
        return self.transformer.embed_tokens

    def set_input_embeddings(self, value):
        self.transformer.embed_tokens = value

    @can_return_tuple
    @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        start_positions: Optional[torch.LongTensor] = None,
        end_positions: Optional[torch.LongTensor] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        **kwargs,
    ) -> QuestionAnsweringModelOutput:
        r"""
        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the start of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
            are not taken into account for computing the loss.
        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
            Labels for position (index) of the end of the labelled span for computing the token classification loss.
            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
            are not taken into account for computing the loss.
        """

        outputs: BaseModelOutputWithPast = self.transformer(
            input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
        )

        sequence_output = outputs.last_hidden_state

        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1).contiguous()
        end_logits = end_logits.squeeze(-1).contiguous()

        loss = None
        if start_positions is not None and end_positions is not None:
            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)

        return QuestionAnsweringModelOutput(
            loss=loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3Model",
    "Qwen3PreTrainedModel",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
]
