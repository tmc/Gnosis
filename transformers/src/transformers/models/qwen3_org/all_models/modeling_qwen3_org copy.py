#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/qwen3/modular_qwen3.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_qwen3.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
# coding=utf-8
# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Callable, Optional, Union

import torch
from torch import nn

from ...activations import ACT2FN
from ...cache_utils import Cache, DynamicCache
from ...generation import GenerationMixin
from ...integrations import use_kernel_forward_from_hub
from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask
from ...modeling_flash_attention_utils import FlashAttentionKwargs
from ...modeling_layers import (
    GenericForQuestionAnswering,
    GenericForSequenceClassification,
    GenericForTokenClassification,
    GradientCheckpointingLayer,
)
from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...processing_utils import Unpack
from ...utils import TransformersKwargs, auto_docstring, can_return_tuple
from ...utils.deprecation import deprecate_kwarg
from ...utils.generic import check_model_inputs
from .configuration_qwen3 import Qwen3Config


#myedit, Visualization************************************************************************************************
# stop_viz.py (no cropping, small figs, no ticks)
import os, uuid, math
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from numbers import Integral
from typing import Optional, List, Sequence, Tuple
from matplotlib import colors  # <- already added above

# ---------- robust limits ----------
def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> tuple[float, float]:
    """Percentile-based vmin/vmax with safe fallback so we never get a flat colormap."""
    v = np.asarray(M)
    v = v[np.isfinite(v)]
    if v.size == 0:
        return 0.0, 1.0
    vmin = float(np.percentile(v, lo))
    vmax = float(np.percentile(v, hi))
    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
        vmin = float(v.min()) if v.size else 0.0
        vmax = float(v.max()) if v.size else 1.0
        if vmin == vmax:
            vmax = vmin + 1e-6
    return vmin, vmax

# ---------- small, enhanced token map (keep only ONE definition of this) ----------
def _plot_token_single_full(M, think_positions, path):
    S = M.shape[0]
    vmin, vmax = _robust_limits(M)
    norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)  # boost mid/low contrast

    plt.figure(figsize=(4, 4))  # small, fixed
    plt.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
    for t in (think_positions or []):
        if 0 <= t < S:
            plt.axvline(t, ls="--", lw=0.8, color="w")
            plt.axhline(t, ls="--", lw=0.8, color="w")
            plt.scatter([t], [t], s=10, c="red", marker="o")
    plt.xticks([]); plt.yticks([])  # no ticks to keep it light
    plt.title("Token attention (full, enhanced)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

@torch.no_grad()
def _unify_attentions(attn_stack):
    """
    attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
    Returns A_all: (B,L,S,S) after head-avg and square-izing.
    """
    if attn_stack is None or len(attn_stack) == 0:
        return None
    layers = []
    for A in attn_stack:
        A = A.float()  # (B,H,q,S)
        if A.dim() != 4:
            continue
        B, H, q, S = A.shape
        A = A.mean(dim=1)  # (B,q,S)
        if q == 1:
            sq = A.new_zeros(B, S, S)
            sq[:, -1, :] = A[:, 0, :]
            A_sq = sq
        elif q == S:
            A_sq = A
        else:
            if q < S:
                A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
            A_sq = A[:, :S, :S]
        layers.append(A_sq)  # (B,S,S)
    if not layers:
        return None
    return torch.stack(layers, dim=1)  # (B,L,S,S)

@torch.no_grad()
def _token_to_sentence_attention(A_tok: torch.Tensor, input_ids: torch.Tensor, sep_id: int):
    """
    A_tok: (B,L,S,S). Returns (B,L,Ns,Ns), seg, Ns.
    """
    B, L, S, _ = A_tok.shape
    seg = torch.cumsum((input_ids == sep_id).long(), dim=1)  # (B,S)
    Ns = int(seg.max().item()) + 1
    H = F.one_hot(seg, Ns).to(dtype=A_tok.dtype)             # (B,S,Ns)
    A_sum = torch.einsum("bqi, blqt, btj -> blij", H, A_tok, H)
    counts = H.sum(dim=1).clamp(min=1.0)                     # (B,Ns)
    denom = counts.unsqueeze(1).unsqueeze(3) * counts.unsqueeze(1).unsqueeze(2)
    return (A_sum / denom), seg, Ns

def _grid_dims(n: int) -> Tuple[int, int]:
    cols = int(np.ceil(np.sqrt(n)))
    rows = int(np.ceil(n / cols))
    return rows, cols

# ---------- outlier smoothing (confidence) ----------
@torch.no_grad()
def interp_outliers_inplace_token_probs(token_probs: torch.Tensor,
                                        mask: torch.Tensor,
                                        low: float = 1.0, high: float = 99.0) -> None:
    """
    In-place linear interpolation of outliers in token_probs per batch item.
    Outliers = values outside [low, high] percentiles over valid (mask>0) positions.
    """
    B, S = token_probs.shape
    for b in range(B):
        valid = (mask[b] > 0).cpu().numpy()
        if valid.sum() < 3:
            continue
        y = token_probs[b, valid].detach().float().cpu().numpy()
        x = np.arange(y.shape[0])

        lo, hi = np.percentile(y, [low, high])
        bad = (y < lo) | (y > hi)
        if not bad.any():
            continue

        x_good = x[~bad]
        y_good = y[~bad]
        if x_good.size >= 2:
            y[bad] = np.interp(x[bad], x_good, y_good)
        else:
            y = np.clip(y, lo, hi)

        token_probs[b, valid] = torch.as_tensor(y, device=token_probs.device, dtype=token_probs.dtype)

# ---------- marker finding (</think>) ----------
@torch.no_grad()
def _find_marker_positions(ids_row: torch.Tensor,
                           think_id: Optional[int] = None,
                           think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
    """
    Returns start indices where the marker occurs in input_ids.
    - If think_token_seq is provided, it searches for that subsequence.
    - Else falls back to single-token search using think_id.
    """
    ids = ids_row.tolist()
    positions: List[int] = []

    if think_token_seq and len(think_token_seq) > 0:
        pat = list(think_token_seq)
        m, n = len(pat), len(ids)
        if m <= n:
            for i in range(0, n - m + 1):
                if ids[i:i+m] == pat:
                    positions.append(i)
        return positions

    if think_id is not None:
        for i, tok in enumerate(ids):
            if tok == think_id:
                positions.append(i)
    return positions

# ---------- plotting helpers (small figs) ----------
def _plot_conf_full(probs, mask, think_positions, path):
    K = probs.shape[0]
    x = np.arange(K)
    plt.figure(figsize=(30, 5))
    plt.plot(x[mask > 0], probs[mask > 0], linewidth=1.0)
    for pos in think_positions:
        tmk = pos - 1  # probs[t] predicts token t+1
        if 0 <= tmk < K:
            plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
    plt.xticks([]); plt.yticks([])
    plt.title("Confidence (full)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

# ---------- NEW: uncertainty plots ----------
def _plot_uncertainty_full(unc, mask, think_positions, path, title="Uncertainty (full)"):
    """
    unc: numpy array shape (K,) with per-token uncertainty (e.g., -log p_gold or entropy).
    mask: numpy array shape (K,) in {0,1}
    """
    K = unc.shape[0]
    x = np.arange(K)
    plt.figure(figsize=(30, 5))
    plt.plot(x[mask > 0], unc[mask > 0], linewidth=1.0)
    for pos in think_positions:
        tmk = pos - 1  # aligns with next-token prediction index
        if 0 <= tmk < K:
            plt.axvline(int(tmk), ls="--", lw=0.8, color="r")
    plt.xticks([]); plt.yticks([])
    plt.title(title, fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _plot_token_grid_full(A_layers, think_positions, path, max_layers=16):
    """
    Show exactly the last up-to-3 layers (3Ã—1). Each subplot uses robust normalization.
    """
    L, S, _ = A_layers.shape
    if L == 0:
        return

    idxs = list(range(max(0, L - 3), L))  # [L-3, L-2, L-1]
    panels = len(idxs)

    fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
    if panels == 1:
        axes = [axes]

    for ax, li in zip(axes, idxs):
        M = A_layers[li]
        vmin, vmax = _robust_limits(M)
        norm = colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax)
        ax.imshow(M, aspect="auto", interpolation="nearest", norm=norm)
        for t in (think_positions or []):
            if 0 <= t < S:
                ax.axvline(t, ls="--", lw=0.8, color="w")
                ax.axhline(t, ls="--", lw=0.8, color="w")
                ax.scatter([t], [t], s=8, c="red", marker="o")
        ax.set_title(f"L{li}", fontsize=8)
        # keep ticks default off to stay light

    fig.tight_layout()
    fig.savefig(path, dpi=180)
    plt.close(fig)

def _plot_sentence_single(Ms, s_star, path):
    plt.figure(figsize=(4, 4))
    plt.imshow(Ms, aspect="auto", interpolation="nearest")
    if s_star is not None and 0 <= s_star < Ms.shape[0]:
        plt.axvline(s_star, ls="--", lw=0.8, color="w")
        plt.axhline(s_star, ls="--", lw=0.8, color="w")
    plt.xticks([]); plt.yticks([])
    plt.title("Sentence attention (full)", fontsize=9)
    plt.tight_layout()
    plt.savefig(path, dpi=200)
    plt.close()

def _plot_sentence_grid(M_layers, s_star, path, max_layers=16):
    L = M_layers.shape[0]
    L_show = min(L, max_layers)
    rows, cols = _grid_dims(L_show)
    vmax = float(M_layers[:L_show].max()) if L_show > 0 else 1.0
    vmax = max(vmax, 1e-12)

    fig, axes = plt.subplots(rows, cols, figsize=(2.0*cols, 2.0*rows))
    axes = np.atleast_2d(axes)
    idx = 0
    for r in range(rows):
        for c in range(cols):
            ax = axes[r, c]
            if idx < L_show:
                Ms = M_layers[idx]
                ax.imshow(Ms, aspect="auto", interpolation="nearest", vmin=0.0, vmax=vmax)
                if s_star is not None and 0 <= s_star < Ms.shape[0]:
                    ax.axvline(s_star, ls="--", lw=0.8, color="w")
                    ax.axhline(s_star, ls="--", lw=0.8, color="w")
                ax.set_title(f"L{idx}", fontsize=8)
                ax.set_xticks([]); ax.set_yticks([])
            else:
                ax.axis("off")
            idx += 1
    fig.tight_layout()
    fig.savefig(path, dpi=180)
    plt.close(fig)

# ---------- main (NO cropping) ----------
@torch.no_grad()
def plot_stop_viz(
    input_ids: torch.Tensor,          # (B,S)
    token_probs: torch.Tensor,        # (B,S)
    mask: torch.Tensor,               # (B,S)
    attn_stack,                       # list[L] of (B,H,S,S) or (B,H,1,S)
    save_root: str,
    sep_id: int = 198,
    think_id: Optional[int] = 151668,
    think_token_seq: Optional[Sequence[int]] = None,  # pass if </think> is multi-token
    batch_index: int = 0,
    max_layers_grid: int = 16,
    tag: Optional[str] = None,
    per_token_uncertainty: Optional[torch.Tensor] = None,  # <-- NEW (optional entropy/SCE)
    plot_uncertainty_delta: bool = False,                  # <-- NEW
):
    """
    Saves (FULL views, small figs, no ticks):
      - confidence/stop_conf_{tag}.png
      - uncertainty/stop_unc_{tag}.png                (NEW; surprisal or provided entropy)
      - uncertainty/stop_unc_delta_{tag}.png         (NEW; Î” uncertainty, optional)
      - token_attn/single/stop_tok_attn_{tag}.png
      - token_attn/grid/stop_tok_attn_grid_{tag}.png
      - sentence_attn/single/stop_sent_attn_{tag}.png
      - sentence_attn/grid/stop_sent_attn_grid_{tag}.png
    """
    if not isinstance(sep_id, Integral):
        sep_id = 198

    _ensure_dir(save_root)
    d_conf        = os.path.join(save_root, "confidence")
    d_unc         = os.path.join(save_root, "uncertainty")  # NEW
    d_tok_single  = os.path.join(save_root, "token_attn", "single")
    d_tok_grid    = os.path.join(save_root, "token_attn", "grid")
    d_sent_single = os.path.join(save_root, "sentence_attn", "single")
    d_sent_grid   = os.path.join(save_root, "sentence_attn", "grid")
    for d in [d_conf, d_unc, d_tok_single, d_tok_grid, d_sent_single, d_sent_grid]:
        _ensure_dir(d)

    tag = tag or uuid.uuid4().hex[:8]
    b = batch_index
    S = int(input_ids.size(1))
    K = S  # FULL length; discard any kept-token logic

    # find </think> occurrences
    think_positions: List[int] = _find_marker_positions(
        input_ids[b], think_id=think_id, think_token_seq=think_token_seq
    )

    # confidence (full) â€” smooth outliers then plot
    interp_outliers_inplace_token_probs(token_probs, mask)
    probs_np = token_probs[b, :K].detach().float().cpu().numpy()
    mask_np  = mask[b, :K].detach().float().cpu().numpy()
    _plot_conf_full(
        probs=probs_np,
        mask=mask_np,
        think_positions=think_positions,
        path=os.path.join(d_conf, f"stop_conf_{tag}.png"),
    )

    # ---------- NEW: uncertainty (full sequence) ----------
    # If an uncertainty tensor is provided (e.g., entropy), plot that.
    # Otherwise, use surprisal: u = -log p_gold.
    if per_token_uncertainty is not None:
        U = per_token_uncertainty[b, :K].detach().float().cpu().numpy()
        unc_title = "Uncertainty (provided)"
    else:
        eps = 1e-8
        U = np.log(np.clip(probs_np, eps, 1.0))
        unc_title = "Uncertainty = -log p_gold"

    _plot_uncertainty_full(
        unc=U,
        mask=mask_np,
        think_positions=think_positions,
        path=os.path.join(d_unc, f"stop_unc_{tag}.png"),
        title=unc_title
    )

    if plot_uncertainty_delta:
        # simple first difference to visualize "change" in uncertainty
        dU = np.diff(U, prepend=U[:1])
        _plot_uncertainty_full(
            unc=dU,
            mask=mask_np,
            think_positions=think_positions,
            path=os.path.join(d_unc, f"stop_unc_delta_{tag}.png"),
            title="Î” Uncertainty (first difference)"
        )

    # token attention (full)
    A_all = _unify_attentions(attn_stack)  # (B,L,S,S) or None
    if A_all is not None and A_all.size(1) > 0:
        A_all_b = torch.nan_to_num(A_all[b].float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0)
        A_layers_full = A_all_b.cpu().numpy()     # (L,S,S)
        A_last_full   = A_layers_full[-1]         # (S,S)

        _plot_token_single_full(
            M=A_last_full,
            think_positions=think_positions,
            path=os.path.join(d_tok_single, f"stop_tok_attn_{tag}.png"),
        )

        _plot_token_grid_full(
            A_layers=A_layers_full,
            think_positions=think_positions,
            path=os.path.join(d_tok_grid, f"stop_tok_attn_grid_{tag}.png"),
            max_layers=max_layers_grid,
        )

        # sentence attention (full)
        A_sent, seg, Ns = _token_to_sentence_attention(A_all[b:b+1], input_ids[b:b+1], sep_id)
        A_sent_np = A_sent[0].float().clamp_min(0).cpu().numpy()

        s_mark = None
        if think_positions:
            last_pos = think_positions[-1]
            s_mark = int(seg[0, last_pos].item())

        _plot_sentence_single(
            Ms=A_sent_np[-1],
            s_star=s_mark,
            path=os.path.join(d_sent_single, f"stop_sent_attn_{tag}.png"),
        )
        _plot_sentence_grid(
            M_layers=A_sent_np,
            s_star=s_mark,
            path=os.path.join(d_sent_grid, f"stop_sent_attn_grid_{tag}.png"),
            max_layers=max_layers_grid,
        )

#**********************************************************************************************************************
#Plot attn maps
import os, uuid
import numpy as np
import torch
import torch.nn.functional as F
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import colors
from typing import Optional, Sequence, List, Tuple


# =========================
# utils
# =========================
def _ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)

def _unique_path(path: str) -> str:
    """Return path if free; otherwise append _001, _002, ... before extension."""
    if not os.path.exists(path):
        return path
    base, ext = os.path.splitext(path)
    i = 1
    while True:
        cand = f"{base}_{i:03d}{ext}"
        if not os.path.exists(cand):
            return cand
        i += 1

@torch.no_grad()
def _unify_attentions(attn_stack):
    """
    attn_stack: list[L] of (B,H,S,S) or (B,H,1,S).
    â†’ returns (B,L,S,S) with heads averaged and 'q' expanded to S (last query row if q=1).
    """
    if attn_stack is None or len(attn_stack) == 0:
        return None
    layers = []
    for A in attn_stack:
        if A is None or A.dim() != 4:
            continue
        A = A.float()                   # (B,H,q,S)
        B, H, q, S = A.shape
        A = A.mean(dim=1)               # (B,q,S)
        if q == 1:
            sq = A.new_zeros(B, S, S)
            sq[:, -1, :] = A[:, 0, :]
            A_sq = sq
        elif q == S:
            A_sq = A
        else:
            if q < S:
                A = F.pad(A, (0, 0, S - q, 0))  # pad rows at top
            A_sq = A[:, :S, :S]
        layers.append(A_sq)              # (B,S,S)
    if not layers:
        return None
    return torch.stack(layers, dim=1)   # (B,L,S,S)

def _robust_limits(M: np.ndarray, lo: float = 1.0, hi: float = 99.0) -> Tuple[float, float]:
    v = np.asarray(M)
    v = v[np.isfinite(v)]
    if v.size == 0:
        return 0.0, 1.0
    vmin = float(np.percentile(v, lo))
    vmax = float(np.percentile(v, hi))
    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:
        vmin = float(v.min()) if v.size else 0.0
        vmax = float(v.max()) if v.size else 1.0
    if vmin == vmax:
        vmax = vmin + 1e-6
    return vmin, vmax

@torch.no_grad()
def _find_marker_positions(ids_row: torch.Tensor,
                           think_id: Optional[int] = None,
                           think_token_seq: Optional[Sequence[int]] = None) -> List[int]:
    """
    Return start positions where the marker occurs in input_ids row.
    - If think_token_seq is provided, match that subsequence.
    - Else use single-token think_id.
    """
    ids = ids_row.tolist()
    pos: List[int] = []
    if think_token_seq and len(think_token_seq) > 0:
        pat = list(think_token_seq)
        m, n = len(pat), len(ids)
        for i in range(0, n - m + 1):
            if ids[i:i+m] == pat:
                pos.append(i)
        return pos
    if think_id is not None:
        for i, t in enumerate(ids):
            if t == think_id:
                pos.append(i)
    return pos

def _draw_think_marks(ax, positions: Sequence[int], S: int):
    for t in positions or []:
        if 0 <= t < S:
            ax.axvline(t, ls="--", lw=0.8, color="w")
            ax.axhline(t, ls="--", lw=0.8, color="w")
            ax.scatter([t], [t], s=8, c="red", marker="o")

def _project_marks_to_size(positions: Sequence[int], S_src: int, S_tgt: int) -> List[int]:
    if S_src <= 0 or S_tgt <= 0:
        return []
    out = []
    for p in positions or []:
        q = int(np.floor(p * S_tgt / max(1, S_src)))
        if 0 <= q < S_tgt:
            out.append(q)
    return sorted(set(out))

def _save_grid(fig, path, dpi=180) -> str:
    fig.tight_layout()
    path = _unique_path(path)
    fig.savefig(path, dpi=dpi)
    plt.close(fig)
    print(f"[attn_reductions] saved: {path}")
    return path

def _save_line(fig, path, dpi=200) -> str:
    fig.tight_layout()
    path = _unique_path(path)
    fig.savefig(path, dpi=dpi)
    plt.close(fig)
    print(f"[attn_reductions] saved: {path}")
    return path


# =========================
# reductions (CPU-safe)
# =========================
def _strided_pool_maps(A_last: torch.Tensor, sizes=(8,16,32)) -> List[torch.Tensor]:
    # A_last: (B,S,S) on CPU
    assert A_last.device.type == "cpu"
    X = A_last.unsqueeze(1)  # (B,1,S,S)
    out = []
    B, S, _ = A_last.shape
    for k in sizes:
        stride = max(1, S // k)
        out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
    return out

def _gauss_blur_pool_maps(A_last: torch.Tensor, sizes=(8,16,32), sigma=1.0) -> List[torch.Tensor]:
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    X = A_last.unsqueeze(1)  # (B,1,S,S)
    rad = int(3*sigma + 0.5)
    t = torch.arange(-rad, rad+1, dtype=A_last.dtype)
    g = torch.exp(-(t**2)/(2*sigma**2)); g = g / g.sum()
    k2 = (g[:,None] @ g[None,:]).unsqueeze(0).unsqueeze(0)  # (1,1,K,K)
    X = F.conv2d(X, k2, padding=rad)
    out = []
    for k in sizes:
        stride = max(1, S // k)
        out.append(F.avg_pool2d(X, stride, stride, ceil_mode=True).squeeze(1))
    return out

def _distance_binned_profile(A_last: torch.Tensor, bins=64) -> torch.Tensor:
    # (B,S,S) â†’ (B,bins), mean attention per |i-j| bin (CPU)
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    dists = torch.arange(S, dtype=torch.long)
    D = (dists[None,:] - dists[:,None]).abs().unsqueeze(0)  # (1,S,S)
    A_use = A_last
    edges = torch.linspace(0, S-1, bins+1)
    idx = torch.bucketize(D, edges) - 1                     # (1,S,S) in [0,bins-1]
    profiles = []
    for b in range(bins):
        m = (idx == b)
        num = (A_use * m).sum(dim=(1,2))
        den = m.sum(dim=(1,2)).clamp_min(1)
        profiles.append(num / den)
    return torch.stack(profiles, dim=-1)  # (B,bins)

def _topk_sparse_pool(A_last: torch.Tensor, ratios=(0.01, 0.05, 0.10), size=32) -> List[torch.Tensor]:
    # CPU top-k per row â†’ sparse â†’ pooled
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    for r in ratios:
        k = max(1, int(S * r))
        vals, idx = torch.topk(A_last, k, dim=-1)           # (B,S,k) long indices
        M = torch.zeros_like(A_last)
        M.scatter_(-1, idx.long(), 1.0)                     # 1 at top-k keys
        Sparse = A_last * M
        stride = max(1, S // size)
        out.append(F.avg_pool2d(Sparse.unsqueeze(1), stride, stride, ceil_mode=True).squeeze(1))
    return out

def _kmeans1d_assign(S: int, K: int, iters=8) -> torch.Tensor:
    # CPU 1D k-means on positions
    x = torch.linspace(0, 1, S, dtype=torch.float32).unsqueeze(-1)   # (S,1)
    centers = torch.linspace(0, 1, K, dtype=torch.float32).unsqueeze(-1)
    for _ in range(iters):
        d = (x - centers.T)**2                                       # (S,K)
        assign = d.argmin(dim=1)                                     # (S,) long
        for k in range(K):
            pts = x[assign == k]
            centers[k] = pts.mean(dim=0) if pts.numel() else centers[k]
    return assign.long()

def _cluster_pool(A_last: torch.Tensor, Ks=(16,32,64)) -> List[torch.Tensor]:
    # CPU positional clusters â†’ clusterÃ—cluster map
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    for K in Ks:
        K = min(K, S)
        assign = _kmeans1d_assign(S, K)                   # (S,) long
        H = F.one_hot(assign, K).to(A_last.dtype)         # (S,K)
        Ht = H.t().unsqueeze(0).expand(B, -1, -1)         # (B,K,S)
        Hb = H.unsqueeze(0).expand(B, -1, -1)             # (B,S,K)
        Cluster = torch.bmm(Ht, torch.bmm(A_last, Hb))    # (B,K,K)
        cnt = H.sum(dim=0).clamp_min(1.0)                 # (K,)
        Cluster = Cluster / (cnt.unsqueeze(0).unsqueeze(-1) * cnt.unsqueeze(0).unsqueeze(1))
        out.append(Cluster)
    return out

def _landmark_pool(A_last: torch.Tensor, Ms=(16,32,64)) -> List[torch.Tensor]:
    # CPU landmarks by out-degree
    assert A_last.device.type == "cpu"
    B, S, _ = A_last.shape
    out = []
    outdeg = A_last.sum(-1)                            # (B,S)
    for M in Ms:
        M = min(M, S)
        idx = torch.topk(outdeg, M, dim=-1).indices.long()   # (B,M)
        batch = torch.arange(B, dtype=torch.long)[:, None]   # (B,1)
        L = A_last[batch, idx][:, :, idx]                    # (B,M,M)
        sort_idx = idx.sort(dim=-1).indices
        L = L[batch, sort_idx][:, :, sort_idx]
        out.append(L)
    return out

def _pyramid_maps(A_last: torch.Tensor, scales=(8,16,32)) -> List[torch.Tensor]:
    # same as strided pool, just named as pyramid (multi-scale)
    return _strided_pool_maps(A_last, sizes=scales)


# =========================
# Internal/External (span-based) helpers
# =========================
def _find_internal_span(input_ids_row: torch.Tensor,
                        close_positions: List[int],
                        think_open_seq: Optional[Sequence[int]] = None,
                        internal_window: int = 128) -> Optional[Tuple[int, int]]:
    """
    Return (start, end) token indices for the internal region.
    - Prefer the last '<think>' ... '</think>' span if think_open_seq is provided.
    - Else fallback to window: [max(0, t_close - internal_window), t_close)
    Excludes the close token itself.
    """
    if not close_positions:
        return None
    t_close = close_positions[-1]  # last close
    if think_open_seq and len(think_open_seq) > 0:
        ids = input_ids_row.tolist()
        pat = list(think_open_seq)
        last_open = -1
        for i in range(0, t_close - len(pat) + 1):
            if ids[i:i+len(pat)] == pat:
                last_open = i
        if last_open >= 0 and last_open < t_close:
            return (last_open, t_close)
    s = max(0, t_close - int(internal_window))
    return (s, t_close)

def _int_ext_fractions(A: torch.Tensor, start: int, end: int) -> Tuple[float, float, float, float]:
    """
    A: (S,S) CPU attention (nonnegative). Internal = [start, end) ; External = complement.
    Returns fractions (II, IE, EI, EE) that sum to 1.
    """
    S = A.size(0)
    I = torch.zeros(S, dtype=torch.float32)
    I[start:end] = 1.0
    E = 1.0 - I
    M_II = torch.ger(I, I)
    M_IE = torch.ger(I, E)
    M_EI = torch.ger(E, I)
    M_EE = torch.ger(E, E)
    tot = A.sum().item() + 1e-12
    II = float((A * M_II).sum().item() / tot)
    IE = float((A * M_IE).sum().item() / tot)
    EI = float((A * M_EI).sum().item() / tot)
    EE = float((A * M_EE).sum().item() / tot)
    return II, IE, EI, EE

def _plot_int_ext_layers(A_layers: torch.Tensor,  # (L,S,S) CPU
                         start: int, end: int,
                         path: str):
    L, S, _ = A_layers.shape
    II_list, IE_list, EI_list = [], [], []
    for li in range(L):
        II, IE, EI, EE = _int_ext_fractions(A_layers[li], start, end)
        II_list.append(II); IE_list.append(IE); EI_list.append(EI)
    x = np.arange(L)
    fig = plt.figure(figsize=(6.5, 2.0))
    plt.plot(x, II_list, label="Iâ†’I", linewidth=1.2)
    plt.plot(x, IE_list, label="Iâ†’E", linewidth=1.0)
    plt.plot(x, EI_list, label="Eâ†’I", linewidth=1.0)
    plt.legend(loc="upper right", fontsize=7, framealpha=0.25)
    plt.title("Internal vs External attention (per layer)", fontsize=9)
    _save_line(fig, path, dpi=200)

def _plot_int_ext_last_bar(A_last: torch.Tensor,  # (S,S) CPU
                           start: int, end: int,
                           path: str):
    II, IE, EI, EE = _int_ext_fractions(A_last, start, end)
    vals = [II, IE, EI, EE]
    labels = ["Iâ†’I", "Iâ†’E", "Eâ†’I", "Eâ†’E"]
    fig = plt.figure(figsize=(4.5, 2.2))
    xpos = np.arange(len(vals))
    plt.bar(xpos, vals, width=0.6)
    for i, v in enumerate(vals):
        plt.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
    plt.title("Internal/External fractions (last layer)", fontsize=9)
    _save_line(fig, path, dpi=200)


# =========================
# NEW: Grouped Internal/External helpers
# =========================
def _group_intra_external(A_layers: torch.Tensor, K: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    A_layers: (L,S,S) CPU, nonnegative
    K: number of groups
    Returns:
      intra[L]   : sum diag(H^T A H) / sum(A)
      external[L]: 1 - intra[L]
      last_B(K,K): group block attention for last layer, normalised by total mass
    """
    L, S, _ = A_layers.shape
    K = max(1, min(int(K), S))

    # fixed positional k-means assignment (deterministic, CPU)
    assign = _kmeans1d_assign(S, K)                   # (S,)
    H = F.one_hot(assign, K).to(A_layers.dtype)       # (S,K)
    Ht = H.t()                                        # (K,S)

    intra = np.zeros(L, dtype=np.float32)
    external = np.zeros(L, dtype=np.float32)
    last_B = None

    for li in range(L):
        A = A_layers[li]                              # (S,S)
        B = Ht @ (A @ H)                              # (K,K)
        tot = float(A.sum().item() + 1e-12)
        intra_li = float(torch.trace(B).item()) / tot
        intra[li] = intra_li
        external[li] = max(0.0, 1.0 - intra_li)
        if li == L - 1:
            last_B = (B / tot).cpu().numpy()

    return intra, external, last_B

def _plot_group_intra_external_lines(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
    """
    3Ã—1 panels: for each K, plot intra vs external vs layers.
    """
    fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
    if len(Ks) == 1:
        axes = [axes]
    for ax, K in zip(axes, Ks):
        intra, ext, _ = _group_intra_external(A_layers, K)
        x = np.arange(len(intra))
        ax.plot(x, intra, label="intra", linewidth=1.2)
        ax.plot(x, ext,   label="external", linewidth=1.0)
        ax.set_title(f"group K={K}", fontsize=9)
        ax.legend(loc="upper right", fontsize=7, framealpha=0.25)
    _save_grid(fig, path, dpi=200)

def _plot_group_intra_external_bars(A_layers: torch.Tensor, path: str, Ks=(8,16,32)):
    """
    3Ã—1 panels: for each K, bar of last layer intra vs external.
    """
    L, S, _ = A_layers.shape
    fig, axes = plt.subplots(1, len(Ks), figsize=(2.6*len(Ks), 2.0))
    if len(Ks) == 1:
        axes = [axes]
    for ax, K in zip(axes, Ks):
        intra, ext, _ = _group_intra_external(A_layers, K)
        vals = [float(intra[-1]), float(ext[-1])]
        xpos = np.arange(2)
        ax.bar(xpos, vals, width=0.6)
        ax.set_title(f"last layer (K={K})", fontsize=9)
        for i, v in enumerate(vals):
            ax.text(i, v + 0.01, f"{v:.2f}", ha="center", va="bottom", fontsize=7)
    _save_grid(fig, path, dpi=200)


# =========================
# plotting helpers
# =========================
def _imshow_small(ax, M: np.ndarray, title: str, think_pos: Sequence[int]):
    vmin, vmax = _robust_limits(M)
    ax.imshow(M, interpolation="nearest", aspect="auto",
              norm=colors.PowerNorm(gamma=0.6, vmin=vmin, vmax=vmax))
    S = M.shape[0]
    _draw_think_marks(ax, think_pos, S)
    ax.set_title(title, fontsize=8)

def _imshow_small_project(ax, M: np.ndarray, title: str,
                          think_pos_tokens: Sequence[int], S_src_tokens: int):
    # Project token indices into this mapâ€™s axis length
    S_tgt = M.shape[0]
    marks = _project_marks_to_size(think_pos_tokens, S_src_tokens, S_tgt)
    _imshow_small(ax, M, title, marks)

def _plot_line_profile(vec: np.ndarray, path: str, title="Distance-binned attention"):
    fig = plt.figure(figsize=(5, 1.6))
    x = np.arange(vec.shape[-1])
    plt.plot(x, vec, linewidth=1.0)
    plt.title(title, fontsize=9)
    _save_line(fig, path, dpi=200)


# =========================
# main API
# =========================
@torch.no_grad()
def plot_all_token_attn_reductions(
    input_ids: torch.Tensor,     # (B,S)
    attn_stack,                  # list[L] of (B,H,S,S) or (B,H,1,S)
    save_root: str,
    think_id: Optional[int] = 151668,
    think_token_seq: Optional[Sequence[int]] = None,
    think_open_seq: Optional[Sequence[int]] = None,   # optional "<think>" opener
    internal_window: int = 128,                        # fallback window
    batch_index: int = 0,
    tag: Optional[str] = None,
):
    """
    Writes PNGs under {save_root}/attn_reductions/<method>/...  (small figs, 3Ã—1 where applicable).
    Shows </think> position(s) in all map-based plots (projected where needed). Unique filenames.
    Also plots Internal vs External (span-based) and Grouped Intra/External (Kâˆˆ{8,16,32}).
    """
    _ensure_dir(save_root)
    base = os.path.join(save_root, "attn_reductions")
    subdirs = {
        "token_full_single": os.path.join(base, "token_full", "single"),
        "token_full_last3":  os.path.join(base, "token_full", "last3"),
        "strided_pool":      os.path.join(base, "strided_pool"),
        "gauss_pool":        os.path.join(base, "gauss_pool"),
        "dist_binned":       os.path.join(base, "dist_binned"),
        "topk_pool":         os.path.join(base, "topk_pool"),
        "cluster_pool":      os.path.join(base, "cluster_pool"),
        "landmarks":         os.path.join(base, "landmarks"),
        "pyramid":           os.path.join(base, "pyramid"),
        "int_ext_ratio":     os.path.join(base, "int_ext_ratio"),
        "group_int_ext":     os.path.join(base, "group_int_ext"),
    }
    for d in subdirs.values():
        _ensure_dir(d)

    tag = tag or uuid.uuid4().hex[:8]
    b = batch_index

    # ---- unify attentions and select last layer ----
    A_all = _unify_attentions(attn_stack)  # (B,L,S,S)
    if A_all is None or A_all.size(1) == 0:
        print("[attn_reductions] no attention to plot")
        return

    # Move to CPU once; sanitize; keep shapes
    A_all_cpu = torch.nan_to_num(A_all.detach().float(), nan=0.0, posinf=0.0, neginf=0.0).clamp_min(0).cpu()
    A_last = A_all_cpu[b, -1].unsqueeze(0)  # (1,S,S) CPU
    B_, S, _ = A_last.shape

    # find all </think> positions (token indices)
    think_pos_tokens = _find_marker_positions(input_ids[b], think_id=think_id, think_token_seq=think_token_seq)

    print(f"[attn_reductions] S={S}, Layers={int(A_all_cpu.shape[1])}, think_positions={think_pos_tokens}")

    # === Normal token attention (comparison) ===
    try:
        print("[attn_reductions] plotting: token_full_single")
        M = A_last[0].numpy()
        fig, ax = plt.subplots(1, 1, figsize=(4, 4))
        _imshow_small(ax, M, "Token attention (last layer, full)", think_pos_tokens)
        _save_grid(fig, os.path.join(subdirs["token_full_single"], f"token_full_single_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:token_full_single] skipped: {e}")

    try:
        print("[attn_reductions] plotting: token_full_last3")
        L = int(A_all_cpu.shape[1])
        idxs = list(range(max(0, L - 3), L))
        panels = len(idxs)
        fig, axes = plt.subplots(1, panels, figsize=(2.6 * panels, 2.2))
        if panels == 1:
            axes = [axes]
        for ax, li in zip(axes, idxs):
            M = A_all_cpu[b, li].numpy()
            _imshow_small(ax, M, f"Layer {li}", think_pos_tokens)
        _save_grid(fig, os.path.join(subdirs["token_full_last3"], f"token_full_last3_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:token_full_last3] skipped: {e}")

    # === Reductions (3Ã—1 each where applicable) ===
    try:
        print("[attn_reductions] plotting: strided_pool (8,16,32)")
        maps = _strided_pool_maps(A_last, sizes=(8,16,32))
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"avg k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["strided_pool"], f"strided_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:strided_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: gauss_pool (8,16,32)")
        maps = _gauss_blur_pool_maps(A_last, sizes=(8,16,32), sigma=1.0)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"gauss k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["gauss_pool"], f"gauss_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:gauss_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: dist_binned (bins=64)")
        prof = _distance_binned_profile(A_last, bins=64)[0].numpy()
        fig = plt.figure(figsize=(5, 1.6))
        x = np.arange(prof.shape[-1])
        plt.plot(x, prof, linewidth=1.0)
        plt.title("Distance-binned attention", fontsize=9)
        _save_line(fig, os.path.join(subdirs["dist_binned"], f"dist_binned_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:dist_binned] skipped: {e}")

    try:
        print("[attn_reductions] plotting: topk_pool (1%,5%,10%)")
        maps = _topk_sparse_pool(A_last, ratios=(0.01, 0.05, 0.10), size=32)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, label in zip(axes, maps, ("top-1%", "top-5%", "top-10%")):
            M = m[0].numpy()
            _imshow_small_project(ax, M, label, think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["topk_pool"], f"topk_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:topk_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: cluster_pool (K=16,32,64)")
        Ks = (16,32,64)
        maps = _cluster_pool(A_last, Ks=Ks)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, K in zip(axes, maps, Ks):
            assign = _kmeans1d_assign(S, K)
            marks = [int(assign[p].item()) for p in think_pos_tokens if 0 <= p < S]
            _imshow_small(ax, m[0].numpy(), f"cluster K={K}", marks)
        _save_grid(fig, os.path.join(subdirs["cluster_pool"], f"cluster_pool_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:cluster_pool] skipped: {e}")

    try:
        print("[attn_reductions] plotting: landmarks (M=16,32,64)")
        Ms_ = (16,32,64)
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        outdeg = A_last.sum(-1)[0]  # (S,)
        for ax, Msize in zip(axes, Ms_):
            Msize = min(Msize, S)
            idx = torch.topk(outdeg, Msize, dim=-1).indices.long()
            idx_sorted, order = idx.sort()
            pos_map = {int(tok.item()): int(i) for i, tok in enumerate(idx_sorted)}
            marks = [pos_map[p] for p in think_pos_tokens if p in pos_map]
            Lm = A_last[0][idx][:, idx]
            Lm = Lm[order][:, order]
            _imshow_small(ax, Lm.numpy(), f"landmarks M={Msize}", marks)
        _save_grid(fig, os.path.join(subdirs["landmarks"], f"landmarks_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:landmarks] skipped: {e}")

    try:
        print("[attn_reductions] plotting: pyramid (8,16,32)")
        maps = _pyramid_maps(A_last, scales=(8,16,32))
        fig, axes = plt.subplots(1, 3, figsize=(7.8, 2.2))
        for ax, m, k in zip(axes, maps, (8,16,32)):
            M = m[0].numpy()
            _imshow_small_project(ax, M, f"pyr k={k}", think_pos_tokens, S)
        _save_grid(fig, os.path.join(subdirs["pyramid"], f"pyramid_{tag}.png"))
    except Exception as e:
        print(f"[attn_reductions:pyramid] skipped: {e}")

    # === Span-based Internal vs External ===
    try:
        print("[attn_reductions] computing: internal/external ratios (span)")
        close_positions = think_pos_tokens
        span = _find_internal_span(input_ids[b], close_positions,
                                   think_open_seq=think_open_seq,
                                   internal_window=internal_window)
        if span is None:
            print("[attn_reductions:int_ext_ratio] skipped (no </think> found)")
        else:
            s_int, e_int = span
            print(f"[attn_reductions:int_ext_ratio] internal span: [{s_int}, {e_int})")
            print("[attn_reductions] plotting: int_ext_ratio per-layer line")
            _plot_int_ext_layers(
                A_layers=A_all_cpu[b],
                start=s_int, end=e_int,
                path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_layers_{tag}.png")
            )
            print("[attn_reductions] plotting: int_ext_ratio last-layer bar")
            _plot_int_ext_last_bar(
                A_last=A_last[0],
                start=s_int, end=e_int,
                path=os.path.join(subdirs["int_ext_ratio"], f"int_ext_last_{tag}.png")
            )
    except Exception as e:
        print(f"[attn_reductions:int_ext_ratio] skipped: {e}")

    # === Grouped Internal vs External ===
    try:
        print("[attn_reductions] plotting: group_int_ext lines (K=8,16,32)")
        _plot_group_intra_external_lines(
            A_layers=A_all_cpu[b],
            path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_lines_{tag}.png"),
            Ks=(8,16,32),
        )
    except Exception as e:
        print(f"[attn_reductions:group_int_ext_lines] skipped: {e}")

    try:
        print("[attn_reductions] plotting: group_int_ext bars (K=8,16,32)")
        _plot_group_intra_external_bars(
            A_layers=A_all_cpu[b],
            path=os.path.join(subdirs["group_int_ext"], f"group_int_ext_bars_{tag}.png"),
            Ks=(8,16,32),
        )
    except Exception as e:
        print(f"[attn_reductions:group_int_ext_bars] skipped: {e}")


#**********************************************************************************************************************



#myedit
# ---- utils ----
def lower_tri_mask(N, device):
    return torch.tril(torch.ones(N, N, device=device, dtype=torch.bool))

def conv1d_fp32(x, w, b=None, dilation=1, causal_pad_left=0, groups=1, stride=1):
    x32 = F.pad(x, (causal_pad_left, 0)).float() if causal_pad_left > 0 else x.float()
    return F.conv1d(x32, w.float(), None if b is None else b.float(),
                    stride=stride, padding=0, dilation=dilation, groups=groups)

def conv2d_fp32(x, conv: nn.Conv2d):
    return F.conv2d(
        x.float(),
        conv.weight.float(),
        None if conv.bias is None else conv.bias.float(),
        stride=conv.stride, padding=conv.padding, dilation=conv.dilation, groups=conv.groups
    )

# ---- stable causal TCN (1D) ----
class _CausalConv1d(nn.Module):
    def __init__(self, c_in, c_out, kernel_size=3, dilation=1, bias=True):
        super().__init__()
        self.ks, self.dil = kernel_size, dilation
        self.weight = nn.Parameter(torch.empty(c_out, c_in, kernel_size))
        self.bias   = nn.Parameter(torch.zeros(c_out)) if bias else None
        nn.init.kaiming_normal_(self.weight, nonlinearity="linear")

    def forward(self, x):  # (B,C,N)
        pad_left = (self.ks - 1) * self.dil
        return conv1d_fp32(x, self.weight, self.bias, dilation=self.dil, causal_pad_left=pad_left)  # fp32

class _Pointwise1d(nn.Module):
    def __init__(self, c_in, c_out, bias=True):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(c_out, c_in, 1))
        self.bias   = nn.Parameter(torch.zeros(c_out)) if bias else None
        nn.init.kaiming_normal_(self.weight, nonlinearity="linear")

    def forward(self, x):  # (B,C,N) fp32
        return conv1d_fp32(x, self.weight, self.bias)

class CausalTCN1D(nn.Module):
    """causal, fp32 math inside, returns to input dtype; residual + RMSNorm to avoid NaNs in bf16."""
    def __init__(self, c_in, d, layers=3, kernel_size=3, dropout=0.0):
        super().__init__()
        self.blocks = nn.ModuleList()
        cin = c_in
        for i in range(layers):
            dil = 2 ** i
            self.blocks.append(nn.ModuleDict({
                "causal": _CausalConv1d(cin, d, kernel_size=kernel_size, dilation=dil, bias=True),
                "act1":  nn.GELU(),
                "pw":    _Pointwise1d(d, d, bias=True),
                "act2":  nn.GELU(),
                "drop":  nn.Dropout(dropout),
                "rms":   nn.RMSNorm(d, eps=1e-5),
                "res":   _Pointwise1d(cin, d, bias=False) if cin != d else nn.Identity(),
            }))
            cin = d
        self.res_scale = 0.5
        self.main_scale = 0.5

    def forward(self, x):  # (B*, C, N)
        in_dtype = x.dtype
        h = x
        for blk in self.blocks:
            y = blk["causal"](h)         # fp32
            y = blk["act1"](y)           # fp32
            y = blk["pw"](y)             # fp32
            y = blk["act2"](y)           # fp32
            y = blk["drop"](y)           # fp32
            y = blk["rms"](y.transpose(1, 2)).transpose(1, 2)  # fp32, norm over channels
            res = h if isinstance(blk["res"], nn.Identity) else blk["res"](h)  # fp32
            h = self.main_scale * y + self.res_scale * res                      # fp32
            h = torch.clamp(torch.nan_to_num(h, nan=0.0), -1e3, 1e3)            # safety
        return h.to(in_dtype)

# ---- depthwise 2D (fp32 convs inside) ----
class Depthwise2D(nn.Module):
    def __init__(self, c_in, d, blocks=2, ks=3):
        super().__init__()
        blks = []
        cin = c_in
        for _ in range(blocks):
            blks += [nn.Conv2d(cin, cin, ks, padding=ks//2, groups=cin), nn.GELU(),
                     nn.Conv2d(cin, d, 1), nn.GELU()]
            blks += [nn.Conv2d(d, d, (1,7), padding=(0,3)), nn.GELU(),
                     nn.Conv2d(d, d, (7,1), padding=(3,0)), nn.GELU()]
            cin = d
        self.net = nn.Sequential(*blks)
        self.out_dim = d

    def forward(self, x):  # (B,C,N,N)
        dt = x.dtype
        y = x
        for m in self.net:
            if isinstance(m, nn.Conv2d):
                y = conv2d_fp32(y, m)    # fp32 conv with fp32 weights/bias
            else:
                y = m(y.float())         # activations in fp32
        return y.to(dt)

# ---- super simple causal encoder ----
class SimpleCausalBlock(nn.Module):
    def __init__(self, d_model, nhead, dim_ff=2048, dropout=0.0):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True, dropout=dropout)
        self.ff   = nn.Sequential(nn.Linear(d_model, dim_ff), nn.GELU(),
                                  nn.Linear(dim_ff, d_model), nn.Dropout(dropout))
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):  # (B,N,D)
        x = x.to(next(self.attn.parameters()).dtype)
        N = x.size(1)
        mask = torch.triu(torch.ones((N, N), device=x.device, dtype=torch.bool), 1)
        y, _ = self.attn(x, x, x, attn_mask=mask, need_weights=False)
        x = self.norm1(x + y)
        x = self.norm2(x + self.ff(x))
        return x

class SimpleCausalEncoder(nn.Module):
    def __init__(self, d_model, nhead=8, num_layers=2, dim_ff=1024, dropout=0.0):
        super().__init__()
        self.layers = nn.ModuleList([SimpleCausalBlock(d_model, nhead, dim_ff, dropout) for _ in range(num_layers)])
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# ---- the embedder ----
class AttnFeatureEmbedder(nn.Module):
    def __init__(self, c_in, d_model=256, d_emb=256, bands=(0,1,2,4,8), tcn_layers=3, use_2d=True):
        super().__init__()
        self.use_2d = use_2d
        self.bands  = bands

        self.tcn_row = CausalTCN1D(c_in, d_model, layers=tcn_layers)
        self.tcn_col = CausalTCN1D(c_in, d_model, layers=tcn_layers)

        if use_2d:
            self.cnn2d = Depthwise2D(c_in, d_model // 2, blocks=2)
            d2, n_band = self.cnn2d.out_dim, len(bands)
        else:
            d2, n_band = 0, 0

        self.q_row = nn.Parameter(torch.randn(d_model))
        self.q_col = nn.Parameter(torch.randn(d_model))

        n_stats = 7
        fuse_in = (2 * d_model) + d2 + n_stats + n_band
        self.fuse = nn.Sequential(nn.LayerNorm(fuse_in), nn.Linear(fuse_in, d_emb))
        self.causal_enc = SimpleCausalEncoder(d_model=d_emb, nhead=8, num_layers=3)

    @staticmethod
    def _stats(A):  # A: (B,N,N), nonneg
        Af = A.float()
        Af = Af / (Af.sum(-1, keepdim=True) + 1e-8)
        eps = 1e-6
        B, N, _ = Af.shape
        H    = - (Af.clamp_min(eps) * Af.clamp_min(eps).log()).sum(-1)
        topk = Af.topk(k=min(4, N), dim=-1).values.sum(-1)
        outd = Af.sum(-1); ind = Af.sum(-2)
        asym = (Af - Af.transpose(-1,-2)).abs().sum(-1)
        j = torch.arange(N, device=A.device, dtype=Af.dtype)
        dist = (j[None,None,:] - j[None,:,None]).abs()
        mu  = (Af * dist).sum(-1)
        var = (Af * (dist - mu.unsqueeze(-1))**2).sum(-1)
        return [H, topk, outd, ind, asym, mu, var]

    def forward(self, X):  # X: (B,C,N,N)
        B, C, N, _ = X.shape
        device = X.device

        # causal mask and masked attention map
        m  = lower_tri_mask(N, device)
        Xc = X.masked_fill(~m[None, None], 0.0)

        # rows: (B,N_rows,C,N_cols) -> (B*N_rows,C,N_cols)
        rows = Xc.permute(0, 2, 1, 3).reshape(B * N, C, N)
        row_feats = self.tcn_row(rows).transpose(1, 2).reshape(B, N, N, -1)
        q_row = self.q_row.to(row_feats.dtype)[None, None, None, :]
        logits = (row_feats * q_row).sum(-1)  # (B,N,N)

        # cols: swap N,N then same
        cols = Xc.transpose(-1, -2).permute(0, 2, 1, 3).reshape(B * N, C, N)
        col_feats = self.tcn_col(cols).transpose(1, 2).reshape(B, N, N, -1)
        q_col = self.q_col.to(col_feats.dtype)[None, None, None, :]
        logits_c = (col_feats * q_col).sum(-1)

        # stable softmax with additive -inf (fp32)
        mask_inf = torch.triu(torch.full((N, N), float('-inf'), device=device, dtype=torch.float32), 1)
        w  = torch.softmax(logits.float()   + mask_inf, dim=-1).to(row_feats.dtype)
        wc = torch.softmax(logits_c.float() + mask_inf, dim=-1).to(col_feats.dtype)

        r = (row_feats * w[...,  None]).sum(2)  # (B,N,d_model)
        c = (col_feats * wc[..., None]).sum(2)  # (B,N,d_model)

        parts = [r, c]
        band_feats = []

        if self.use_2d:
            U = self.cnn2d(Xc)                               # (B,d2,N,N) safe fp32 inside
            u_row = U.mean(-1)                               # (B,d2,N)
            u_col = U.mean(-2)                               # (B,d2,N)
            Utok  = (u_row.transpose(1, 2) + u_col.transpose(1, 2)) * 0.5  # (B,N,d2)
            parts.append(Utok)

            for k in self.bands:
                L = max(N - k, 0)
                diag = torch.diagonal(U, offset=-k, dim1=-2, dim2=-1).contiguous()  # (B,d2,L)
                if N - L > 0:
                    diag = F.pad(diag, (0, N - L))                                   # (B,d2,N)
                band_feats.append(diag.mean(1))                                      # (B,N)

        # graph/stat feats
        A = (Xc.sum(1) + 1e-8)                # (B,N,N)
        S = [s.to(Xc.dtype) for s in self._stats(A)]  # 7*(B,N)

        # fuse
        feats   = torch.cat(parts, dim=-1)                   # (B,N, 2*d_model (+ d2))
        scalars = torch.stack(S, dim=-1)                     # (B,N,7)
        if band_feats:
            band_stack = torch.stack(band_feats, dim=-1)     # (B,N,|bands|)
            alltok = torch.cat([feats, scalars, band_stack], dim=-1)
        else:
            alltok = torch.cat([feats, scalars], dim=-1)

        z = self.fuse(alltok.to(next(self.fuse.parameters()).dtype))  # (B,N,d_emb)
        z = self.causal_enc(z.to(next(self.causal_enc.parameters()).dtype))
        return z


class ConfidenceCurveEncoder(nn.Module):
    """
    Turns scalar token-prob curves (B, S) into causal features (B, S, D).
    Receptive field grows exponentially with layer depth.
    """
    def __init__(
        self,
        out_dim: int,
        hidden: int = 64,
        n_layers: int = 4,
        kernel_size: int = 5,          # 3 works well; k must be odd and >= 2
    ):
        super().__init__()

        layers = []
        in_ch = 1
        dilation = 1
        for _ in range(n_layers):
            # causal padding: pad only on the **left**
            pad = (kernel_size - 1) * dilation
            layers.append(nn.ConstantPad1d((pad, 0), 0.0))   # (left,right)
            layers.append(
                nn.Conv1d(
                    in_channels=in_ch,
                    out_channels=hidden,
                    kernel_size=kernel_size,
                    dilation=dilation,
                    bias=False,
                )
            )
            layers.append(nn.GELU())
            in_ch = hidden
            dilation *= 2           # double dilation â†’ exponential RF

        # final 1Ã—1 projection to out_dim
        layers.append(nn.Conv1d(in_ch, out_dim, kernel_size=1, bias=False))
        self.net = nn.Sequential(*layers)

        # register output dtype for convenient casting later
        self.out_dtype = torch.get_default_dtype()

    def forward(self, probs: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:
        """
        probs : (B, S)  â€“ token probabilities (any float dtype)
        mask  : (B, S)  â€“ 1 for real tokens, 0 for padding
        returns (B, S, out_dim)
        """
        x = probs.unsqueeze(1)          # (B, 1, S)
        x = self.net(x)                # (B, out_dim, S)
        x = x.transpose(1, 2)          # (B, S, out_dim)
        return x * mask.unsqueeze(-1)  # keep padding at zero



def gauss_blur_adaptive_pool(
    A_tok: torch.Tensor,   # (B, C, S, S)
    size: int,             # target k (e.g., 16/32/64/â€¦)
    sigma: float = 1.0
) -> torch.Tensor:
    """
    Depthwise Gaussian blur (per-channel) + adaptive avg pool to (size,size).
    Preserves channels. Works on CPU/GPU and remains differentiable.
    Returns: (B, C, size, size)
    """
    assert A_tok.dim() == 4, "A_tok must be (B,C,S,S)"
    B, C, S, T = A_tok.shape
    assert S == T, "attention maps must be square"

    # sanitize size
    k = int(size)
    if k <= 0:
        raise ValueError(f"`size` must be > 0, got {size}")

    device, dtype = A_tok.device, A_tok.dtype

    # build separable Gaussian kernel
    if sigma <= 0:
        k2 = torch.ones((1, 1, 1, 1), device=device, dtype=dtype)
        pad = 0
    else:
        rad = int(3 * sigma + 0.5)
        t = torch.arange(-rad, rad + 1, device=device, dtype=dtype)
        g = torch.exp(-(t * t) / (2 * sigma * sigma))
        g = g / (g.sum() + 1e-12)
        k2 = (g[:, None] @ g[None, :]).unsqueeze(0).unsqueeze(0)  # (1,1,K,K)
        pad = k2.size(-1) // 2

    # depthwise blur (no channel mixing)
    weight = k2.expand(C, 1, k2.size(-2), k2.size(-1)).contiguous()
    X = F.conv2d(A_tok, weight=weight, bias=None, stride=1, padding=pad, groups=C)

    # exact kÃ—k
    return F.adaptive_avg_pool2d(X, (k, k))  # (B,C,k,k)

#*****************************************************************************************


logger = logging.get_logger(__name__)

_CHECKPOINT_FOR_DOC = "Qwen/Qwen3-8B"
_CONFIG_FOR_DOC = "Qwen3Config"


@dataclass
class CausalLMOutputWithPast(ModelOutput):
    """
    Outputs for causal language modeling with an optional stop-probability head.

    Args:
        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):
            Training loss (e.g., LM loss or stop-head loss).
        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, vocab_size)`):
            Pre-softmax scores from the LM head.
        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*):
            A nested tuple with length `config.num_hidden_layers`. Each inner tuple contains
            key/value tensors of shape `(batch_size, num_heads, seq_len, head_dim)`.
            Can be fed to speed up sequential decoding.
        hidden_states (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of hidden states (output of embeddings + each layer), each of shape
            `(batch_size, sequence_length, hidden_size)`.
        attentions (`tuple(torch.FloatTensor)`, *optional*):
            Tuple of attention maps for each layer, each of shape
            `(batch_size, num_heads, sequence_length, sequence_length)`.
        stop_prob (`torch.FloatTensor` of shape `(batch_size, sequence_length)` in train
            or `(batch_size, 1, 1)` at inference, *optional*):
            Probability from the stop head indicating that thinking should end at each position.
    """
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor, ...], ...]] = None
    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None
    stop_prob: Optional[torch.FloatTensor] = None


def assert_finite(**named_tensors):
    """Raise if any tensor contains NaN/Inf."""
    for k, v in named_tensors.items():
        if not torch.all(torch.isfinite(v)):
            raise FloatingPointError(f"{k} has NaN/Inf")




@use_kernel_forward_from_hub("RMSNorm")
class Qwen3RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen3RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen3MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        return down_proj


def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`, *optional*):
            Deprecated and unused.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs: Unpack[TransformersKwargs],
):
    key_states = repeat_kv(key, module.num_key_value_groups)
    value_states = repeat_kv(value, module.num_key_value_groups)

    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
    if attention_mask is not None:
        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        attn_weights = attn_weights + causal_mask

    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)
    attn_output = torch.matmul(attn_weights, value_states)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class Qwen3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.head_dim = getattr(config, "head_dim", config.hidden_size // config.num_attention_heads)
        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads
        self.scaling = self.head_dim**-0.5
        self.attention_dropout = config.attention_dropout
        self.is_causal = True

        self.q_proj = nn.Linear(
            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias
        )
        self.k_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.v_proj = nn.Linear(
            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias
        )
        self.o_proj = nn.Linear(
            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias
        )
        self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!
        self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape
        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == "sliding_attention" else None

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        position_embeddings: tuple[torch.Tensor, torch.Tensor],
        attention_mask: Optional[torch.Tensor],
        past_key_values: Optional[Cache] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[FlashAttentionKwargs],
    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:
        input_shape = hidden_states.shape[:-1]
        hidden_shape = (*input_shape, -1, self.head_dim)

        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)

        cos, sin = position_embeddings
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)

        if past_key_values is not None:
            # sin and cos are specific to RoPE models; cache_position needed for the static cache
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}
            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        attn_output, attn_weights = attention_interface(
            self,
            query_states,
            key_states,
            value_states,
            attention_mask,
            dropout=0.0 if not self.training else self.attention_dropout,
            scaling=self.scaling,
            sliding_window=self.sliding_window,  # diff with Llama
            **kwargs,
        )

        attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        attn_output = self.o_proj(attn_output)
        return attn_output, attn_weights


class Qwen3DecoderLayer(GradientCheckpointingLayer):
    def __init__(self, config: Qwen3Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)

        self.mlp = Qwen3MLP(config)
        self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.attention_type = config.layer_types[layer_idx]

    @deprecate_kwarg("past_key_value", new_name="past_key_values", version="4.58")
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        use_cache: Optional[bool] = False,
        cache_position: Optional[torch.LongTensor] = None,
        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC
        **kwargs: Unpack[TransformersKwargs],
    ) -> torch.Tensor:
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        # Self Attention
        hidden_states, _ = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=position_embeddings,
            **kwargs,
        )
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


@auto_docstring
class Qwen3PreTrainedModel(PreTrainedModel):
    config: Qwen3Config
    base_model_prefix = "model"
    supports_gradient_checkpointing = True
    _no_split_modules = ["Qwen3DecoderLayer"]
    _skip_keys_device_placement = ["past_key_values"]
    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = True
    _supports_attention_backend = True
    _can_record_outputs = {
        "hidden_states": Qwen3DecoderLayer,
        "attentions": Qwen3Attention,
    }


class Qwen3RotaryEmbedding(nn.Module):
    inv_freq: torch.Tensor  # fix linting for `register_buffer`

    def __init__(self, config: Qwen3Config, device=None):
        super().__init__()
        # BC: "rope_type" was originally "type"
        if hasattr(config, "rope_scaling") and isinstance(config.rope_scaling, dict):
            self.rope_type = config.rope_scaling.get("rope_type", config.rope_scaling.get("type"))
        else:
            self.rope_type = "default"
        self.max_seq_len_cached = config.max_position_embeddings
        self.original_max_seq_len = config.max_position_embeddings

        self.config = config
        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]

        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self.original_inv_freq = self.inv_freq

    @torch.no_grad()
    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)
    def forward(self, x, position_ids):
        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        position_ids_expanded = position_ids[:, None, :].float()

        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != "mps" else "cpu"
        with torch.autocast(device_type=device_type, enabled=False):  # Force float32
            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
            emb = torch.cat((freqs, freqs), dim=-1)
            cos = emb.cos() * self.attention_scaling
            sin = emb.sin() * self.attention_scaling

        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)


@auto_docstring
class Qwen3Model(Qwen3PreTrainedModel):
    def __init__(self, config: Qwen3Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList(
            [Qwen3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.rotary_emb = Qwen3RotaryEmbedding(config=config)
        self.gradient_checkpointing = False
        self.has_sliding_layers = "sliding_attention" in self.config.layer_types

        # Initialize weights and apply final processing
        self.post_init()

    @check_model_inputs
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs: Unpack[TransformersKwargs],
    ) -> BaseModelOutputWithPast:
        if (input_ids is None) ^ (inputs_embeds is not None):
            raise ValueError("You must specify exactly one of input_ids or inputs_embeds")

        if inputs_embeds is None:
            inputs_embeds = self.embed_tokens(input_ids)

        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.config,
                "input_embeds": inputs_embeds,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            # The sliding window alternating layers are not always activated depending on the config
            if self.has_sliding_layers:
                causal_mask_mapping["sliding_attention"] = create_sliding_window_causal_mask(**mask_kwargs)

        hidden_states = inputs_embeds

        # create position embeddings to be shared across the decoder layers
        position_embeddings = self.rotary_emb(hidden_states, position_ids)

        for decoder_layer in self.layers[: self.config.num_hidden_layers]:
            hidden_states = decoder_layer(
                hidden_states,
                attention_mask=causal_mask_mapping[decoder_layer.attention_type],
                position_ids=position_ids,
                past_key_values=past_key_values,
                use_cache=use_cache,
                cache_position=cache_position,
                position_embeddings=position_embeddings,
                **kwargs,
            )

        hidden_states = self.norm(hidden_states)
        return BaseModelOutputWithPast(
            last_hidden_state=hidden_states,
            past_key_values=past_key_values if use_cache else None,
        )


@auto_docstring
class Qwen3ForCausalLM(Qwen3PreTrainedModel, GenerationMixin):
    _tied_weights_keys = ["lm_head.weight"]
    _tp_plan = {"lm_head": "colwise_rep"}
    _pp_plan = {"lm_head": (["hidden_states"], ["logits"])}

    def __init__(self, config):
        super().__init__(config)
        self.model = Qwen3Model(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    @can_return_tuple
    @auto_docstring
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        logits_to_keep: Union[int, torch.Tensor] = 0,
        **kwargs: Unpack[TransformersKwargs],
    ) -> CausalLMOutputWithPast:
        r"""
        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.

        Example:

        ```python
        >>> from transformers import AutoTokenizer, Qwen3ForCausalLM

        >>> model = Qwen3ForCausalLM.from_pretrained("Qwen/Qwen3-8B")
        >>> tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

        >>> prompt = "Hey, are you conscious? Can you talk to me?"
        >>> inputs = tokenizer(prompt, return_tensors="pt")

        >>> # Generate
        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
        ```"""
        outputs: BaseModelOutputWithPast = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            cache_position=cache_position,
            **kwargs,
        )

        hidden_states = outputs.last_hidden_state
        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])

        loss = None
        if labels is not None:
            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=logits,
            past_key_values=outputs.past_key_values,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )


class Qwen3ForSequenceClassification(GenericForSequenceClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForTokenClassification(GenericForTokenClassification, Qwen3PreTrainedModel):
    pass


class Qwen3ForQuestionAnswering(GenericForQuestionAnswering, Qwen3PreTrainedModel):
    base_model_prefix = "transformer"  # For BC, where `transformer` was used instead of `model`


__all__ = [
    "Qwen3ForCausalLM",
    "Qwen3ForQuestionAnswering",
    "Qwen3PreTrainedModel",
    "Qwen3Model",
    "Qwen3ForSequenceClassification",
    "Qwen3ForTokenClassification",
]
